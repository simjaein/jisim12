[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/2022-03-28-4주차_빅데이터분석특강.html",
    "href": "posts/2022-03-28-4주차_빅데이터분석특강.html",
    "title": "jisim12",
    "section": "",
    "text": "빅데이터분석특강\n\n\ntoc:false\nbranch: master\nbadges: true\ncomments: true\nauthor: 심재인\n\n\n\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nimport tensorflow.experimental.numpy as tnp\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\n\n\n\n\n\n예제9 : 카페예제로 돌아오자.\n\n\nx = tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4])\n\n2022-04-04 20:55:58.466144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n\ntf.random.set_seed(43052)\nepsilon=tf.random.normal([10])\ny=10.2 + 2.2*x + epsilon\n\n\ny\n\n<tf.Tensor: shape=(10,), dtype=float64, numpy=\narray([55.4183651 , 58.19427589, 61.23082496, 62.31255873, 63.1070028 ,\n       63.69569103, 67.24704918, 71.43650092, 73.10130336, 77.84988286])>\n\n\n\nbeta0 = tf.Variable(9.0)\nbeta1 = tf.Variable(2.0)\n\n\nwith tf.GradientTape(persistent=True) as tape:\n    loss = sum((y-beta0-beta1*x)**2)\n\n\ntape.gradient(loss, beta0), tape.gradient(loss, beta1)\n\n(<tf.Tensor: shape=(), dtype=float32, numpy=-126.78691>,\n <tf.Tensor: shape=(), dtype=float32, numpy=-3208.8396>)\n\n\n\n예제10:카페예제의 매트릭스 버전\n\n\nX = tnp.array([1]*10 +[20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]).reshape(2,10).T\nX\n\n<tf.Tensor: shape=(10, 2), dtype=float64, numpy=\narray([[ 1. , 20.1],\n       [ 1. , 22.2],\n       [ 1. , 22.7],\n       [ 1. , 23.3],\n       [ 1. , 24.4],\n       [ 1. , 25.1],\n       [ 1. , 26.2],\n       [ 1. , 27.3],\n       [ 1. , 28.4],\n       [ 1. , 30.4]])>\n\n\n\nbeta = tnp.array([9.0,2.0]).reshape(2,1)\nbeta\n\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[9.],\n       [2.]])>\n\n\n\nX@beta\n\n<tf.Tensor: shape=(10, 1), dtype=float64, numpy=\narray([[49.2],\n       [53.4],\n       [54.4],\n       [55.6],\n       [57.8],\n       [59.2],\n       [61.4],\n       [63.6],\n       [65.8],\n       [69.8]])>\n\n\n\nbeta_true = tnp.array([10.2,2.2]).reshape(2,1)\ny = X@beta_true+epsilon.reshape(10,1)\ny\n\n<tf.Tensor: shape=(10, 1), dtype=float64, numpy=\narray([[55.4183651 ],\n       [58.19427589],\n       [61.23082496],\n       [62.31255873],\n       [63.1070028 ],\n       [63.69569103],\n       [67.24704918],\n       [71.43650092],\n       [73.10130336],\n       [77.84988286]])>\n\n\n\nwith tf.GradientTape(persistent=True) as tape:\n    tape.watch(beta)\n    yhat= X@beta\n    loss= (y-yhat).T @(y-yhat)\n\n\ntape.gradient(loss,beta)\n\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[ -126.78690968],\n       [-3208.83947922]])>\n\n\n\n이론적인 값을 확인하면\n\n\n-2*X.T @ y + 2*X.T@X@beta\n\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[ -126.78690968],\n       [-3208.83947922]])>\n\n\n\n예제11 : 위의 예제에서 이론적인 \\(\\beta\\)의 최적값을 찾아보고 (즉 \\(\\hat\\beta\\)을 찾고) 그 지점에서 loss의 미분값(=접선의 기울기)를 구하라. 결과가 0인지 확인하라. (단 0은 길이가 2이고 각 원소가 0인 벡터)\n\n\\(\\beta\\)의 최적값은 \\((X'X)^{-1}X'y\\)이다.\n\nbeta_optimal = tf.linalg.inv(X.T @ X) @ X.T  @y\n\n\nwith tf.GradientTape(persistent=True) as tape:\n    tape.watch(beta_optimal)\n    yhat= X@beta_optimal\n    loss= (y-yhat).T @(y-yhat)\n\n\ntape.gradient(loss,beta_optimal)\n\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[-5.57065505e-12],\n       [-1.40943257e-10]])>\n\n\n- beta_true에서의 기울기도 계산해보자.\n\nwith tf.GradientTape(persistent=True) as tape:\n    tape.watch(beta_true)\n    yhat= X@beta_true\n    loss= (y-yhat).T @(y-yhat)\n\n\ntape.gradient(loss,beta_true)\n\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[ -2.74690968],\n       [-71.45947922]])>\n\n\n\n샘플사이즈가 커진다면 tape.gradient(loss,beta_true)\\(\\approx\\) tape.gradient(loss,beta_optimal)\n샘플사이즈가 커진다면 beta_true \\(\\approx\\) beta_optimal\n\n\n\n\n\n\n\n- \\(loss = (\\frac{1}{2}\\beta-1)^2\\)를 최소하는 \\(\\beta\\)를 컴퓨터를 활용하여 구하는 문제를 생각해보자. - 답은 \\(\\beta = 2\\)임을 알고 있다.\n\n\n\n\n\n\n\nbeta = [-10.00,-9.99,…,10.00] 와 같은 리스트를 만든다.\n(1)의 리스트의 각원소에 해당하는 loss를 구한다.\n(2)에서 구한 loss를 제일 작게 만드는 beta를 찾는다.\n\n\n\n\n\nbeta = np.linspace(-10,10,100)\nloss = (beta/2-1)**2\n\n\ntnp.argmin([1,2,-3,3,4])\n\n<tf.Tensor: shape=(), dtype=int64, numpy=2>\n\n\n\ntnp.argmin([1,2,3,-3,4])\n\n<tf.Tensor: shape=(), dtype=int64, numpy=3>\n\n\n\ntnp.argmin(loss)\n\n<tf.Tensor: shape=(), dtype=int64, numpy=59>\n\n\n\nbeta[59]\n\n1.9191919191919187\n\n\n\n(beta[60]/2-1)**2\n\n0.0036730945821854847\n\n\n\n\n\n\n\n\n\n\n\n\nbeta = -5로 셋팅한다. (초깃값으로 셋팅)\n\n\n(-5/2-1)**2\n\n12.25\n\n\n\nbeta= -5 근처에서 조금씩 이동하여 loss를 조사해본다.\n\n\n(-4.99/2-1)**2 ## 오른쪽으로 0.01 이동하고 loss조사  (미분)\n\n12.215025\n\n\n\n(-5.01/2-1)**2 ## 왼쪽으로 0.01 이동하고 loss조사  (미분)\n\n12.285025\n\n\n\n(2)의 결과를 잘 해석하고 더 유리한 쪽으로 이동 (미분 결과 관찰 후 유리한 쪽으로 이동)\n위의 과정을 반복하고 왼쪽, 오른쪽 어느쪽으로 움직여도 이득이 없다면 멈춘다.\n\n\n\n\n- (2)-(3)의 과정은 beta=-5 에서 미분계수를 구하고 미분계수가 양수이면 왼쪽으로 움직이고 음수면 오른쪽으로 움직인다고 해석가능. 아래그림을 보면 더 잘 이해가 된다.\n\nplt.plot(beta,loss)\n\n\n\n\n\n\n\n- 아래와 같이 해석가능 - 오른쪽으로 0.01 간다 = beta_old에 0.01을 더함. (미분계수가 음수이면) - 왼쪽으로 0.01 간다 = beta_old에 0.01을 뺀다. (미분계수가 양수이면)\n- 그렇다면\n$_{new} =\n\\[\\begin{cases}\n\\beta_{old} + 0.01, & loss'(\\beta_{old})<0  \\\\\n\\beta_{old} - 0.01, & loss'(\\beta_{old})>0\n\\end{cases}\\]\n$\n\n\n\n- 항상 0.01씩 움직여야 하는가?\n\nplt.plot(beta,loss)\n\n\n\n\n- \\(\\beta= -10\\)일 경우의 접선의 기울기? \\(\\beta=-4\\)일때 접선의 기울기?\n\n\\(\\beta=-10\\) => 기울기는 -6\n\\(\\beta=-4\\) => 기울기는 -3\n\n- 실제로 6,3씩 이동할수는 없으니 적당한 \\(\\alpha(예를 들면 \\alpha = 0.01)\\)를 잡아서 곱한만큼 이동하자.\n- 수식화하면 - \\(\\beta_{new} = \\beta_{old} - \\alpha~ loss'(\\beta_{old})\\) - \\(\\beta_{new} = \\beta_{old} - \\alpha~ \\left[\\frac{\\partial}{\\partial \\beta}loss(\\beta)\\right]_{\\beta=\\beta_{old}}\\)\n- \\(\\alpha\\)의 의미 - \\(\\alpha\\)가 크면 크게크게 움직이고 작으면 작게작게 움직인다. - \\(\\alpha>0\\) 이어야 한다.\n\n\n\n- iter 1\n- \\(\\beta=-10\\)이라고 하자.\n\nbeta = tf.Variable(-10.0)\n\n\nwith tf.GradientTape(persistent=True) as tape:\n    loss = (beta/2-1)**2\n\n\ntape.gradient(loss,beta)\n\n<tf.Tensor: shape=(), dtype=float32, numpy=-6.0>\n\n\n\\(\\beta = -10\\)에서 0.01만큼 움직이고 싶다.\n\nalpha = 0.01/6\n\n\nalpha * tape.gradient(loss,beta)\n\n<tf.Tensor: shape=(), dtype=float32, numpy=-0.01>\n\n\n\nbeta.assign_sub(alpha * tape.gradient(loss,beta))\n\n<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=-9.99>\n\n\n\nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.99>\n\n\n-iter 2\n\nwith tf.GradientTape(persistent=True) as tape:\n    loss = (beta/2-1)**2\n\n\nbeta.assign_sub(tape.gradient(loss,beta)*alpha)\n\n<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=-9.980008>\n\n\n- for 문을 이용하자.\n(강의용)\n\nbeta = tf.Variable(-10.0)\n\n\nfor k in range(10000):\n    with tf.GradientTape(persistent=True) as tape:\n        loss = (beta/2-1)**2\n    beta.assign_sub(tape.gradient(loss,beta)*alpha)\n\n\nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.997125>\n\n\n(시도 1)\n\nbeta = tf.Variable(-10.0)\n\n\nfor k in range(100):\n    with tf.GradientTape(persistent=True) as tape:\n        loss = (beta/2-1)**2\n    beta.assign_sub(tape.gradient(loss,beta)*alpha)\n\n\nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.040152>\n\n\n(시도 2)\n\nbeta = tf.Variable(-10.0)\n\n\nfor k in range(1000):\n    with tf.GradientTape(persistent=True) as tape:\n        loss = (beta/2-1)**2\n    beta.assign_sub(tape.gradient(loss,beta)*alpha)\n\n\nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-3.2133687>\n\n\n- 너무 느린 것 같다? \\(\\to\\) \\(\\alpha\\)를 키워보자!\n\n\n\n- 목표 : \\(\\alpha\\)에 따라서 수렴과정이 어떻게 달라지는지 시각화해보자.\n[시각화 코드 예비학습]\n\nfig = plt.figure()  # 도화지가 만들어지고 fig라는 이름을 붙인다.\n\n<Figure size 432x288 with 0 Axes>\n\n\n\nfig\n\n<Figure size 432x288 with 0 Axes>\n\n\n\nax = fig.add_subplot()  #fig는 ax라는 물체를 만든다.\n\n\nid(fig.axes[0])\n\n139706970921024\n\n\n\nid(ax)\n\n139706970921024\n\n\n\npnts, = ax.plot([1,2,3],[4,5,6],'or')\npnts\n\n<matplotlib.lines.Line2D at 0x7f101062b670>\n\n\n\npnts.get_xdata()\n\narray([1, 2, 3])\n\n\n\npnts.get_ydata()\n\narray([4, 5, 6])\n\n\n\nfig\n\n\n\n\n\npnts.set_ydata([5,5,5])\n\n\npnts.get_ydata()\n\n[5, 5, 5]\n\n\n\nfig\n\n\n\n\n-응용\n\nplt.rcParams[\"animation.html\"] = \"jshtml\"\nfrom matplotlib import animation\n\n\ndef animate(i):\n    if i%2 == 0:\n        pnts.set_ydata([4,5,6])\n    else:\n        pnts.set_ydata([5,5,5])\n\n\nani=animation.FuncAnimation(fig,animate,frames=10)\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n예비학습 끝!\n- beta_lst = [-10,-9,-8] 로 이동한다고 하자.\n\nbeta_lst = [-10,-9,-8]\nloss_lst = [(-10/2-1)**2, (-9/2-1)**2, (-8/2-1)**2]\n\n\nfig = plt.figure()\n\n<Figure size 432x288 with 0 Axes>\n\n\n\nax = fig.add_subplot()\n\n\n_beta = np.linspace(-15,19,100)\n\n\nax.plot(_beta,(_beta/2-1)**2)\n\n\nfig\n\n\n\n\n\npnts, = ax.plot(beta_lst[0],loss_lst[0],'ro')\nfig\n\n\n\n\n\ndef animate(i):\n    pnts.set_xdata(beta_lst[:(i+1)])\n    pnts.set_ydata(loss_lst[:(i+1)])\n\n\nani = animation.FuncAnimation(fig, animate, frames=3)\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n-최종아웃풋\n\nbeta = tf.Variable(-10.0)\nalpha = 0.01/6\n\n\nbeta_lst = []\nloss_lst = []\n\n\nbeta_lst.append(beta.numpy())\nloss_lst.append((beta.numpy()/2-1)**2)\n\n\nwith tf.GradientTape(persistent=True) as tape:\n    tape.watch(beta)\n    loss = (beta/2-1)**2\n\n\nbeta.assign_sub(tape.gradient(loss,beta)*alpha)\n\n<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=-9.99>\n\n\n\nbeta_lst.append(beta.numpy())\nloss_lst.append((beta.numpy()/2-1)**2)\n\n\nbeta_lst, loss_lst\n\n([-10.0, -9.99], [36.0, 35.94002362785341])\n\n\n- for\n\nbeta = tf.Variable(-10.0)\nalpha = 0.01/6\nbeta_lst = []\nloss_lst = []\nbeta_lst.append(beta.numpy())\nloss_lst.append((beta.numpy()/2-1)**2)\nfor k in range(100):\n    with tf.GradientTape(persistent=True) as tape:\n        tape.watch(beta)\n        loss = (beta/2-1)**2\n    beta.assign_sub(tape.gradient(loss,beta)*alpha)\n    beta_lst.append(beta.numpy())\n    loss_lst.append((beta.numpy()/2-1)**2)\n\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.plot(_beta,(_beta/2-1)**2)\npnts,= ax.plot(beta_lst[0],loss_lst[0],'or')\n\n\n\n\n\nani = animation.FuncAnimation(fig,animate,frames=100)\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n- y=(x-1)^2을 최소화하는 x를 확률적 경사하강법으로 찾고 애니메이션으로 찾고 시각화할것\n\nbeta = tf.Variable(-3.0)\nalpha = 0.01/3\n\nbeta_lst=[]\nloss_lst=[]\n\nbeta_lst.append(beta.numpy())\nloss_lst.append((beta.numpy()-1)**2)\n\nfor k in range(100):\n    with tf.GradientTape(persistent=True) as tape:\n        tape.watch(beta)\n        loss = (beta-1)**2\n    beta.assign_sub(tape.gradient(loss, beta)*alpha)\n    beta_lst.append(beta.numpy())\n    loss_lst.append((beta.numpy()-1)**2)\n\n\n_beta = np.linspace(-20,22,100)\n\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.plot(_beta,(_beta-1)**2)\npnts, = ax.plot(beta_lst[0],loss_lst[0],'or')\n\n\n\n\n\ndef animate(i):\n    pnts.set_xdata(beta_lst[:(i+1)])\n    pnts.set_ydata(loss_lst[:(i+1)])\n\n\nani = animation.FuncAnimation(fig, animate, frames=100)\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/2022-04-04-5주차_빅데이터분석특강.html",
    "href": "posts/2022-04-04-5주차_빅데이터분석특강.html",
    "title": "jisim12",
    "section": "",
    "text": "5주차-4월 04일\n\n빅데이터분석특강\n\n\ntoc:false\nbranch: master\nbadges: true\ncomments: true\nauthor: 심재인\n\n\n강의영상\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-wVWUAZ5xT35INvWbNOXpBx\n\n\n\nimports\n\n#\n#!conda install -c conda-forge python-graphviz -y\n\n\nimport tensorflow as tf \nimport numpy as np\nimport matplotlib.pyplot as plt \n\n\nimport tensorflow.experimental.numpy as tnp \n\n\ntnp.experimental_enable_numpy_behavior() \n\n\n\n최적화의 문제\n- \\(loss=(\\frac{1}{2}\\beta-1)^2\\)\n- 기존에 했던 방법은 수식을 알고 있어야 한다는 단점이 있음\n\n\ntf.keras.optimizers를 이용한 최적화방법\n\n방법1: opt.apply_gradients()를 이용\n\nalpha= 0.01/6\n\n\nbeta= tf.Variable(-10.0) \n\n2022-04-25 14:39:32.750112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n\nopt = tf.keras.optimizers.SGD(alpha)\n\n- iter1\n\nwith tf.GradientTape() as tape: \n    tape.watch(beta) \n    loss=(beta/2-1)**2 \nslope = tape.gradient(loss,beta)\n\n\nopt.apply_gradients([(slope,beta)]) # beta.assign_sub(slope * alpha) \nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.99>\n\n\n- iter2\n\nwith tf.GradientTape() as tape: \n    tape.watch(beta) \n    loss=(beta/2-1)**2 \nslope = tape.gradient(loss,beta)\nopt.apply_gradients([(slope,beta)]) # beta.assign_sub(slope * alpha) \nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.980008>\n\n\n- for문으로 정리\n\nalpha= 0.01/6\nbeta= tf.Variable(-10.0) \nopt = tf.keras.optimizers.SGD(alpha)\n\n\nfor epoc in range(10000): \n    with tf.GradientTape() as tape: \n        tape.watch(beta) \n        loss=(beta/2-1)**2 \n    slope = tape.gradient(loss,beta)\n    opt.apply_gradients([(slope,beta)]) # beta.assign_sub(slope * alpha) \n    beta\n\n\nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9971251>\n\n\n\nopt.apply_gradients()의 입력은 pair 의 list\n\n\n\n방법2: opt.minimize()\n\nalpha= 0.01/6\nbeta= tf.Variable(-10.0) \nopt = tf.keras.optimizers.SGD(alpha)\n\n\nloss_fn = lambda: (beta/2-1)**2\n\n\nlambda x: x**2 <=> lambda(x)=x^2\nlambda x,y: x+y <=> lambda(x,y)=x+y\nlambda: y <=> lambda()=y, 입력이 없으며 출력은 항상 y인 함수\n\n\nloss_fn() # 입력은 없고 출력은 뭔가 계산되는 함수 \n\n<tf.Tensor: shape=(), dtype=float32, numpy=36.0>\n\n\n- iter 1\n\nopt.minimize(loss_fn, beta)\n\n<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=1>\n\n\n\nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.99>\n\n\n- iter2\n\nopt.minimize(loss_fn, beta)\nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.980008>\n\n\n- for문으로 정리하면\n\nalpha= 0.01/6\nbeta= tf.Variable(-10.0) \nopt = tf.keras.optimizers.SGD(alpha)\nloss_fn = lambda: (beta/2-1)**2\nfor epoc in range(10000): \n    opt.minimize(loss_fn, beta)\nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9971251>\n\n\n\n\n\n회귀분석 문제\n- \\({\\bf y} \\approx 2.5 + 4.0 {\\bf x}\\)\n\ntnp.random.seed(43052)\nN = 200\nx = tnp.linspace(0,1,N) \nepsilon = tnp.random.randn(N)*0.5\ny = 2.5+4*x + epsilon\ny_true = 2.5+4*x\n\n\nplt.plot(x,y,'.')\nplt.plot(x,y_true,'r--')\n\n\n\n\n\n\n이론적 풀이\n\n풀이1: 스칼라버전\n- 포인트 - \\(S_{xx}=\\), \\(S_{xy}=\\) - \\(\\hat{\\beta}_0=\\), \\(\\hat{\\beta}_1=\\)\n- 풀이\n\nSxx = sum((x-x.mean())**2)\nSxy = sum((x-x.mean())*(y-y.mean()))\n\n\nbeta1_hat = Sxy/Sxx \nbeta1_hat\n\n<tf.Tensor: shape=(), dtype=float64, numpy=3.933034516733168>\n\n\n\nbeta0_hat = y.mean() - x.mean()*beta1_hat\nbeta0_hat\n\n<tf.Tensor: shape=(), dtype=float64, numpy=2.583667211565867>\n\n\n\n\n풀이2: 벡터버전\n- 포인트 - \\(\\hat{\\beta}=(X'X)^{-1}X'y\\)\n- 풀이\n\ny=y.reshape(N,1)\nX=tf.stack([tf.ones(N,dtype=tf.float64),x],axis=1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\ntf.linalg.inv(X.T @ X ) @ X.T @ y \n\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[2.58366721],\n       [3.93303452]])>\n\n\n\n\n풀이3: 벡터버전, 손실함수의 도함수이용\n- 포인트 - \\(loss'(\\beta)=-2X'y +2X'X\\beta\\) - \\(\\beta_{new} = \\beta_{old} - \\alpha \\times loss'(\\beta_{old})\\)\n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tnp.array([-5,10]).reshape(2,1)\nbeta_hat\n\n<tf.Tensor: shape=(2, 1), dtype=int64, numpy=\narray([[-5],\n       [10]])>\n\n\n\nslope = (-2*X.T @ y + 2*X.T @ X @ beta_hat) / N \nslope\n\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[-9.10036894],\n       [-3.52886113]])>\n\n\n\nalpha= 0.1 \n\n\nstep = slope*alpha\nstep\n\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[-0.91003689],\n       [-0.35288611]])>\n\n\n\nfor epoc in range(1000): \n    slope = (-2*X.T @ y + 2*X.T @ X @ beta_hat)/N \n    beta_hat = beta_hat - alpha* slope\n\n\nbeta_hat\n\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])>\n\n\n\n\n\nGradientTape를 이용\n\n풀이1: 벡터버전\n- 포인트\n## 포인트코드1: 그레디언트 테입  \nwith tf.GradientTape() as tape: \n    loss = \n## 포인트코드2: 미분 \nslope = tape.gradient(loss,beta_hat) \n## 포인트코드3: update \nbeta_hat.assign_sub(slope*alph) \n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])>\n\n\n\nalpha=0.1\n\n\nfor epoc in range(1000):\n    with tf.GradientTape() as tape: \n        yhat= X@beta_hat\n        loss= (y-yhat).T @ (y-yhat) / N\n    slope = tape.gradient(loss,beta_hat) \n    beta_hat.assign_sub(alpha*slope) \n\n\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])>\n\n\n\n\n풀이2: 스칼라버전\n- 포인트\n## 포인트코드: 미분\nslope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat])\n- 풀이\n\ny=y.reshape(-1)\ny.shape,x.shape\n\n(TensorShape([200]), TensorShape([200]))\n\n\n\nbeta0_hat = tf.Variable(-5.0)\nbeta1_hat = tf.Variable(10.0)\n\n\nalpha=0.1\n\n\nfor epoc in range(1000):\n    with tf.GradientTape() as tape: \n        yhat= beta0_hat + x*beta1_hat \n        loss= tf.reduce_sum((y-yhat)**2)/N #loss= sum((y-yhat)**2)/N\n    slope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat]) \n    beta0_hat.assign_sub(alpha*slope0)\n    beta1_hat.assign_sub(alpha*slope1)\n\n\nbeta0_hat,beta1_hat\n\n(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.58366>,\n <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.933048>)\n\n\n\n\n\nGradientTape + opt.apply_gradients\n\n풀이1: 벡터버전\n- 포인트\n## 포인트코드: 업데이트\nopt.apply_gradients([(slope,beta_hat)])  ## pair의 list가 입력 \n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])>\n\n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000):\n    with tf.GradientTape() as tape: \n        yhat= X@beta_hat\n        loss= (y-yhat).T @ (y-yhat) / N\n    slope = tape.gradient(loss,beta_hat)\n    opt.apply_gradients([(slope,beta_hat)])\n    #beta_hat.assign_sub(alpha*slope) \n\n\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])>\n\n\n\n\n풀이2: 스칼라버전\n- 포인트\n## 포인트코드: 업데이트 \nopt.apply_gradients([(slope0,beta0_hat),(slope1,beta1_hat)]) ## pair의 list가 입력 \n- 풀이\n\ny=y.reshape(-1)\ny.shape,x.shape\n\n(TensorShape([200]), TensorShape([200]))\n\n\n\nbeta0_hat = tf.Variable(-5.0)\nbeta1_hat = tf.Variable(10.0)\n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000):\n    with tf.GradientTape() as tape: \n        yhat= beta0_hat + beta1_hat*x #X@beta_hat\n        loss= tf.reduce_sum((y-yhat)**2) / N\n    slope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat])\n    opt.apply_gradients([(slope0,beta0_hat),(slope1,beta1_hat)])\n\n\nbeta0_hat,beta1_hat\n\n(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.58366>,\n <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.933048>)\n\n\n\n\n\nopt.minimize\n\n풀이1: 벡터버전, 사용자정의 손실함수 with lambda\n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])>\n\n\n\nloss_fn = lambda: (y-X@beta_hat).T @ (y-X@beta_hat) / N \n\n\nalpha=0.1 \nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000): \n    opt.minimize(loss_fn,beta_hat)\n\n\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])>\n\n\n\n\n풀이2: 스칼라버전, 사용자정의 손실함수 with lambda\n- 포인트\n## 포인트코드: 미분 & 업데이트 = minimize \nopt.minimize(loss_fn,[beta0_hat,beta1_hat])\n- 풀이\n\ny=y.reshape(-1)\ny.shape,x.shape\n\n(TensorShape([200]), TensorShape([200]))\n\n\n\nbeta0_hat = tf.Variable(-5.0)\nbeta1_hat = tf.Variable(10.0) \n\n\nloss_fn = lambda: tf.reduce_sum((y-beta0_hat-beta1_hat*x )**2) / N \n\n\nalpha=0.1 \nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000): \n    opt.minimize(loss_fn,[beta0_hat,beta1_hat])\n\n\nbeta0_hat,beta1_hat\n\n(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.58366>,\n <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.933048>)\n\n\n\n\n풀이3: 벡터버전, 사용자정의 (짧은) 손실함수\n- 포인트\n## 포인트코드: 손실함수정의 \ndef loss_fn():\n    return ??\n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])>\n\n\n\ndef loss_fn():\n    return (y-X@beta_hat).T @ (y-X@beta_hat) / N \n\n\nalpha=0.1 \nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000): \n    opt.minimize(loss_fn,beta_hat)\n\n\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])>\n\n\n\n\n풀이4: 벡터버전, 사용자정의 (긴) 손실함수\n- 포인트\n## 포인트코드: 손실함수정의 \ndef loss_fn():\n    ??\n    ??\n    return ??\n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])>\n\n\n\ndef loss_fn():\n    yhat= X@beta_hat # 컴퓨터한테 전달할 수식1\n    loss = (y-yhat).T @ (y-yhat) / N # 컴퓨터한테 전달할 수식 2 \n    return loss # tape.gradient(loss,beta_hat) 에서의 미분당하는애 \n\n\nalpha=0.1 \nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000): \n    opt.minimize(loss_fn,beta_hat)\n\n\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])>\n\n\n\n\n풀이5: 벡터버전, 사용자정의 손실함수 <- tf.losses.MSE\n- 포인트\n## 포인트코드: 미리구현되어있는 손실함수 이용 \ntf.losses.MSE(y,yhat)\n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])>\n\n\n\ndef loss_fn():\n    yhat= X@beta_hat # 컴퓨터한테 전달할 수식1\n    loss = tf.keras.losses.MSE(y.reshape(-1),yhat.reshape(-1)) # 컴퓨터한테 전달할 수식 2 \n    return loss # tape.gradient(loss,beta_hat) 에서의 미분당하는애 \n\n\nalpha=0.1 \nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000): \n    opt.minimize(loss_fn,beta_hat)\n\n\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])>\n\n\n\n\n풀이6: 벡터버전, 사용자정의 손실함수 <- tf.losses.MeaSquaredError\n- 포인트\n## 포인트코드: 클래스로부터 손실함수 오브젝트 생성 (함수를 찍어내는 클래스) \nmse_fn = tf.losses.MeanSquaredError()\nmse_fn(y,yhat)\n- 풀이\n\nmseloss_fn = tf.losses.MeanSquaredError()\n\n\nmseloss_fn = tf.keras.losses.MSE 라고 보면된다.\n\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])>\n\n\n\ndef loss_fn():\n    yhat= X@beta_hat # 컴퓨터한테 전달할 수식1\n    loss = mseloss_fn(y.reshape(-1),yhat.reshape(-1)) # 컴퓨터한테 전달할 수식 2 \n    return loss # tape.gradient(loss,beta_hat) 에서의 미분당하는애 \n\n\nalpha=0.1 \nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000): \n    opt.minimize(loss_fn,beta_hat)\n\n\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])>\n\n\n\n\n\ntf.keras.Sequential\n- \\(\\hat{y}_i=\\hat{\\beta}_0+\\hat{\\beta}_1x_i\\) 의 서로다른 표현\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+s + '; }')\n\n\ngv(''' \n    \"1\" -> \"beta0_hat + x*beta1_hat,    bias=False\"[label=\"* beta0_hat\"]\n    \"x\" -> \"beta0_hat + x*beta1_hat,    bias=False\"[label=\"* beta1_hat\"]\n    \"beta0_hat + x*beta1_hat,    bias=False\" -> \"yhat\"[label=\"indentity\"]\n    ''')\n\nExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\n\n\n<graphviz.sources.Source at 0x7fe34059cb50>\n\n\n\ngv('''\n\"x\" -> \"x*beta1_hat,    bias=True\"[label=\"*beta1_hat\"] ;\n\"x*beta1_hat,    bias=True\" -> \"yhat\"[label=\"indentity\"] ''')\n\nExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\n\n\n<graphviz.sources.Source at 0x7fe32c6f5030>\n\n\n\ngv('''\n\"X=[1 x]\" -> \"X@beta_hat,    bias=False\"[label=\"@beta_hat\"] ;\n\"X@beta_hat,    bias=False\" -> \"yhat\"[label=\"indentity\"] ''')\n\nExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\n\n\n<graphviz.sources.Source at 0x7fe32c59f520>\n\n\n\n풀이1: 벡터버전, 사용자정의 손실함수\n- 포인트\n## 포인트코드1: 네트워크 생성 \nnet = tf.keras.Sequential()\n\n## 포인트코드2: 네트워크의 아키텍처 설계 \nnet.add(tf.keras.layers.Dense(1,input_shape=(2,),use_bias=False)) \n\n## 포인트코드3: 네트워크 컴파일 = 아키텍처 + 손실함수 + 옵티마이저\nnet.compile(opt,loss=loss_fn2)\n\n## 포인트코드4: 미분 & update \nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) \n- 풀이\n\nnet = tf.keras.Sequential() \n\n\nnet.add(tf.keras.layers.Dense(units=1,input_shape=(2,),use_bias=False)) ## yhat을 구하는 방법정의 = 아키텍처가 설계 \n\n\nunits는 layer의 출력의 차원, 이 경우는 yhat의 차원, yhat은 (200,1) 이므로 1임.\ninput_shape는 layer의 입력의 차원, 이 경우는 X의 차원, X는 (200,2) 이므로 2임.\n\n\ndef loss_fn2(y,yhat):\n    return (y-yhat).T @ (y-yhat) / N \n\n\nalpha=0.1\nopt =tf.optimizers.SGD(alpha)\n\n\n[np.array([[-5.0],[10.0]],dtype=np.float32)]\n\n[array([[-5.],\n        [10.]], dtype=float32)]\n\n\n\nnet.set_weights([np.array([[-5.0],[10.0]],dtype=np.float32)])\n\n\nnet.weights\n\n[<tf.Variable 'dense/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[-5.],\n        [10.]], dtype=float32)>]\n\n\n\nnet.compile(opt,loss=tf.losses.MSE)\n# 아키텍처 + 손실함수 + 옵티마이저 => 네트워크에 다 합치자 => 네트워크를 컴파일한다. \n\n\nnet.fit(X,y,epochs=1000,batch_size=N,verbose=0) # 미분 + 파라메터업데이트 = net.fit \n\n<keras.callbacks.History at 0x7fe2c4621ab0>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.58366 ],\n        [3.933048]], dtype=float32)>]"
  },
  {
    "objectID": "posts/Untitled1.html",
    "href": "posts/Untitled1.html",
    "title": "Cough Analysis",
    "section": "",
    "text": "2020년 3월 11일에 COVID-19는 전세계 최소 114국으로 퍼졌으며 약 4,000명이 사망하여 세계보건기구는 COVID-19가 세계적 대유행임을 선언하였다.\nCOVID-19가 전세계적으로 대유행을 하면서 우리들의 많은 일상에 변화들이 일어나게 되었다.\n그로 인해서 조금이나마 우리가 COVID-19에 예방이라도 할 수 있을까? 라는 생각을 하게 되었고,\nCOVID-19의 증상으로는, 가족 내에서 발생한 사례를 분석하였을 때 노출 후 3–7일 경에 열과 호흡기 증상이 발생하였다.\n열, 마른 기침, 피로 감이 주요 증상이며, 코막힘, 콧물, 인후통, 근육통 등은 비교적 드물게 나타난다.\n발병 초기에 드물지만 두근거림, 설사, 두통과 같은 호흡기와 무관한 증상이 먼저 나타나기도 하였다.\\(\\quad\\) 병의 초기에 열이 없는 경우도 있었다.\nCOVID-19는 무증상 감염부터 폐렴 및 사망까지 일으킨다.\n심한 경우 폐렴으로까지 가는 COVID-19는 주로 나타나는 증상 중 기침이 있습니다.\n그래서 기침소리를 분석해서 사람들의 기침 소리로 COVID-19를 분류할 수 있을지 연구해봐야겠다고 생각하게 되었습니다.\n\n메타데이터 가져오고 녹음 오디오 올바른거 선택\n나이와 기침소리 감지 점수에 따라서 다른 등급 정의\nMinMaxScaler, OneHotEncoder,LabelEncoder으로 데이터값을 0과 1사이의 범위 값으로 변환 (1에 가까우면 진짜 기침 소리일 가능성 높고, 0에 가까울수록 가짜 기침 소리일 가능성이 높다)\nCOVID 식별 모델 교육\n각 audio_class에 대해, 녹음의 오디오 속성뿐만 아니라 주요 메타데이터를 기반으로 상태를 표시하는 별도의 모델을 교육한다.\n모든 오디오 파일에 대한 DSR 루프 출력\n데이터 프레임에서 클래스 균형 조정\n데이터 프레임을 저장하여 값 지정\n지정된 UUID에 대한 사운드 및 스펙트로그램 시각화\n분할 열차 시험\n데이터 집합의 불균형으로 인해 작은 클래스를 보존하려고 ‘stratify’ 인수를 사용.\n데이터셋 정규화\n데이터 유출을 방지하기 위해 훈련 세트만 사용하여 스케일러를 장착.\n모형 정확도를 표시하고 평가하는 데 사용되는 도우미 함수\n로지스틱 회귀 분석 설정(멀티 클래스)\n로지스틱 회귀분석 대표적인 이진분류 기법으로, train 값과 라벨링(0 또는 1)을 통해 모델을 생성, 생성된 모델에 test 값을 넣어 예측해본다.\nXGBoost를 사용하여 BDT 분류기 교육\nXG Boost 교육 진단\n특정 변수의 상대적 영향도를 측정하여 분류 모델의 불순도를 더 많이 낮춰주는 변수들을 찾고, 정렬되기 전 인덱스를 리스트 형태로 반환하고, 배열을 뒤집는다.\n예측을 원래 데이터 프레임으로 다시 병합\n병합 후 XGBoost에서 잘못된 작업을 수행하는 항목을 선택하고 출력.\n\n\n!git add .\n!git commit -m .\n!git push\n!quarto publish gh-pages --no-prompt --no-browser\n\n[main d5e6fe8] .\n 59 files changed, 549908 insertions(+), 29 deletions(-)\n create mode 100644 \"posts/.ipynb_checkpoints/2021-12-20-\\353\\215\\260\\354\\235\\264\\355\\204\\260\\354\\213\\234\\352\\260\\201\\355\\231\\224_\\352\\270\\260\\353\\247\\220\\352\\263\\240\\354\\202\\254\\355\\222\\200\\354\\235\\264-checkpoint.ipynb\"\n create mode 100644 \"posts/.ipynb_checkpoints/2021-12-31-python\\354\\235\\204 \\354\\235\\264\\354\\232\\251\\355\\225\\234 \\352\\263\\240\\354\\234\\240\\352\\260\\222\\352\\263\\274 \\352\\263\\240\\354\\234\\240\\353\\262\\241\\355\\204\\260 \\352\\265\\254\\355\\225\\230\\352\\270\\260-checkpoint.ipynb\"\n create mode 100644 \"posts/.ipynb_checkpoints/2021-12-31-\\354\\204\\240\\355\\230\\225\\353\\214\\200\\354\\210\\230_\\352\\263\\240\\354\\234\\240\\352\\260\\222_\\352\\263\\240\\354\\234\\240\\353\\262\\241\\355\\204\\260-checkpoint.ipynb\"\n create mode 100644 \"posts/.ipynb_checkpoints/2022-01-12-\\355\\232\\214\\352\\267\\200\\353\\266\\204\\354\\204\\235_\\353\\213\\250\\354\\210\\234\\354\\204\\240\\355\\230\\225\\355\\232\\214\\352\\267\\200\\353\\266\\204\\354\\204\\235-checkpoint.ipynb\"\n create mode 100644 \"posts/.ipynb_checkpoints/2022-01-13-\\355\\232\\214\\352\\267\\200\\353\\266\\204\\354\\204\\235_\\354\\244\\221\\355\\232\\214\\352\\267\\200\\353\\252\\250\\355\\230\\225-checkpoint.ipynb\"\n create mode 100644 \"posts/.ipynb_checkpoints/2022-01-14-\\355\\232\\214\\352\\267\\200\\353\\266\\204\\354\\204\\235-\\352\\267\\200\\354\\247\\204\\353\\213\\250-checkpoint.ipynb\"\n create mode 100644 \"posts/.ipynb_checkpoints/2022-01-14-\\355\\232\\214\\352\\267\\200\\353\\266\\204\\354\\204\\235-\\353\\263\\200\\354\\210\\230\\354\\204\\240\\355\\203\\235-checkpoint.ipynb\"\n create mode 100644 \"posts/.ipynb_checkpoints/2022-03-07-1\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225-checkpoint.ipynb\"\n create mode 100644 \"posts/.ipynb_checkpoints/2022-03-14-2\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225-checkpoint.ipynb\"\n create mode 100644 \"posts/.ipynb_checkpoints/2022-03-21-3\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225-checkpoint.ipynb\"\n create mode 100644 \"posts/.ipynb_checkpoints/2022-03-28-4\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225-checkpoint.ipynb\"\n create mode 100644 \"posts/.ipynb_checkpoints/2022-04-04-5\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225-checkpoint.ipynb\"\n create mode 100644 \"posts/.ipynb_checkpoints/2022-04-11-6\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225-checkpoint.ipynb\"\n create mode 100644 \"posts/.ipynb_checkpoints/2022-04-18-7\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225-checkpoint.ipynb\"\n create mode 100644 \"posts/.ipynb_checkpoints/2022-04-28-\\354\\244\\221\\352\\260\\204\\352\\263\\240\\354\\202\\254\\354\\230\\210\\354\\203\\201\\353\\254\\270\\354\\240\\234_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225-checkpoint.ipynb\"\n create mode 100644 \"posts/.ipynb_checkpoints/2022-04-29-\\354\\213\\254\\354\\236\\254\\354\\235\\270_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_\\354\\244\\221\\352\\260\\204\\352\\263\\240\\354\\202\\254-checkpoint.ipynb\"\n create mode 100644 \"posts/.ipynb_checkpoints/2022-04-30-\\354\\244\\221\\352\\260\\204\\352\\263\\240\\354\\202\\254\\355\\225\\264\\354\\204\\244-checkpoint.ipynb\"\n create mode 100644 \"posts/.ipynb_checkpoints/2022-05-02-9\\354\\243\\274\\354\\260\\250(1)_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225-checkpoint.ipynb\"\n create mode 100644 \"posts/.ipynb_checkpoints/2022-05-03-9\\354\\243\\274\\354\\260\\250(2)_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225-checkpoint.ipynb\"\n create mode 100644 \"posts/.ipynb_checkpoints/2022-05-09-10\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225-checkpoint.ipynb\"\n create mode 100644 \"posts/.ipynb_checkpoints/2022-05-16-11\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225-checkpoint.ipynb\"\n create mode 100644 \"posts/.ipynb_checkpoints/2022-05-23-12\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225-checkpoint.ipynb\"\n create mode 100644 \"posts/.ipynb_checkpoints/2022-06-13-\\352\\270\\260\\353\\247\\220\\352\\263\\240\\354\\202\\254_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225-checkpoint.ipynb\"\n create mode 100644 \"posts/.ipynb_checkpoints/2022-06-30-\\352\\270\\260\\353\\247\\220\\352\\263\\240\\354\\202\\254\\354\\230\\210\\354\\203\\201\\353\\254\\270\\354\\240\\234_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225-checkpoint.ipynb\"\n create mode 100644 \"posts/.ipynb_checkpoints/2022.09.02_\\353\\260\\234\\355\\221\\234\\354\\236\\220\\353\\243\\214-checkpoint.ipynb\"\n create mode 100644 posts/.ipynb_checkpoints/Untitled-checkpoint.ipynb\n create mode 100644 posts/.ipynb_checkpoints/Untitled1-checkpoint.ipynb\n create mode 100644 \"posts/2021-12-20-\\353\\215\\260\\354\\235\\264\\355\\204\\260\\354\\213\\234\\352\\260\\201\\355\\231\\224_\\352\\270\\260\\353\\247\\220\\352\\263\\240\\354\\202\\254\\355\\222\\200\\354\\235\\264.ipynb\"\n create mode 100644 \"posts/2021-12-31-python\\354\\235\\204 \\354\\235\\264\\354\\232\\251\\355\\225\\234 \\352\\263\\240\\354\\234\\240\\352\\260\\222\\352\\263\\274 \\352\\263\\240\\354\\234\\240\\353\\262\\241\\355\\204\\260 \\352\\265\\254\\355\\225\\230\\352\\270\\260.ipynb\"\n create mode 100644 \"posts/2021-12-31-\\354\\204\\240\\355\\230\\225\\353\\214\\200\\354\\210\\230_\\352\\263\\240\\354\\234\\240\\352\\260\\222_\\352\\263\\240\\354\\234\\240\\353\\262\\241\\355\\204\\260.ipynb\"\n create mode 100644 \"posts/2022-01-12-\\355\\232\\214\\352\\267\\200\\353\\266\\204\\354\\204\\235_\\353\\213\\250\\354\\210\\234\\354\\204\\240\\355\\230\\225\\355\\232\\214\\352\\267\\200\\353\\266\\204\\354\\204\\235.ipynb\"\n create mode 100644 \"posts/2022-01-13-\\355\\232\\214\\352\\267\\200\\353\\266\\204\\354\\204\\235_\\354\\244\\221\\355\\232\\214\\352\\267\\200\\353\\252\\250\\355\\230\\225.ipynb\"\n create mode 100644 \"posts/2022-01-14-\\355\\232\\214\\352\\267\\200\\353\\266\\204\\354\\204\\235-\\352\\267\\200\\354\\247\\204\\353\\213\\250.ipynb\"\n create mode 100644 \"posts/2022-01-14-\\355\\232\\214\\352\\267\\200\\353\\266\\204\\354\\204\\235-\\353\\263\\200\\354\\210\\230\\354\\204\\240\\355\\203\\235.ipynb\"\n create mode 100644 \"posts/2022-03-07-1\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.ipynb\"\n create mode 100644 \"posts/2022-03-14-2\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.ipynb\"\n create mode 100644 \"posts/2022-03-21-3\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.ipynb\"\n create mode 100644 \"posts/2022-03-28-4\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.ipynb\"\n create mode 100644 \"posts/2022-04-04-5\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.ipynb\"\n create mode 100644 \"posts/2022-04-11-6\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.ipynb\"\n create mode 100644 \"posts/2022-04-18-7\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.ipynb\"\n create mode 100644 \"posts/2022-04-28-\\354\\244\\221\\352\\260\\204\\352\\263\\240\\354\\202\\254\\354\\230\\210\\354\\203\\201\\353\\254\\270\\354\\240\\234_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.ipynb\"\n create mode 100644 \"posts/2022-04-29-\\354\\213\\254\\354\\236\\254\\354\\235\\270_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_\\354\\244\\221\\352\\260\\204\\352\\263\\240\\354\\202\\254.ipynb\"\n create mode 100644 \"posts/2022-04-30-\\354\\244\\221\\352\\260\\204\\352\\263\\240\\354\\202\\254\\355\\225\\264\\354\\204\\244.ipynb\"\n create mode 100644 \"posts/2022-05-02-9\\354\\243\\274\\354\\260\\250(1)_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.ipynb\"\n create mode 100644 \"posts/2022-05-03-9\\354\\243\\274\\354\\260\\250(2)_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.ipynb\"\n create mode 100644 \"posts/2022-05-09-10\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.ipynb\"\n create mode 100644 \"posts/2022-05-16-11\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.ipynb\"\n create mode 100644 \"posts/2022-05-23-12\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.ipynb\"\n create mode 100644 \"posts/2022-06-13-\\352\\270\\260\\353\\247\\220\\352\\263\\240\\354\\202\\254_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.ipynb\"\n create mode 100644 \"posts/2022-06-30-\\352\\270\\260\\353\\247\\220\\352\\263\\240\\354\\202\\254\\354\\230\\210\\354\\203\\201\\353\\254\\270\\354\\240\\234_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.ipynb\"\n create mode 100644 \"posts/2022.09.02_\\353\\260\\234\\355\\221\\234\\354\\236\\220\\353\\243\\214.ipynb\"\n create mode 100644 posts/Untitled.ipynb\n create mode 100644 posts/Untitled1.ipynb\n delete mode 100644 posts/_metadata.yml\n delete mode 100644 posts/post-with-code/image.jpg\n delete mode 100644 posts/post-with-code/index.qmd\n delete mode 100644 posts/welcome/index.qmd\n delete mode 100644 posts/welcome/thumbnail.jpg\nEnumerating objects: 33, done.\nCounting objects: 100% (33/33), done.\nDelta compression using up to 16 threads\nCompressing objects: 100% (31/31), done.\nWriting objects: 100% (31/31), 13.61 MiB | 2.40 MiB/s, done.\nTotal 31 (delta 14), reused 0 (delta 0)\nremote: Resolving deltas: 100% (14/14), completed with 1 local object.\nTo https://github.com/simjaein/jisim12.git\n   4a62e16..d5e6fe8  main -> main\nFrom https://github.com/simjaein/jisim12\n * branch            gh-pages   -> FETCH_HEAD\nRendering for publish:\n\n[ 1/29] about.qmd\n[ 2/29] posts/2022-03-28-4주차_빅데이터분석특강.ipynb\n[WARNING] This document format requires a nonempty <title> element.\n  Defaulting to 'quarto-input051a285a' as the title.\n  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n[ 3/29] posts/2022-04-04-5주차_빅데이터분석특강.ipynb\n[WARNING] This document format requires a nonempty <title> element.\n  Defaulting to 'quarto-input9de28635' as the title.\n  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n[ 4/29] posts/Untitled1.ipynb\n[ 5/29] posts/2022-03-14-2주차_빅데이터분석특강.ipynb\n[WARNING] This document format requires a nonempty <title> element.\n  Defaulting to 'quarto-input4c72b3b4' as the title.\n  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n[ 6/29] posts/2021-12-31-선형대수_고유값_고유벡터.ipynb\n[WARNING] This document format requires a nonempty <title> element.\n  Defaulting to 'quarto-inputf34e1eee' as the title.\n  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n[ 7/29] posts/2022-01-14-회귀분석-귀진단.ipynb\n[WARNING] This document format requires a nonempty <title> element.\n  Defaulting to 'quarto-input0ad48c59' as the title.\n  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n[ 8/29] posts/2022-05-03-9주차(2)_빅데이터분석특강.ipynb\n[WARNING] This document format requires a nonempty <title> element.\n  Defaulting to 'quarto-input7a11f1a1' as the title.\n  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n[ 9/29] posts/2022-01-12-회귀분석_단순선형회귀분석.ipynb\n[WARNING] This document format requires a nonempty <title> element.\n  Defaulting to 'quarto-input2005f1be' as the title.\n  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n[10/29] posts/2022.09.02_발표자료.ipynb\n[WARNING] This document format requires a nonempty <title> element.\n  Defaulting to 'quarto-inpute52e4848' as the title.\n  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n[11/29] posts/2022-03-21-3주차_빅데이터분석특강.ipynb\n[WARNING] This document format requires a nonempty <title> element.\n  Defaulting to 'quarto-input21673e78' as the title.\n  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n[12/29] posts/2022-01-14-회귀분석-변수선택.ipynb\n[WARNING] This document format requires a nonempty <title> element.\n  Defaulting to 'quarto-input3f8e1eb9' as the title.\n  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n[13/29] posts/2022-04-30-중간고사해설.ipynb\n[WARNING] This document format requires a nonempty <title> element.\n  Defaulting to 'quarto-inputb49c858a' as the title.\n  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n[14/29] posts/2021-12-20-데이터시각화_기말고사풀이.ipynb\n[WARNING] This document format requires a nonempty <title> element.\n  Defaulting to 'quarto-inpute16c2df1' as the title.\n  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n[15/29] posts/2022-06-30-기말고사예상문제_빅데이터분석특강.ipynb\n[WARNING] This document format requires a nonempty <title> element.\n  Defaulting to 'quarto-inputc12b2836' as the title.\n  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n[16/29] posts/2022-01-13-회귀분석_중회귀모형.ipynb\n[WARNING] This document format requires a nonempty <title> element.\n  Defaulting to 'quarto-input3cd0b6f5' as the title.\n  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n[17/29] posts/2022-04-11-6주차_빅데이터분석특강.ipynb\n[WARNING] This document format requires a nonempty <title> element.\n  Defaulting to 'quarto-inputbfeb3cab' as the title.\n  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n[18/29] posts/2022-05-09-10주차_빅데이터분석특강.ipynb\n[WARNING] This document format requires a nonempty <title> element.\n  Defaulting to 'quarto-inputb1d6ec0f' as the title.\n  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n[19/29] posts/2022-04-18-7주차_빅데이터분석특강.ipynb\n[WARNING] This document format requires a nonempty <title> element.\n  Defaulting to 'quarto-input9e37fe8f' as the title.\n  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n[20/29] posts/2022-04-28-중간고사예상문제_빅데이터분석특강.ipynb\n[WARNING] This document format requires a nonempty <title> element.\n  Defaulting to 'quarto-inputbf67bd41' as the title.\n  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n[21/29] posts/2022-04-29-심재인_빅데이터분석특강_중간고사.ipynb\n[WARNING] This document format requires a nonempty <title> element.\n  Defaulting to 'quarto-input1393595f' as the title.\n  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n[22/29] posts/2022-05-23-12주차_빅데이터분석특강.ipynb\n[WARNING] This document format requires a nonempty <title> element.\n  Defaulting to 'quarto-inputc222a989' as the title.\n  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n[23/29] posts/Untitled.ipynb\n[24/29] posts/2022-05-02-9주차(1)_빅데이터분석특강.ipynb\n[WARNING] This document format requires a nonempty <title> element.\n  Defaulting to 'quarto-inputc6ad2276' as the title.\n  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n[25/29] posts/2022-03-07-1주차_빅데이터분석특강.ipynb\n[WARNING] This document format requires a nonempty <title> element.\n  Defaulting to 'quarto-input8c6fd22a' as the title.\n  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n[26/29] posts/2022-05-16-11주차_빅데이터분석특강.ipynb\n[WARNING] This document format requires a nonempty <title> element.\n  Defaulting to 'quarto-input9400628b' as the title.\n  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n[27/29] posts/2022-06-13-기말고사_빅데이터분석특강.ipynb\n[WARNING] This document format requires a nonempty <title> element.\n  Defaulting to 'quarto-input43fb1a9b' as the title.\n  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n[28/29] posts/2021-12-31-python을 이용한 고유값과 고유벡터 구하기.ipynb\n[29/29] index.qmd\nWARNING: File /home/jaein/Dropbox/jisim12/posts/2022-03-28-4주차_빅데이터분석특강.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/jaein/Dropbox/jisim12/posts/2022-04-04-5주차_빅데이터분석특강.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/jaein/Dropbox/jisim12/posts/Untitled1.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/jaein/Dropbox/jisim12/posts/2022-03-14-2주차_빅데이터분석특강.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/jaein/Dropbox/jisim12/posts/2021-12-31-선형대수_고유값_고유벡터.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/jaein/Dropbox/jisim12/posts/2022-01-14-회귀분석-귀진단.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/jaein/Dropbox/jisim12/posts/2022-05-03-9주차(2)_빅데이터분석특강.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/jaein/Dropbox/jisim12/posts/2022-01-12-회귀분석_단순선형회귀분석.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/jaein/Dropbox/jisim12/posts/2022.09.02_발표자료.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/jaein/Dropbox/jisim12/posts/2022-03-21-3주차_빅데이터분석특강.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/jaein/Dropbox/jisim12/posts/2022-01-14-회귀분석-변수선택.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/jaein/Dropbox/jisim12/posts/2022-04-30-중간고사해설.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/jaein/Dropbox/jisim12/posts/2021-12-20-데이터시각화_기말고사풀이.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/jaein/Dropbox/jisim12/posts/2022-06-30-기말고사예상문제_빅데이터분석특강.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/jaein/Dropbox/jisim12/posts/2022-01-13-회귀분석_중회귀모형.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/jaein/Dropbox/jisim12/posts/2022-04-11-6주차_빅데이터분석특강.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/jaein/Dropbox/jisim12/posts/2022-05-09-10주차_빅데이터분석특강.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/jaein/Dropbox/jisim12/posts/2022-04-18-7주차_빅데이터분석특강.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/jaein/Dropbox/jisim12/posts/2022-04-28-중간고사예상문제_빅데이터분석특강.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/jaein/Dropbox/jisim12/posts/2022-04-29-심재인_빅데이터분석특강_중간고사.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/jaein/Dropbox/jisim12/posts/2022-05-23-12주차_빅데이터분석특강.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/jaein/Dropbox/jisim12/posts/Untitled.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/jaein/Dropbox/jisim12/posts/2022-05-02-9주차(1)_빅데이터분석특강.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/jaein/Dropbox/jisim12/posts/2022-03-07-1주차_빅데이터분석특강.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/jaein/Dropbox/jisim12/posts/2022-05-16-11주차_빅데이터분석특강.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/jaein/Dropbox/jisim12/posts/2022-06-13-기말고사_빅데이터분석특강.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/jaein/Dropbox/jisim12/posts/2021-12-31-python을 이용한 고유값과 고유벡터 구하기.ipynb in the listing 'listing' contains no metadata.\n\nPreparing worktree (resetting branch 'gh-pages'; was at 648a522)\nBranch 'gh-pages' set up to track remote branch 'gh-pages' from 'origin'.\nHEAD is now at 648a522 Built site for gh-pages\n[gh-pages a06b556] Built site for gh-pages\n 104 files changed, 260781 insertions(+), 69 deletions(-)\n create mode 100644 \"posts/2021-12-20-\\353\\215\\260\\354\\235\\264\\355\\204\\260\\354\\213\\234\\352\\260\\201\\355\\231\\224_\\352\\270\\260\\353\\247\\220\\352\\263\\240\\354\\202\\254\\355\\222\\200\\354\\235\\264.html\"\n create mode 100644 \"posts/2021-12-31-python\\354\\235\\204 \\354\\235\\264\\354\\232\\251\\355\\225\\234 \\352\\263\\240\\354\\234\\240\\352\\260\\222\\352\\263\\274 \\352\\263\\240\\354\\234\\240\\353\\262\\241\\355\\204\\260 \\352\\265\\254\\355\\225\\230\\352\\270\\260.html\"\n create mode 100644 \"posts/2021-12-31-\\354\\204\\240\\355\\230\\225\\353\\214\\200\\354\\210\\230_\\352\\263\\240\\354\\234\\240\\352\\260\\222_\\352\\263\\240\\354\\234\\240\\353\\262\\241\\355\\204\\260.html\"\n create mode 100644 \"posts/2022-01-12-\\355\\232\\214\\352\\267\\200\\353\\266\\204\\354\\204\\235_\\353\\213\\250\\354\\210\\234\\354\\204\\240\\355\\230\\225\\355\\232\\214\\352\\267\\200\\353\\266\\204\\354\\204\\235.html\"\n create mode 100644 \"posts/2022-01-13-\\355\\232\\214\\352\\267\\200\\353\\266\\204\\354\\204\\235_\\354\\244\\221\\355\\232\\214\\352\\267\\200\\353\\252\\250\\355\\230\\225.html\"\n create mode 100644 \"posts/2022-01-14-\\355\\232\\214\\352\\267\\200\\353\\266\\204\\354\\204\\235-\\352\\267\\200\\354\\247\\204\\353\\213\\250.html\"\n create mode 100644 \"posts/2022-01-14-\\355\\232\\214\\352\\267\\200\\353\\266\\204\\354\\204\\235-\\353\\263\\200\\354\\210\\230\\354\\204\\240\\355\\203\\235.html\"\n create mode 100644 \"posts/2022-03-07-1\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.html\"\n create mode 100644 \"posts/2022-03-07-1\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-19-output-1.png\"\n create mode 100644 \"posts/2022-03-07-1\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-6-output-1.png\"\n create mode 100644 \"posts/2022-03-14-2\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.html\"\n create mode 100644 \"posts/2022-03-21-3\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.html\"\n create mode 100644 \"posts/2022-03-28-4\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.html\"\n create mode 100644 \"posts/2022-03-28-4\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-32-output-1.png\"\n create mode 100644 \"posts/2022-03-28-4\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-33-output-1.png\"\n create mode 100644 \"posts/2022-03-28-4\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-60-output-1.png\"\n create mode 100644 \"posts/2022-03-28-4\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-63-output-1.png\"\n create mode 100644 \"posts/2022-03-28-4\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-72-output-1.png\"\n create mode 100644 \"posts/2022-03-28-4\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-73-output-1.png\"\n create mode 100644 \"posts/2022-03-28-4\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-84-output-1.png\"\n create mode 100644 \"posts/2022-03-28-4\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-88-output-1.png\"\n create mode 100644 \"posts/2022-04-04-5\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.html\"\n create mode 100644 \"posts/2022-04-04-5\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-23-output-1.png\"\n create mode 100644 \"posts/2022-04-11-6\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.html\"\n create mode 100644 \"posts/2022-04-11-6\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-110-output-1.png\"\n create mode 100644 \"posts/2022-04-11-6\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-75-output-1.png\"\n create mode 100644 \"posts/2022-04-11-6\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-78-output-1.png\"\n create mode 100644 \"posts/2022-04-11-6\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-80-output-1.png\"\n create mode 100644 \"posts/2022-04-11-6\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-89-output-1.png\"\n create mode 100644 \"posts/2022-04-11-6\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-98-output-1.png\"\n create mode 100644 \"posts/2022-04-18-7\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.html\"\n create mode 100644 \"posts/2022-04-18-7\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-11-output-1.png\"\n create mode 100644 \"posts/2022-04-18-7\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-15-output-1.png\"\n create mode 100644 \"posts/2022-04-18-7\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-26-output-1.png\"\n create mode 100644 \"posts/2022-04-18-7\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-30-output-1.png\"\n create mode 100644 \"posts/2022-04-18-7\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-35-output-1.png\"\n create mode 100644 \"posts/2022-04-18-7\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-36-output-1.png\"\n create mode 100644 \"posts/2022-04-18-7\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-50-output-1.png\"\n create mode 100644 \"posts/2022-04-18-7\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-51-output-1.png\"\n create mode 100644 \"posts/2022-04-18-7\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-52-output-1.png\"\n create mode 100644 \"posts/2022-04-18-7\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-53-output-1.png\"\n create mode 100644 \"posts/2022-04-18-7\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-55-output-1.png\"\n create mode 100644 \"posts/2022-04-18-7\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-57-output-1.png\"\n create mode 100644 \"posts/2022-04-18-7\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-58-output-1.png\"\n create mode 100644 \"posts/2022-04-18-7\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-6-output-1.png\"\n create mode 100644 \"posts/2022-04-18-7\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-61-output-1.png\"\n create mode 100644 \"posts/2022-04-18-7\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-68-output-1.png\"\n create mode 100644 \"posts/2022-04-18-7\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-69-output-1.png\"\n create mode 100644 \"posts/2022-04-18-7\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-71-output-1.png\"\n create mode 100644 \"posts/2022-04-18-7\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-74-output-1.png\"\n create mode 100644 \"posts/2022-04-18-7\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-79-output-1.png\"\n create mode 100644 \"posts/2022-04-18-7\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-81-output-1.png\"\n create mode 100644 \"posts/2022-04-18-7\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-83-output-2.png\"\n create mode 100644 \"posts/2022-04-28-\\354\\244\\221\\352\\260\\204\\352\\263\\240\\354\\202\\254\\354\\230\\210\\354\\203\\201\\353\\254\\270\\354\\240\\234_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.html\"\n create mode 100644 \"posts/2022-04-28-\\354\\244\\221\\352\\260\\204\\352\\263\\240\\354\\202\\254\\354\\230\\210\\354\\203\\201\\353\\254\\270\\354\\240\\234_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-14-output-1.png\"\n create mode 100644 \"posts/2022-04-28-\\354\\244\\221\\352\\260\\204\\352\\263\\240\\354\\202\\254\\354\\230\\210\\354\\203\\201\\353\\254\\270\\354\\240\\234_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-22-output-1.png\"\n create mode 100644 \"posts/2022-04-28-\\354\\244\\221\\352\\260\\204\\352\\263\\240\\354\\202\\254\\354\\230\\210\\354\\203\\201\\353\\254\\270\\354\\240\\234_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-31-output-1.png\"\n create mode 100644 \"posts/2022-04-28-\\354\\244\\221\\352\\260\\204\\352\\263\\240\\354\\202\\254\\354\\230\\210\\354\\203\\201\\353\\254\\270\\354\\240\\234_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-39-output-1.png\"\n create mode 100644 \"posts/2022-04-28-\\354\\244\\221\\352\\260\\204\\352\\263\\240\\354\\202\\254\\354\\230\\210\\354\\203\\201\\353\\254\\270\\354\\240\\234_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-50-output-1.png\"\n create mode 100644 \"posts/2022-04-28-\\354\\244\\221\\352\\260\\204\\352\\263\\240\\354\\202\\254\\354\\230\\210\\354\\203\\201\\353\\254\\270\\354\\240\\234_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-51-output-1.png\"\n create mode 100644 \"posts/2022-04-28-\\354\\244\\221\\352\\260\\204\\352\\263\\240\\354\\202\\254\\354\\230\\210\\354\\203\\201\\353\\254\\270\\354\\240\\234_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-55-output-1.png\"\n create mode 100644 \"posts/2022-04-28-\\354\\244\\221\\352\\260\\204\\352\\263\\240\\354\\202\\254\\354\\230\\210\\354\\203\\201\\353\\254\\270\\354\\240\\234_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-60-output-1.png\"\n create mode 100644 \"posts/2022-04-29-\\354\\213\\254\\354\\236\\254\\354\\235\\270_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_\\354\\244\\221\\352\\260\\204\\352\\263\\240\\354\\202\\254.html\"\n create mode 100644 \"posts/2022-04-29-\\354\\213\\254\\354\\236\\254\\354\\235\\270_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_\\354\\244\\221\\352\\260\\204\\352\\263\\240\\354\\202\\254_files/figure-html/cell-15-output-1.png\"\n create mode 100644 \"posts/2022-04-29-\\354\\213\\254\\354\\236\\254\\354\\235\\270_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_\\354\\244\\221\\352\\260\\204\\352\\263\\240\\354\\202\\254_files/figure-html/cell-25-output-1.png\"\n create mode 100644 \"posts/2022-04-29-\\354\\213\\254\\354\\236\\254\\354\\235\\270_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_\\354\\244\\221\\352\\260\\204\\352\\263\\240\\354\\202\\254_files/figure-html/cell-33-output-1.png\"\n create mode 100644 \"posts/2022-04-29-\\354\\213\\254\\354\\236\\254\\354\\235\\270_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_\\354\\244\\221\\352\\260\\204\\352\\263\\240\\354\\202\\254_files/figure-html/cell-42-output-1.png\"\n create mode 100644 \"posts/2022-04-29-\\354\\213\\254\\354\\236\\254\\354\\235\\270_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_\\354\\244\\221\\352\\260\\204\\352\\263\\240\\354\\202\\254_files/figure-html/cell-50-output-1.png\"\n create mode 100644 \"posts/2022-04-29-\\354\\213\\254\\354\\236\\254tml/cell-33-output-1.png\"\n create mode 100644 \"posts/2022-05-02-9\\354\\243\\274\\354\\260\\250(1)_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.html\"\n create mode 100644 \"posts/2022-05-03-9\\354\\243\\274\\354\\260\\250(2)_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.html\"\n create mode 100644 \"posts/2022-05-03-9\\354\\243\\274\\354\\260\\250(2)_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-10-output-2.png\"\n create mode 100644 \"posts/2022-05-03-9\\354\\243\\274\\354\\260\\250(2)_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-14-output-2.png\"\n create mode 100644 \"posts/2022-05-03-9\\354\\243\\274\\354\\260\\250(2)_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-5-output-1.png\"\n create mode 100644 \"posts/2022-05-03-9\\354\\243\\274\\354\\260\\250(2)_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-6-output-1.png\"\n create mode 100644 \"posts/2022-05-09-10\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.html\"\n create mode 100644 \"posts/2022-05-09-10\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-12-output-1.svg\"\n create mode 100644 \"posts/2022-05-09-10\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-8-output-1.svg\"\n create mode 100644 \"posts/2022-05-16-11\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.html\"\n create mode 100644 \"posts/2022-05-23-12\\354\\243\\274\\354\\260\\250_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.html\"\n create mode 100644 \"posts/2022-06-13-\\352\\270\\260\\353\\247\\220\\352\\263\\240\\354\\202\\254_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.html\"\n create mode 100644 \"posts/2022-06-13-\\352\\270\\260\\353\\247\\220\\352\\263\\240\\354\\202\\254_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-37-output-1.png\"\n create mode 100644 \"posts/2022-06-13-\\352\\270\\260\\353\\247\\220\\352\\263\\240\\354\\202\\254_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-40-output-1.png\"\n create mode 100644 \"posts/2022-06-13-\\352\\270\\260\\353\\247\\220\\352\\263\\240\\354\\202\\254_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-6-output-1.svg\"\n create mode 100644 \"posts/2022-06-30-\\352\\270\\260\\353\\247\\220\\352\\263\\240\\354\\202\\254\\354\\230\\210\\354\\203\\201\\353\\254\\270\\354\\240\\234_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225.html\"\n create mode 100644 \"posts/2022-06-30-\\352\\270\\260\\353\\247\\220\\352\\263\\240\\354\\202\\254\\354\\230\\210\\354\\203\\201\\353\\254\\270\\354\\240\\234_\\353\\271\\205\\353\\215\\260\\354\\235\\264\\355\\204\\260\\353\\266\\204\\354\\204\\235\\355\\212\\271\\352\\260\\225_files/figure-html/cell-7-output-1.svg\"\n create mode 100644 \"posts/2022.09.02_\\353\\260\\234\\355\\221\\234\\354\\236\\220\\353\\243\\214.html\"\n create mode 100644 posts/Untitled.html\n create mode 100644 posts/Untitled1.html\n create mode 100644 robots.txt\n rewrite search.json (66%)\n create mode 100644 sitemap.xml\norigin  https://github.com/simjaein/jisim12.git (fetch)\norigin  https://github.com/simjaein/jisim12.git (push)\nTo https://github.com/simjaein/jisim12.git\n   648a522..a06b556  HEAD -> gh-pages\n\nNOTE: GitHub Pages sites use caching so you might need to click the refresh\nbutton within your web browser to see changes after deployment.\n\n[✓] Published to https://simjaein.github.io/jisim12/\n\nNOTE: GitHub Pages deployments normally take a few minutes (your site updates\nwill be visible once the deploy completes)"
  },
  {
    "objectID": "posts/2022-03-14-2주차_빅데이터분석특강.html",
    "href": "posts/2022-03-14-2주차_빅데이터분석특강.html",
    "title": "jisim12",
    "section": "",
    "text": "2주차-3월 14일\n\n빅데이터분석특강\n\n\ntoc:false\nbranch: master\nbadges: true\ncomments: true\nauthor: 심재인\n\n\nimport\n\nimport tensorflow as tf\nimport numpy as np\n\n\ntf.config.experimental.list_physical_devices('GPU')\n\n2022-04-25 14:46:52.378177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n\n\n\n\ntf.constant\n\n예비학습: 중첩리스트\n- 리스트\n\nlst = [1,2,4,5,6]\nlst\n\n[1, 2, 4, 5, 6]\n\n\n\nlst[1] # 두번쨰원소\n\n2\n\n\n\nlst[-1] # 마지막원소\n\n6\n\n\n- (2,2) matrix 느낌의 list\n\nlst= [[1,2],[3,4]]\nlst\n\n[[1, 2], [3, 4]]\n\n\n위를 아래와 같은 매트릭스로 생각할수 있다.\n1 2 \n3 4 \n\nprint(lst[0][0]) # (1,1) \nprint(lst[0][1]) # (1,2) \nprint(lst[1][0]) # (2,1) \nprint(lst[1][1]) # (2,2) \n\n1\n2\n3\n4\n\n\n- (4,1) matrix 느낌의 list\n\nlst=[[1],[2],[3],[4]] # (4,1) matrix = 길이가 4인 col-vector\nlst\n\n[[1], [2], [3], [4]]\n\n\n- (1,4) matrix 느낌의 list\n\nlst=[[1,2,3,4]] # (1,4) matrix = 길이가 4인 row-vector \nlst\n\n[[1, 2, 3, 4]]\n\n\n\n\n선언\n- 스칼라\n\ntf.constant(3.14)\n\n<tf.Tensor: shape=(), dtype=float32, numpy=3.14>\n\n\n\ntf.constant(3.14)+tf.constant(3.14)\n\n<tf.Tensor: shape=(), dtype=float32, numpy=6.28>\n\n\n- 벡터\n\n_vector=tf.constant([1,2,3])\n\n\n_vector[-1]\n\n<tf.Tensor: shape=(), dtype=int32, numpy=3>\n\n\n- 매트릭스\n\n_matrix= tf.constant([[1,0],[0,1]])\n_matrix\n\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 0],\n       [0, 1]], dtype=int32)>\n\n\n- array\n\ntf.constant([[[0,1,1],[1,2,-1]],[[0,1,2],[1,2,-1]]])\n\n<tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[ 0,  1,  1],\n        [ 1,  2, -1]],\n\n       [[ 0,  1,  2],\n        [ 1,  2, -1]]], dtype=int32)>\n\n\n\n\n타입\n\ntype(tf.constant(3.14))\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\n\n인덱싱\n\n_matrix = tf.constant([[1,2],[3,4]])\n_matrix\n\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4]], dtype=int32)>\n\n\n\n_matrix[0][0]\n\n<tf.Tensor: shape=(), dtype=int32, numpy=1>\n\n\n\n_matrix[0]\n\n<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>\n\n\n\n_matrix[0,:]\n\n<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>\n\n\n\n_matrix[:,0]\n\n<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 3], dtype=int32)>\n\n\n\n\ntf.constant는 불편하다.\n- 불편한점 1. 모든 원소가 같은 dtype을 가지고 있어야함. 2. 원소 수정이 불가능함. 3. 묵시적 형변환이 불가능하다.\n- 원소수정이 불가능함\n\na=tf.constant([1,22,33])\na\n\n<tf.Tensor: shape=(3,), dtype=int32, numpy=array([ 1, 22, 33], dtype=int32)>\n\n\n\na[0]=11\n\nTypeError: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment\n\n\n- 묵시적 형변환이 불가능하다\n\ntf.constant(1)+tf.constant(3.14)\n\nInvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:AddV2]\n\n\n\ntf.constant(1.0)+tf.constant(3.14)\n\n<tf.Tensor: shape=(), dtype=float32, numpy=4.1400003>\n\n\n- 같은 float도 안되는 경우가 있음\n\ntf.constant(1.0,dtype=tf.float64)\n\n<tf.Tensor: shape=(), dtype=float64, numpy=1.0>\n\n\n\ntf.constant(3.14)\n\n<tf.Tensor: shape=(), dtype=float32, numpy=3.14>\n\n\n\ntf.constant(1.0,dtype=tf.float64)+tf.constant(3.14)\n\nInvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:AddV2]\n\n\n\n\ntf.constant \\(\\to\\) 넘파이\n\nnp.array(tf.constant(1)) # 방법1\n\narray(1, dtype=int32)\n\n\n\na=tf.constant([3.14,-3.14])\ntype(a)\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\na.numpy()\n\narray([ 3.14, -3.14], dtype=float32)\n\n\n\n#### 연산\n\n- 더하기\n\na=tf.constant([1,2])\nb=tf.constant([3,4])\na+b\n\n<tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)>\n\n\n\ntf.add(a,b)\n\n<tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)>\n\n\n- 곱하기\n\na=tf.constant([[1,2],[3,4]])\nb=tf.constant([[5,6],[7,8]])\na*b\n\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[ 5, 12],\n       [21, 32]], dtype=int32)>\n\n\n\ntf.multiply(a,b)\n\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[ 5, 12],\n       [21, 32]], dtype=int32)>\n\n\n- 매트릭스의곱\n\na=tf.constant([[1,0],[0,1]]) # (2,2)\nb=tf.constant([[5],[7]]) # (2,1) \na@b\n\n<tf.Tensor: shape=(2, 1), dtype=int32, numpy=\narray([[5],\n       [7]], dtype=int32)>\n\n\n\ntf.matmul(a,b)\n\n<tf.Tensor: shape=(2, 1), dtype=int32, numpy=\narray([[5],\n       [7]], dtype=int32)>\n\n\n- 역행렬\n\na=tf.constant([[1,0],[0,2]])\na\n\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 0],\n       [0, 2]], dtype=int32)>\n\n\n\ntf.linalg.inv(a)\n\nInvalidArgumentError: Value for attr 'T' of int32 is not in the list of allowed values: double, float, half, complex64, complex128\n    ; NodeDef: {{node MatrixInverse}}; Op<name=MatrixInverse; signature=input:T -> output:T; attr=adjoint:bool,default=false; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]> [Op:MatrixInverse]\n\n\n\na=tf.constant([[1.0,0.0],[0.0,2.0]])\ntf.linalg.inv(a)\n\n<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[1. , 0. ],\n       [0. , 0.5]], dtype=float32)>\n\n\n- tf.linalg. + tab을 누르면 좋아보이는 연산들 많음\n\na=tf.constant([[1.0,2.0],[3.0,4.0]])\nprint(a)\ntf.linalg.det(a)\n\ntf.Tensor(\n[[1. 2.]\n [3. 4.]], shape=(2, 2), dtype=float32)\n\n\n<tf.Tensor: shape=(), dtype=float32, numpy=-2.0>\n\n\n\ntf.linalg.trace(a)\n\n<tf.Tensor: shape=(), dtype=float32, numpy=5.0>\n\n\n\n\n형태변환\n- 기본: tf.reshape() 를 이용\n\na=tf.constant([1,2,3,4])\na\n\n<tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>\n\n\n\ntf.reshape(a,(4,1))\n\n<tf.Tensor: shape=(4, 1), dtype=int32, numpy=\narray([[1],\n       [2],\n       [3],\n       [4]], dtype=int32)>\n\n\n\ntf.reshape(a,(2,2))\n\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4]], dtype=int32)>\n\n\n\ntf.reshape(a,(2,2,1))\n\n<tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy=\narray([[[1],\n        [2]],\n\n       [[3],\n        [4]]], dtype=int32)>\n\n\n- 다차원\n\na=tf.constant([1,2,3,4,5,6,7,8,9,10,11,12])\na\n\n<tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)>\n\n\n\ntf.reshape(a,(2,2,3))\n\n<tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]], dtype=int32)>\n\n\n\ntf.reshape(a,(4,3))\n\n<tf.Tensor: shape=(4, 3), dtype=int32, numpy=\narray([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 7,  8,  9],\n       [10, 11, 12]], dtype=int32)>\n\n\n- tf.resh\n\na=tf.constant([1,2,3,4,5,6,7,8,9,10,11,12])\na\n\n<tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)>\n\n\n\ntf.reshape(a,(4,-1))\n\n<tf.Tensor: shape=(4, 3), dtype=int32, numpy=\narray([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 7,  8,  9],\n       [10, 11, 12]], dtype=int32)>\n\n\n\ntf.reshape(a,(2,2,-1))\n\n<tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]], dtype=int32)>\n\n\n\nb=tf.reshape(a,(2,2,-1))\nb\n\n<tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]], dtype=int32)>\n\n\n\ntf.reshape(b,-1)\n\n<tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)>\n\n\n\n\n선언고급\n- 다른 자료형 (리스트나 넘파이)로 만들고 바꾸는것도 좋다.\n\nnp.diag([1,2,3,4])\n\narray([[1, 0, 0, 0],\n       [0, 2, 0, 0],\n       [0, 0, 3, 0],\n       [0, 0, 0, 4]])\n\n\n\ntf.constant(np.diag([1,2,3,4]))\n\n<tf.Tensor: shape=(4, 4), dtype=int64, numpy=\narray([[1, 0, 0, 0],\n       [0, 2, 0, 0],\n       [0, 0, 3, 0],\n       [0, 0, 0, 4]])>\n\n\n- tf.ones, tf.zeros\n\ntf.zeros([3,3])\n\n<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]], dtype=float32)>\n\n\n\ntf.reshape(tf.constant([0]*9),(3,3))\n\n<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\narray([[0, 0, 0],\n       [0, 0, 0],\n       [0, 0, 0]], dtype=int32)>\n\n\n- range(10)\n\na=range(0,12)\ntf.constant(a)\n\n<tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11], dtype=int32)>\n\n\n\ntf.constant(range(1,20,3))\n\n<tf.Tensor: shape=(7,), dtype=int32, numpy=array([ 1,  4,  7, 10, 13, 16, 19], dtype=int32)>\n\n\n- tf.linspace\n\ntf.linspace(0,1,10)\n\n<tf.Tensor: shape=(10,), dtype=float64, numpy=\narray([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ])>\n\n\n\n\ntf.concat\n- (2,1) concat (2,1) => (2,2) - 두번째 축이 바뀌었다. => axis=1\n\na=tf.constant([[1],[2]])\nb=tf.constant([[3],[4]])\na,b\n\n(<tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[1],\n        [2]], dtype=int32)>,\n <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[3],\n        [4]], dtype=int32)>)\n\n\n\ntf.concat([a,b],axis=1)\n\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 3],\n       [2, 4]], dtype=int32)>\n\n\n- (2,1) concat (2,1) => (4,1) - 첫번째 축이 바뀌었다. => axis=0\n\na=tf.constant([[1],[2]])\nb=tf.constant([[3],[4]])\na,b\n\n(<tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[1],\n        [2]], dtype=int32)>,\n <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[3],\n        [4]], dtype=int32)>)\n\n\n\ntf.concat([a,b],axis=0)\n\n<tf.Tensor: shape=(4, 1), dtype=int32, numpy=\narray([[1],\n       [2],\n       [3],\n       [4]], dtype=int32)>\n\n\n- (1,2) concat (1,2) => (2,2) - 첫번째 // axis=0\n\na=tf.constant([[1,2]])\nb=tf.constant([[3,4]])\n\n\ntf.concat([a,b],axis=0)\n\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4]], dtype=int32)>\n\n\n- (1,2) concat (1,2) => (1,4) - 첫번째 // axis=0\n- (2,3,4,5) concat (2,3,4,5) => (4,3,4,5) - 첫번째 // axis=0\n\na=tf.reshape(tf.constant(range(120)),(2,3,4,5))\nb=-a\n\n\ntf.concat([a,b],axis=0)\n\n<tf.Tensor: shape=(4, 3, 4, 5), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4],\n         [   5,    6,    7,    8,    9],\n         [  10,   11,   12,   13,   14],\n         [  15,   16,   17,   18,   19]],\n\n        [[  20,   21,   22,   23,   24],\n         [  25,   26,   27,   28,   29],\n         [  30,   31,   32,   33,   34],\n         [  35,   36,   37,   38,   39]],\n\n        [[  40,   41,   42,   43,   44],\n         [  45,   46,   47,   48,   49],\n         [  50,   51,   52,   53,   54],\n         [  55,   56,   57,   58,   59]]],\n\n\n       [[[  60,   61,   62,   63,   64],\n         [  65,   66,   67,   68,   69],\n         [  70,   71,   72,   73,   74],\n         [  75,   76,   77,   78,   79]],\n\n        [[  80,   81,   82,   83,   84],\n         [  85,   86,   87,   88,   89],\n         [  90,   91,   92,   93,   94],\n         [  95,   96,   97,   98,   99]],\n\n        [[ 100,  101,  102,  103,  104],\n         [ 105,  106,  107,  108,  109],\n         [ 110,  111,  112,  113,  114],\n         [ 115,  116,  117,  118,  119]]],\n\n\n       [[[   0,   -1,   -2,   -3,   -4],\n         [  -5,   -6,   -7,   -8,   -9],\n         [ -10,  -11,  -12,  -13,  -14],\n         [ -15,  -16,  -17,  -18,  -19]],\n\n        [[ -20,  -21,  -22,  -23,  -24],\n         [ -25,  -26,  -27,  -28,  -29],\n         [ -30,  -31,  -32,  -33,  -34],\n         [ -35,  -36,  -37,  -38,  -39]],\n\n        [[ -40,  -41,  -42,  -43,  -44],\n         [ -45,  -46,  -47,  -48,  -49],\n         [ -50,  -51,  -52,  -53,  -54],\n         [ -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[ -60,  -61,  -62,  -63,  -64],\n         [ -65,  -66,  -67,  -68,  -69],\n         [ -70,  -71,  -72,  -73,  -74],\n         [ -75,  -76,  -77,  -78,  -79]],\n\n        [[ -80,  -81,  -82,  -83,  -84],\n         [ -85,  -86,  -87,  -88,  -89],\n         [ -90,  -91,  -92,  -93,  -94],\n         [ -95,  -96,  -97,  -98,  -99]],\n\n        [[-100, -101, -102, -103, -104],\n         [-105, -106, -107, -108, -109],\n         [-110, -111, -112, -113, -114],\n         [-115, -116, -117, -118, -119]]]], dtype=int32)>\n\n\n- (2,3,4,5) concat (2,3,4,5) => (2,6,4,5) - 두번째 // axis=1\n\na=tf.reshape(tf.constant(range(120)),(2,3,4,5))\nb=-a\n\n\ntf.concat([a,b],axis=1)\n\n<tf.Tensor: shape=(2, 6, 4, 5), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4],\n         [   5,    6,    7,    8,    9],\n         [  10,   11,   12,   13,   14],\n         [  15,   16,   17,   18,   19]],\n\n        [[  20,   21,   22,   23,   24],\n         [  25,   26,   27,   28,   29],\n         [  30,   31,   32,   33,   34],\n         [  35,   36,   37,   38,   39]],\n\n        [[  40,   41,   42,   43,   44],\n         [  45,   46,   47,   48,   49],\n         [  50,   51,   52,   53,   54],\n         [  55,   56,   57,   58,   59]],\n\n        [[   0,   -1,   -2,   -3,   -4],\n         [  -5,   -6,   -7,   -8,   -9],\n         [ -10,  -11,  -12,  -13,  -14],\n         [ -15,  -16,  -17,  -18,  -19]],\n\n        [[ -20,  -21,  -22,  -23,  -24],\n         [ -25,  -26,  -27,  -28,  -29],\n         [ -30,  -31,  -32,  -33,  -34],\n         [ -35,  -36,  -37,  -38,  -39]],\n\n        [[ -40,  -41,  -42,  -43,  -44],\n         [ -45,  -46,  -47,  -48,  -49],\n         [ -50,  -51,  -52,  -53,  -54],\n         [ -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[  60,   61,   62,   63,   64],\n         [  65,   66,   67,   68,   69],\n         [  70,   71,   72,   73,   74],\n         [  75,   76,   77,   78,   79]],\n\n        [[  80,   81,   82,   83,   84],\n         [  85,   86,   87,   88,   89],\n         [  90,   91,   92,   93,   94],\n         [  95,   96,   97,   98,   99]],\n\n        [[ 100,  101,  102,  103,  104],\n         [ 105,  106,  107,  108,  109],\n         [ 110,  111,  112,  113,  114],\n         [ 115,  116,  117,  118,  119]],\n\n        [[ -60,  -61,  -62,  -63,  -64],\n         [ -65,  -66,  -67,  -68,  -69],\n         [ -70,  -71,  -72,  -73,  -74],\n         [ -75,  -76,  -77,  -78,  -79]],\n\n        [[ -80,  -81,  -82,  -83,  -84],\n         [ -85,  -86,  -87,  -88,  -89],\n         [ -90,  -91,  -92,  -93,  -94],\n         [ -95,  -96,  -97,  -98,  -99]],\n\n        [[-100, -101, -102, -103, -104],\n         [-105, -106, -107, -108, -109],\n         [-110, -111, -112, -113, -114],\n         [-115, -116, -117, -118, -119]]]], dtype=int32)>\n\n\n- (2,3,4,5) concat (2,3,4,5) => (2,3,8,5) - 세번째 // axis=2\n\na=tf.reshape(tf.constant(range(120)),(2,3,4,5))\nb=-a\n\n\ntf.concat([a,b],axis=2)\n\n<tf.Tensor: shape=(2, 3, 8, 5), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4],\n         [   5,    6,    7,    8,    9],\n         [  10,   11,   12,   13,   14],\n         [  15,   16,   17,   18,   19],\n         [   0,   -1,   -2,   -3,   -4],\n         [  -5,   -6,   -7,   -8,   -9],\n         [ -10,  -11,  -12,  -13,  -14],\n         [ -15,  -16,  -17,  -18,  -19]],\n\n        [[  20,   21,   22,   23,   24],\n         [  25,   26,   27,   28,   29],\n         [  30,   31,   32,   33,   34],\n         [  35,   36,   37,   38,   39],\n         [ -20,  -21,  -22,  -23,  -24],\n         [ -25,  -26,  -27,  -28,  -29],\n         [ -30,  -31,  -32,  -33,  -34],\n         [ -35,  -36,  -37,  -38,  -39]],\n\n        [[  40,   41,   42,   43,   44],\n         [  45,   46,   47,   48,   49],\n         [  50,   51,   52,   53,   54],\n         [  55,   56,   57,   58,   59],\n         [ -40,  -41,  -42,  -43,  -44],\n         [ -45,  -46,  -47,  -48,  -49],\n         [ -50,  -51,  -52,  -53,  -54],\n         [ -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[  60,   61,   62,   63,   64],\n         [  65,   66,   67,   68,   69],\n         [  70,   71,   72,   73,   74],\n         [  75,   76,   77,   78,   79],\n         [ -60,  -61,  -62,  -63,  -64],\n         [ -65,  -66,  -67,  -68,  -69],\n         [ -70,  -71,  -72,  -73,  -74],\n         [ -75,  -76,  -77,  -78,  -79]],\n\n        [[  80,   81,   82,   83,   84],\n         [  85,   86,   87,   88,   89],\n         [  90,   91,   92,   93,   94],\n         [  95,   96,   97,   98,   99],\n         [ -80,  -81,  -82,  -83,  -84],\n         [ -85,  -86,  -87,  -88,  -89],\n         [ -90,  -91,  -92,  -93,  -94],\n         [ -95,  -96,  -97,  -98,  -99]],\n\n        [[ 100,  101,  102,  103,  104],\n         [ 105,  106,  107,  108,  109],\n         [ 110,  111,  112,  113,  114],\n         [ 115,  116,  117,  118,  119],\n         [-100, -101, -102, -103, -104],\n         [-105, -106, -107, -108, -109],\n         [-110, -111, -112, -113, -114],\n         [-115, -116, -117, -118, -119]]]], dtype=int32)>\n\n\n- (2,3,4,5) concat (2,3,4,5) => (2,3,4,10) - 네번째 // axis=3 # 0,1,2,3 // -4 -3 -2 -1\n\na=tf.reshape(tf.constant(range(120)),(2,3,4,5))\nb=-a\n\n\ntf.concat([a,b],axis=-1)\n\n<tf.Tensor: shape=(2, 3, 4, 10), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4,    0,   -1,   -2,   -3,   -4],\n         [   5,    6,    7,    8,    9,   -5,   -6,   -7,   -8,   -9],\n         [  10,   11,   12,   13,   14,  -10,  -11,  -12,  -13,  -14],\n         [  15,   16,   17,   18,   19,  -15,  -16,  -17,  -18,  -19]],\n\n        [[  20,   21,   22,   23,   24,  -20,  -21,  -22,  -23,  -24],\n         [  25,   26,   27,   28,   29,  -25,  -26,  -27,  -28,  -29],\n         [  30,   31,   32,   33,   34,  -30,  -31,  -32,  -33,  -34],\n         [  35,   36,   37,   38,   39,  -35,  -36,  -37,  -38,  -39]],\n\n        [[  40,   41,   42,   43,   44,  -40,  -41,  -42,  -43,  -44],\n         [  45,   46,   47,   48,   49,  -45,  -46,  -47,  -48,  -49],\n         [  50,   51,   52,   53,   54,  -50,  -51,  -52,  -53,  -54],\n         [  55,   56,   57,   58,   59,  -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[  60,   61,   62,   63,   64,  -60,  -61,  -62,  -63,  -64],\n         [  65,   66,   67,   68,   69,  -65,  -66,  -67,  -68,  -69],\n         [  70,   71,   72,   73,   74,  -70,  -71,  -72,  -73,  -74],\n         [  75,   76,   77,   78,   79,  -75,  -76,  -77,  -78,  -79]],\n\n        [[  80,   81,   82,   83,   84,  -80,  -81,  -82,  -83,  -84],\n         [  85,   86,   87,   88,   89,  -85,  -86,  -87,  -88,  -89],\n         [  90,   91,   92,   93,   94,  -90,  -91,  -92,  -93,  -94],\n         [  95,   96,   97,   98,   99,  -95,  -96,  -97,  -98,  -99]],\n\n        [[ 100,  101,  102,  103,  104, -100, -101, -102, -103, -104],\n         [ 105,  106,  107,  108,  109, -105, -106, -107, -108, -109],\n         [ 110,  111,  112,  113,  114, -110, -111, -112, -113, -114],\n         [ 115,  116,  117,  118,  119, -115, -116, -117, -118, -119]]]],\n      dtype=int32)>\n\n\n- (4,) concat (4,) => (8,) - 첫번째축? // axis=0\n\na=tf.constant([1,2,3,4])\nb=-a \na,b\n\n(<tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>,\n <tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)>)\n\n\n\ntf.concat([a,b],axis=0)\n\n<tf.Tensor: shape=(8,), dtype=int32, numpy=array([ 1,  2,  3,  4, -1, -2, -3, -4], dtype=int32)>\n\n\n- (4,) concat (4,) => (4,2) - 두번째축? // axis=1 ==> 이런거없다..\n\na=tf.constant([1,2,3,4])\nb=-a \na,b\n\n(<tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>,\n <tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)>)\n\n\n\ntf.concat([a,b],axis=1)\n\nInvalidArgumentError: ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1 [Op:ConcatV2] name: concat\n\n\n\n\ntf.stack\n\na=tf.constant([1,2,3,4])\nb=-a \na,b\n\n(<tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>,\n <tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)>)\n\n\n\ntf.stack([a,b],axis=0)\n\n<tf.Tensor: shape=(2, 4), dtype=int32, numpy=\narray([[ 1,  2,  3,  4],\n       [-1, -2, -3, -4]], dtype=int32)>\n\n\n\ntf.stack([a,b],axis=1)\n\n<tf.Tensor: shape=(4, 2), dtype=int32, numpy=\narray([[ 1, -1],\n       [ 2, -2],\n       [ 3, -3],\n       [ 4, -4]], dtype=int32)>\n\n\n\n\n\ntnp\n- tf는 넘파이에 비하여 텐서만들기가 너무힘듬\n\nnp.diag([1,2,3]).reshape(-1)\n\narray([1, 0, 0, 0, 2, 0, 0, 0, 3])\n\n\n\n넘파이는 이런식으로 np.diag()도 쓸수 있고 reshape을 메소드로 쓸 수도 있는데… #### tnp 사용방법 (불만해결방법)\n\n\nimport tensorflow.experimental.numpy as tnp\ntnp.experimental_enable_numpy_behavior()\n\n\ntype(tnp.array([1,2,3]))\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n- int와 float을 더할 수 있음\n\ntnp.array([1,2,3])+tnp.array([1.0,2.0,3.0])\n\n<tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 6.])>\n\n\n\ntf.constant([1,2,3])+tf.constant([1.0,2.0,3.0])\n\n<tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 6.])>\n\n\n\ntnp.array(1)+tnp.array([1.0,2.0,3.0])\n\n<tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 3., 4.])>\n\n\n\ntnp.diag([1,2,3])\n\n<tf.Tensor: shape=(3, 3), dtype=int64, numpy=\narray([[1, 0, 0],\n       [0, 2, 0],\n       [0, 0, 3]])>\n\n\n\na=tnp.diag([1,2,3])\ntype(a)\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\na=tf.constant([1,2,3])\na.reshape(3,1)\n\n<tf.Tensor: shape=(3, 1), dtype=int32, numpy=\narray([[1],\n       [2],\n       [3]], dtype=int32)>\n\n\n\n선언고급\n\nnp.random.randn(5)\n\narray([-0.26554076,  0.93945129, -1.12311997, -0.89946852, -0.72582148])\n\n\n\ntnp.random.randn(5) # 넘파이가 되면 나도 된다.\n\n<tf.Tensor: shape=(5,), dtype=float64, numpy=array([-0.00898484,  0.31565541,  1.12455947,  0.44374772,  0.77793125])>\n\n\n\n\n타입\n\ntype(tnp.random.randn(5))\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\n\ntf.contant로 만들어도 마치 넘파이인듯 쓰는 기능들\n- 묵시적형변환이 가능\n\ntf.constant([1,1])+tf.constant([2.2,3.3])\n\n<tf.Tensor: shape=(2,), dtype=float64, numpy=array([3.20000005, 4.29999995])>\n\n\n- 메소드를 쓸수 있음.\n\na= tnp.array([[1,2,3,4]])\na.T\n\n<tf.Tensor: shape=(4, 1), dtype=int64, numpy=\narray([[1],\n       [2],\n       [3],\n       [4]])>\n\n\n\n\n그렇지만 np.array는 아님\n- 원소를 할당하는것은 불가능\n\na=tf.constant([1,2,3])\na\n\n<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)>\n\n\n\na[0]=11\n\nTypeError: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment"
  },
  {
    "objectID": "posts/2021-12-31-선형대수_고유값_고유벡터.html",
    "href": "posts/2021-12-31-선형대수_고유값_고유벡터.html",
    "title": "jisim12",
    "section": "",
    "text": "선형대수 고유값 고유벡터\n\n선형대수학\n\n\ntoc:false\nbranch: master\nbadges: true\ncomments: true\nauthor: 심재인\n\n\n고유값, 고유벡터의 정의\n임의의 \\(n \\times n\\) 행렬 \\({\\bf A}\\)에 대하여, 0이 아닌 솔루션 벡터 \\(\\vec{x}\\) 가 존재한다면 숫자 \\(\\lambda\\) 는 행렬 \\({\\bf A}\\)의 고유값이라고 할 수 있다.\n\\({\\bf A}\\vec{x}\\) = \\(\\lambda\\vec{x}\\)\n이 때, 솔루션 벡터 \\(\\vec{x}\\) 는 고유값 \\(\\lambda\\) 에 대응하는 고유벡터이다.\n[주의사항1] 영벡터는 고유벡터로 보지 않는다.\n\\({\\bf A} \\begin{bmatrix} 0 \\\\ 0 \\\\ \\dots \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\dots \\\\ 0\\end{bmatrix}\\)\n즉, \\(\\vec{x}\\) = 영벡터라면 \\({\\bf A}\\vec{x}\\)도 영벡터이다.\n\n\\({\\bf A}\\vec{x} = \\vec{x} = 1\\cdot\\vec{x} = 2\\vec{x} = 3\\vec{x} = \\dots\\) 모든 실수가 고유값임.\n\n[주의사항2] 고유벡터는 무수히 많다.\n- \\(\\vec{x}\\) : \\(\\lambda\\) 에 대한 고유벡터 \\(\\Longrightarrow\\) \\(k\\vec{x}\\) : \\(\\lambda\\) 에 대한 고유벡터 - \\(2\\vec{x}, 3\\vec{x}, -\\vec{x},\\dots\\) 역시 모두 고유벡터\n- \\({\\bf A}\\vec{x} = \\lambda\\vec{x}\\) \\(\\Longrightarrow A(2\\vec{x}) = 2(A\\vec{x}) = 2\\lambda\\vec{x} = \\lambda(2\\vec{x})\\) - \\(\\because 2\\vec{x}\\) 도 \\(\\lambda\\) 에 대한 고유벡터\n\n\\(\\because\\)\n\\(\\therefore\\)\n\n\n\n[고유값 구하기]\nex) \\({\\bf A}\\) = \\(\\begin{bmatrix} 0&1\\\\ 1&0\\\\ \\end{bmatrix}\\) #### 1) 고유값 구하기 \\(|{\\bf A}-\\lambda {\\bf I}| = 0\\)의 해\n\\(\\lambda^2 - 1 = 0\\)\n\\(\\because \\lambda = \\pm 1\\)\n\\(\\because\\) \\(1\\),\\(-1\\)이 \\({\\bf A}\\)의 고유값\n\n\n[고유벡터 구하기] \\(\\leftarrow\\) 연립일차방정식 \\(({\\bf A}-\\lambda{\\bf I})\\vec{x}=\\vec{0}\\)의 해\n(i) \\(\\lambda=1\\)에 대한 고유벡터 : $ =\n\\[\\begin{bmatrix} t\\\\ t\\end{bmatrix}\\]\n,t $\n\\({\\bf A} -\\lambda{\\bf I} = \\begin{bmatrix} -1&1\\\\ 1&-1 \\end{bmatrix}\\)\n\\({\\bf A} = \\begin{bmatrix} 0&1\\\\ 1&0 \\end{bmatrix}\\)\n\\({\\bf A} = \\begin{bmatrix} 0&1|0\\\\ 1&0|0\\\\ \\end{bmatrix} \\sim \\begin{bmatrix} 0&0|0\\\\ 1&-1|0 \\end{bmatrix} \\sim \\begin{bmatrix} 1&-1|0\\\\ 0&0|0\\\\ \\end{bmatrix}\\)\n\n\\(\\vec{x} - \\vec{y} = 0\\)\n\n\n\\(\\because \\begin{cases} x= t \\\\ y=t \\end{cases}\\)\n\n(ii) \\(\\lambda\\) = -1에 대한 고유벡터 : \\(\\vec{x} = \\begin{bmatrix} s\\\\ -s \\end{bmatrix}\\) (\\(s \\neq 0\\) )\n\\({\\bf A}-\\lambda {\\bf I}=\\begin{bmatrix} 1&1\\\\ 1&1 \\end{bmatrix}\\)\n\\({\\bf A} = \\begin{bmatrix} 1&1|0\\\\ 1&1|0 \\end{bmatrix} \\sim \\begin{bmatrix} 1&1|0\\\\ 0&0|0 \\end{bmatrix}\\)\n\n\\(\\because x + y = 0\\)\n\n\n\\(\\because \\begin{cases} x= s \\\\ y=-s \\end{cases}\\)"
  },
  {
    "objectID": "posts/2022-01-14-회귀분석-귀진단.html",
    "href": "posts/2022-01-14-회귀분석-귀진단.html",
    "title": "jisim12",
    "section": "",
    "text": "회귀분석\n\n\ntoc:false\nbranch: master\nbadges: true\ncomments: true\nauthor: 심재인\n\n\n\n\n오차항의 가정 검토\n\n적절한 모형의 선택\n\n독립변수들간의 상관관계 검토\n\n이상치(outlier) 확인\n영향을 크게 주는 측정값 (influential observation)\n## Hat matrix\n잔차\n\\(e=Y-\\hat Y=Y-X\\hat{\\beta}\\)\n\\(\\qquad\\qquad = Y-X(X^TX)^{-1}X^{T}Y = [I-X(X^TX)^{-1}X^T]Y\\)\n\n\\(E(e)=0\\)\n\n\\(Var(e) = [I-X(X^TX)^{-1}X^T]\\sigma^2\\)\n\n\nmatrix\n\nHat matrix : \\(H = X(X^TX)^{-1}X^T, n\\times n\\) matrix\n\\(h_{ij} = X_{i}^T(X^TX)^{-1}x_{j},\\) for \\(i,j = 1,\\dots,n,\\)\nwhere \\(x_i = (1,x_{i1},x_{i2},\\dots,x_{ip})\\)\n\\(\\Rightarrow E(e_i) = 0,\\quad Var(e_i) = (1-h_{ii})\\sigma^2\\)\n\n\\(tr(H) = p+1,\\;0\\leq h_{ii}<1\\)\n\n\\(p=1 : h_{ii} = \\frac{1}{n}+\\frac{(x_{i}-\\overline x)^2}{S_{xx}}\\)\n\n\\(p>1 : h_{ii}=\\frac{1}{n}+(x_{i1}-\\overline x_1,\\dots,x_{ip}-\\overline x_p)S^{-1}\\) \\(\\begin{pmatrix} x_{i1} - {\\overline x_{1}} \\\\ \\vdots \\\\ x_{ip} - {\\overline x_{p}}\\\\  \\end{pmatrix}\\)\n\n\n\n\n\n\nModel : \\(Y=X\\beta+\\epsilon,\\epsilon_i\\sim_{iid}N(0,\\sigma^2)\\)\n\\(\\qquad y_i=\\beta_0+\\beta_1x_{i1}+\\dots+\\beta_px_{ip}+\\epsilon_i\\)\nAssumption\n\n(linearity) \\(E(y|x_i,\\dots,x_p) = \\beta_0+\\beta_1 x_1+\\dots+\\beta_p x_p\\)\n\n(homogeneous variance) \\(Var(\\epsilon_1) = \\dots = Var(\\epsilon_n) =\\sigma^2\\)\n\n(normality) \\(\\epsilon_i,\\sim N(0,\\sigma^2)\\)\n\n(independent) \\(\\epsilon_1,\\dots,\\epsilon_n\\) : independent\n\n\n\n\n\n\n잔차(residual) : \\(\\hat{e_i} = y_i-\\hat{y_i}\\)\n\n잔차의 분산 : \\(Var(Y_i-\\hat Y) = \\sigma^2(1-h_{ii})\\)\n\n스튜던트화 잔차 \\(\\hat{e_{st,i}} = \\frac{\\hat{e}_i}{\\hat{sd(\\hat{e}_i)}} = \\frac{y_i-\\hat{y_i}}{\\sqrt{MSE(1-h_{ii})}}\\)\n\n잔차도(residual plot) : \\((x_1,\\hat{e_{st,i}}),\\dots,(x_n,\\hat{e_{st,n}})\\) 의 산점도\n\n\n\n\n\n\n잔차분석\n\n대략 0에 관하여 대칭적으로 나타나고\\(\\qquad\\qquad\\qquad\\) (선형성)\n\n설명변수의 값에 따른 잔차의 산포가 크게 다르지 않고\\(\\quad\\;\\) (등분산성)\n\n점들이 특정한 형식을 가지고 나타남이 없으며\\(\\qquad\\qquad\\) (독립성)\n\n거의 모든 점이 $\\(2(3)의 범위내에 나타나야한다\\)$ (정규성)\n\n\n\n\n\n\n표준화잔차\n\n내적으로 스튜던트화된 잔차 (internally studentized residual)\n\\[r_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}\\]\n\n외적으로 스튜던트화된 잔차 (externally studentized residual)\n\\[r_i^* = \\frac{e_i}{\\hat{\\sigma_{(i)}}\\sqrt{1-h_{ii}}}\\]\n단, \\(\\hat{\\sigma_{(i)}} = \\Big[(n-p-1)\\hat{\\sigma}^2-\\frac{e_i^2}{1-h_{ii}}\\Big] / (n-p-2)\\)\n\n\n\n\n\n\n영향점 : 회귀분석의 결과가 몇 개의 특정값에 의해 크게 영향을 받는 경우, 영향점이 있다고 말함\n\n영향점을 검출하는 방법\n\n\n\n행렬 \\(H\\)의 대각원소 :\n\\(\\quad\\hat Y = X(X^TX)^{-1}X^TY = HY\\)\n\\(\\quad\\Rightarrow Var(\\hat Y) = h_{ii}\\sigma^2\\)\n\\(\\quad\\Rightarrow h_{ii}\\geq 2(p+1)/n\\) : 영향점으로 판단\n\nDFFITS (Difference if Fits)\n\\[DFFITS(i) = \\frac{\\hat{y}_i-\\hat{y}_i(i)}{\\hat{\\sigma}_{(i)}\\sqrt{h_{ii}}}\\]\n> \\(\\hat{y}_j(i) : i\\)번쨰 데이터를 제외시키고 \\(n-1\\)개 데이터에서 얻은 예측값\n> \\(|DFFITS(i)|\\geq 2\\sqrt{\\frac{p+1}{n-p-1}} \\Rightarrow\\) 영향점\nCook’s Distance\n\\[C_i = \\frac{\\sum_{j=1}^n(\\hat{y}_j - \\hat{y}_j(i))^2}{(p+1)\\hat{\\sigma}^2}\\]\n> \\(C_i = \\frac{h_{ii}}{1-h_{ii}}\\cdot\\frac{r_{i}^2}{p+1}\\)\n> \\(C_i\\geq F_{0.5}(p+1,n-p-1)\\) 또는 \\(C_i\\geq 1\\Rightarrow\\) 영향점"
  },
  {
    "objectID": "posts/2022-05-03-9주차(2)_빅데이터분석특강.html",
    "href": "posts/2022-05-03-9주차(2)_빅데이터분석특강.html",
    "title": "jisim12",
    "section": "",
    "text": "빅데이터분석특강\n\n\ntoc:false\nbranch: master\nbadges: true\ncomments: true\nauthor: 심재인\n\n\n\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow.experimental.numpy as tnp\n\n\ntf.config.experimental.list_physical_devices()\n\n2022-05-09 16:55:02.512547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+ s + ';}')\n\n\n\n\n\n\n- 특이한모형: 오버핏이 일어날 수 없는 모형이다. - 유의미한 coef: 상수항(bias), \\(\\cos(t)\\)의 계수, \\(\\cos(2t)\\)의 계수, \\(\\cos(5t)\\)의 계수. - 유의미하지 않은 coef: \\(\\cos(3t)\\)의 계수, \\(\\cos(4t)\\)의 계수 - 유의미하지 않은 계수는 \\(n%\\)이 커질수록 0으로 추정된다 = \\(\\cos(3t)\\)와 \\(\\cos(5t)\\)는 사용자가 임의로 제외하지 않아도 결국 모형에서 알아서 제거된다 = overfit이 일어나지 않는다. 모형이 알아서 유의미한 변수만 뽑아서 fit하는 느낌\n- 3번문제는 overfit이 일어나지 않는다. 이러한 신기한 일이 일어나는 이유는 모든 설명변수가 직교하기 때문임. - 이런 모형의 장점: overfit이 일어날 위험이 없으므로 train/test로 나누어 학습할 이유가 없다. (샘플만 버리는 꼴, test에 빼둔 observation까지 모아서 학습해 \\(\\beta\\)를 좀 더 정확히 추론하는게 차라리 더 이득) - 이러한 모형에서 할일: 추정된 계수들이 0인지 아닌지만 test하면 된다. (이것을 유의성검정이라고 한다)\n- 직교기저의 예시 - 빨강과 파랑을 255,255만큼 섞으면 보라색이 된다. - 빨강과 파랑과 노랑을 각각 255,255,255만큼 섞으면 검은색이 된다. - 임의의 어떠한 색도 빨강,파랑,노랑의 조합으로 표현가능하다. 즉 \\(\\text{color}= \\text{red}*\\beta_1 + \\text{blue}*\\beta_2 + \\text{yellow}*\\beta_3\\) 이다. - (빨,파,노)는 색을 표현하는 basis이다. (적절한 \\(\\beta_1,\\beta_2,\\beta_3\\)을 구하기만 하면 임의의 색도 표현가능) - (빨,보,노)역시 색을 표현하는 basis라 볼 수 있다. (파란색이 필요할때 보라색-빨간색을 하면되니까) - (빨,보,검)역시 색을 표현하는 basis라 볼 수 있다. (파란색이 필요하면 보라색-빨간색을 하면되고, 노란색이 필요하면 검정색-보라색을 하면 되니까) - (빨,파,노)는 직교기저이다.\n- 3번에서 알아둘 것: (1) 직교기저의 개념 (추후 재설명) (2) 임의의 색을 표현하려면 3개의 basis가 필요함\n\n\n\n- 그림을 그려보자.\n\n_x= tf.constant(np.arange(1,10001)/10000)\n_y= tnp.random.randn(10000) + (0.5 + 2*_x)\nplt.plot(_x,_y,'.',alpha=0.1)\n\n\n\n\n- 저것 꼭 10000개 다 모아서 loss계산해야할까?\n\nplt.plot(_x,_y,'.',alpha=0.1)\nplt.plot(_x[::10],_y[::10],'.')\n\n\n\n\n- 대충 이정도만 모아서 해도 비슷하지 않을까? \\(\\to\\) 해보자!\n\n\n\n\n\n\n- 단순회귀분석에서 샘플 10개 관측: \\((x_1,y_1),\\dots,(x_{10},y_{10})\\).\n(epoch1) \\(loss=\\sum_{i=1}^{10}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n(epoch2) \\(loss=\\sum_{i=1}^{10}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n…\n\n\n\n(epoch1) - \\(loss=(y_1-\\beta_0-\\beta_1x_1)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=(y_2-\\beta_0-\\beta_1x_2)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - … - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n(epoch2) - \\(loss=(y_1-\\beta_0-\\beta_1x_1)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=(y_2-\\beta_0-\\beta_1x_2)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - … - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n…\n\n\n\n\\(m=3\\)이라고 하자.\n(epoch1) - \\(loss=\\sum_{i=1}^{3}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=\\sum_{i=4}^{6}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=\\sum_{i=7}^{9}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n(epoch2) - \\(loss=\\sum_{i=1}^{3}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=\\sum_{i=4}^{6}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=\\sum_{i=7}^{9}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n…\n\n\n\n\n\n- ver1: gradient descent, batch gradient descent\n- ver2: stochastic gradient descent\n- ver3: mini-batch gradient descent, mini-batch stochastic gradient descent\n\n\n\n- ver1: gradient descent\n- ver2: stochastic gradient descent with batch size = 1\n- ver3: stochastic gradient descent - https://www.deeplearningbook.org/contents/optimization.html, 알고리즘 8-1 참고.\nnote: 이렇게 많이 쓰는 이유? ver1,2는 사실상 없는 방법이므로\n\n\n\n\n- ver2,3에서 샘플을 셔플할 수도 있다.\n- ver3에서 일부 샘플이 학습에 참여 안하는 버전도 있다.\n- 개인적 생각: 크게3개정도만 알면 괜찮고 나머지는 그렇게 유의미하지 않아보인다.\n\n\n\n- 핵심개념 - 메모리사용량: ver1 > ver3 > ver2 - 계산속도: ver1 > ver3 > ver2 - local-min에 갇힘: ver1 > ver3 > ver2\n- 본질: GPU 메모리가 한정되어 있어서 ver1을 쓰지는 못한다. GPU 메모리를 가장 적게쓰는것은 ver2인데 이것은 너무 불안정하다.\n- 틀리진 않지만 어색한 블로그 정리 내용들 - 경사하강법은 종종 국소최소점에 갇히는 문제가 있다. 이를 해결하기 위해서 등장한 방법이 확률적 경사하강법이다. –> 영 틀린말은 아니지만 그걸 의도하고 만든건 아님 - 경사하강법은 계산시간이 오래걸린다. 계산을 빠르게 하기 위해서 등장한 방법이 확률적 경사하강법이다. –> 1회 업데이트는 빠르게 계산함. 하지만 그것이 최적의 \\(\\beta\\)를 빠르게 얻을 수 있다는 의미는 아님\n\n\n\n\n\n\n- tf.keras.datasets.fashion_mnist.load_data 의 리턴값 조사\n\ntf.keras.datasets.fashion_mnist.load_data??\n\n\nSignature: tf.keras.datasets.fashion_mnist.load_data()\nSource:   \n@keras_export('keras.datasets.fashion_mnist.load_data')\ndef load_data():\n  \"\"\"Loads the Fashion-MNIST dataset.\n  This is a dataset of 60,000 28x28 grayscale images of 10 fashion categories,\n  along with a test set of 10,000 images. This dataset can be used as\n  a drop-in replacement for MNIST.\n  The classes are:\n  | Label | Description |\n  |:-----:|-------------|\n  |   0   | T-shirt/top |\n  |   1   | Trouser     |\n  |   2   | Pullover    |\n  |   3   | Dress       |\n  |   4   | Coat        |\n  |   5   | Sandal      |\n  |   6   | Shirt       |\n  |   7   | Sneaker     |\n  |   8   | Bag         |\n  |   9   | Ankle boot  |\n  Returns:\n    Tuple of NumPy arrays: `(x_train, y_train), (x_test, y_test)`.\n  **x_train**: uint8 NumPy array of grayscale image data with shapes\n    `(60000, 28, 28)`, containing the training data.\n  **y_train**: uint8 NumPy array of labels (integers in range 0-9)\n    with shape `(60000,)` for the training data.\n  **x_test**: uint8 NumPy array of grayscale image data with shapes\n    (10000, 28, 28), containing the test data.\n  **y_test**: uint8 NumPy array of labels (integers in range 0-9)\n    with shape `(10000,)` for the test data.\n  Example:\n  ```python\n  (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n  assert x_train.shape == (60000, 28, 28)\n  assert x_test.shape == (10000, 28, 28)\n  assert y_train.shape == (60000,)\n  assert y_test.shape == (10000,)\n  ```\n  License:\n    The copyright for Fashion-MNIST is held by Zalando SE.\n    Fashion-MNIST is licensed under the [MIT license](\n    https://github.com/zalandoresearch/fashion-mnist/blob/master/LICENSE).\n  \"\"\"\n  dirname = os.path.join('datasets', 'fashion-mnist')\n  base = 'https://storage.googleapis.com/tensorflow/tf-keras-datasets/'\n  files = [\n      'train-labels-idx1-ubyte.gz', 'train-images-idx3-ubyte.gz',\n      't10k-labels-idx1-ubyte.gz', 't10k-images-idx3-ubyte.gz'\n  ]\n  paths = []\n  for fname in files:\n    paths.append(get_file(fname, origin=base + fname, cache_subdir=dirname))\n  with gzip.open(paths[0], 'rb') as lbpath:\n    y_train = np.frombuffer(lbpath.read(), np.uint8, offset=8)\n  with gzip.open(paths[1], 'rb') as imgpath:\n    x_train = np.frombuffer(\n        imgpath.read(), np.uint8, offset=16).reshape(len(y_train), 28, 28)\n  with gzip.open(paths[2], 'rb') as lbpath:\n    y_test = np.frombuffer(lbpath.read(), np.uint8, offset=8)\n  with gzip.open(paths[3], 'rb') as imgpath:\n    x_test = np.frombuffer(\n        imgpath.read(), np.uint8, offset=16).reshape(len(y_test), 28, 28)\n  return (x_train, y_train), (x_test, y_test)\nFile:      ~/anaconda3/envs/py310/lib/python3.10/site-packages/keras/datasets/fashion_mnist.py\nType:      function\n\n\n\n\n\n\n\n- tf.keras.datasets.fashion_mnist.load_data()를 이용한 데이터 생성\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n32768/29515 [=================================] - 0s 2us/step\n40960/29515 [=========================================] - 0s 2us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n26427392/26421880 [==============================] - 1s 0us/step\n26435584/26421880 [==============================] - 1s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n16384/5148 [===============================================================================================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n4423680/4422102 [==============================] - 0s 0us/step\n4431872/4422102 [==============================] - 0s 0us/step\n\n\n- 차원확인\n\nx_train.shape, y_train.shape, x_test.shape,y_test.shape\n\n((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))\n\n\n\n60000은 obs숫자인듯\n(28,28)은 28픽셀,28픽셀을 의미하는듯\ntrain/test는 6:1로 나눈것 같음\n\n- 첫번째 obs\n\nplt.imshow(x_train[0])\n\n<matplotlib.image.AxesImage at 0x7fe5d0202620>\n\n\n\n\n\n\ny_train[0]\n\n9\n\n\n\n첫번쨰 obs에 대응하는 라벨\n\n- 첫번째 obs와 동일한 라벨을 가지는 그림을 찾아보자.\n\nnp.where(y_train==9)\n\n(array([    0,    11,    15, ..., 59932, 59970, 59978]),)\n\n\n\ny_train[11]\n\n9\n\n\n\nplt.imshow(x_train[11])\n\n<matplotlib.image.AxesImage at 0x7fe5d00646a0>\n\n\n\n\n\n\n\n\n- \\({\\bf X}\\): (n,28,28)\n- \\({\\bf y}\\): (n,) , \\(y=0,1,2,3,\\dots,9\\)\n\n\n\n\n\n\n- y=0,1에 대응하는 이미지만 정리하자. (우리가 배운건 로지스틱이니까)\n\ny= y_train[(y_train==0) | (y_train==1)].reshape(-1,1)\nX= x_train[(y_train==0) | (y_train==1)].reshape(-1,784)\nyy= y_test[(y_test==0) | (y_test==1)].reshape(-1,1)\nXX= x_test[(y_test==0) | (y_test==1)].reshape(-1,784)\n\n\nX.shape, y.shape, XX.shape, yy.shape\n\n((12000, 784), (12000, 1), (2000, 784), (2000, 1))\n\n\n\n\n\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x1\"\n    \"x2\"\n    \"..\"\n    \"x784\"\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x1\" -> \"node1\"\n    \"x2\" -> \"node1\"\n    \"..\" -> \"node1\"\n    \n    \"x784\" -> \"node1\"\n    \"x1\" -> \"node2\"\n    \"x2\" -> \"node2\"\n    \"..\" -> \"node2\"\n    \"x784\" -> \"node2\"\n    \n    \"x1\" -> \"...\"\n    \"x2\" -> \"...\"\n    \"..\" -> \"...\"\n    \"x784\" -> \"...\"\n\n    \"x1\" -> \"node30\"\n    \"x2\" -> \"node30\"\n    \"..\" -> \"node30\"\n    \"x784\" -> \"node30\"\n\n\n    label = \"Layer 1: relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -> \"y\"\n    \"node2\" -> \"y\"\n    \"...\" -> \"y\"\n    \"node30\" -> \"y\"\n    label = \"Layer 2: sigmoid\"\n}\n''')\n\nExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\n\n\n<graphviz.sources.Source at 0x7fe56859de10>\n\n\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='sgd',loss=tf.losses.binary_crossentropy) \nnet.fit(X,y,epochs=100,batch_size=12000)\n\nEpoch 1/100\n1/1 [==============================] - 0s 492ms/step - loss: 220.9145\nEpoch 2/100\n1/1 [==============================] - 0s 3ms/step - loss: 6800.3174\nEpoch 3/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.7045\nEpoch 4/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.7012\nEpoch 5/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.7004\nEpoch 6/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6997\nEpoch 7/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6991\nEpoch 8/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6985\nEpoch 9/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6979\nEpoch 10/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6976\nEpoch 11/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6973\nEpoch 12/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6970\nEpoch 13/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6968\nEpoch 14/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6966\nEpoch 15/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6964\nEpoch 16/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6962\nEpoch 17/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6961\nEpoch 18/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6959\nEpoch 19/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6958\nEpoch 20/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6956\nEpoch 21/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6955\nEpoch 22/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6953\nEpoch 23/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6952\nEpoch 24/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6951\nEpoch 25/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6949\nEpoch 26/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6948\nEpoch 27/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6947\nEpoch 28/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6946\nEpoch 29/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6945\nEpoch 30/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6944\nEpoch 31/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6943\nEpoch 32/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6942\nEpoch 33/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6942\nEpoch 34/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6941\nEpoch 35/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6940\nEpoch 36/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6940\nEpoch 37/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6939\nEpoch 38/100\n1/1 [==============================] - 0s 4ms/step - loss: 0.6939\nEpoch 39/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6938\nEpoch 40/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6937\nEpoch 41/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6937\nEpoch 42/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6936\nEpoch 43/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6936\nEpoch 44/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6935\nEpoch 45/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6935\nEpoch 46/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6934\nEpoch 47/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6934\nEpoch 48/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6934\nEpoch 49/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6933\nEpoch 50/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6933\nEpoch 51/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6933\nEpoch 52/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6933\nEpoch 53/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6933\nEpoch 54/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6933\nEpoch 55/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6933\nEpoch 56/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6933\nEpoch 57/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6933\nEpoch 58/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6933\nEpoch 59/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6933\nEpoch 60/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6933\nEpoch 61/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6933\nEpoch 62/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6933\nEpoch 63/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6933\nEpoch 64/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6933\nEpoch 65/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6933\nEpoch 66/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6933\nEpoch 67/100\n1/1 [==============================] - 0s 5ms/step - loss: 0.6933\nEpoch 68/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6932\nEpoch 69/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6932\nEpoch 70/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6932\nEpoch 71/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6932\nEpoch 72/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6932\nEpoch 73/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6932\nEpoch 74/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6932\nEpoch 75/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6932\nEpoch 76/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6932\nEpoch 77/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6932\nEpoch 78/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6932\nEpoch 79/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6932\nEpoch 80/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6932\nEpoch 81/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6932\nEpoch 82/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6932\nEpoch 83/100\n1/1 [==============================] - 0s 4ms/step - loss: 0.6932\nEpoch 84/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6932\nEpoch 85/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6932\nEpoch 86/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6932\nEpoch 87/100\n1/1 [==============================] - 0s 2ms/step - loss: 0.6932\nEpoch 88/100\n1/1 [==============================] - 0s 2ms/step - loss: 0.6932\nEpoch 89/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6932\nEpoch 90/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6932\nEpoch 91/100\n1/1 [==============================] - 0s 2ms/step - loss: 0.6932\nEpoch 92/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6932\nEpoch 93/100\n1/1 [==============================] - 0s 2ms/step - loss: 0.6932\nEpoch 94/100\n1/1 [==============================] - 0s 2ms/step - loss: 0.6932\nEpoch 95/100\n1/1 [==============================] - 0s 4ms/step - loss: 0.6932\nEpoch 96/100\n1/1 [==============================] - 0s 2ms/step - loss: 0.6932\nEpoch 97/100\n1/1 [==============================] - 0s 2ms/step - loss: 0.6932\nEpoch 98/100\n1/1 [==============================] - 0s 2ms/step - loss: 0.6932\nEpoch 99/100\n1/1 [==============================] - 0s 5ms/step - loss: 0.6932\nEpoch 100/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6932\n\n\n<keras.callbacks.History at 0x7fe568414cd0>\n\n\n\nnp.mean((net(X)>0.5) == y)\n\n0.5000833333333333\n\n\n\nnp.mean((net(XX)>0.5) == yy)\n\n0.5\n\n\n\n\n\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='adam',loss=tf.losses.binary_crossentropy)\nnet.fit(X,y,epochs=100,batch_size=12000)\n\nEpoch 1/100\n1/1 [==============================] - 0s 154ms/step - loss: 220.9145\nEpoch 2/100\n1/1 [==============================] - 0s 3ms/step - loss: 88.9451\nEpoch 3/100\n1/1 [==============================] - 0s 4ms/step - loss: 7.5899\nEpoch 4/100\n1/1 [==============================] - 0s 3ms/step - loss: 33.7523\nEpoch 5/100\n1/1 [==============================] - 0s 3ms/step - loss: 40.2278\nEpoch 6/100\n1/1 [==============================] - 0s 3ms/step - loss: 28.9657\nEpoch 7/100\n1/1 [==============================] - 0s 3ms/step - loss: 16.5118\nEpoch 8/100\n1/1 [==============================] - 0s 3ms/step - loss: 9.4905\nEpoch 9/100\n1/1 [==============================] - 0s 3ms/step - loss: 6.2025\nEpoch 10/100\n1/1 [==============================] - 0s 3ms/step - loss: 5.2417\nEpoch 11/100\n1/1 [==============================] - 0s 3ms/step - loss: 5.5173\nEpoch 12/100\n1/1 [==============================] - 0s 3ms/step - loss: 6.5902\nEpoch 13/100\n1/1 [==============================] - 0s 3ms/step - loss: 7.8607\nEpoch 14/100\n1/1 [==============================] - 0s 3ms/step - loss: 8.5884\nEpoch 15/100\n1/1 [==============================] - 0s 3ms/step - loss: 8.3990\nEpoch 16/100\n1/1 [==============================] - 0s 3ms/step - loss: 7.4674\nEpoch 17/100\n1/1 [==============================] - 0s 3ms/step - loss: 6.2580\nEpoch 18/100\n1/1 [==============================] - 0s 3ms/step - loss: 5.1273\nEpoch 19/100\n1/1 [==============================] - 0s 3ms/step - loss: 4.2381\nEpoch 20/100\n1/1 [==============================] - 0s 3ms/step - loss: 3.6032\nEpoch 21/100\n1/1 [==============================] - 0s 3ms/step - loss: 3.1860\nEpoch 22/100\n1/1 [==============================] - 0s 3ms/step - loss: 2.9232\nEpoch 23/100\n1/1 [==============================] - 0s 3ms/step - loss: 2.7559\nEpoch 24/100\n1/1 [==============================] - 0s 3ms/step - loss: 2.6420\nEpoch 25/100\n1/1 [==============================] - 0s 3ms/step - loss: 2.5490\nEpoch 26/100\n1/1 [==============================] - 0s 3ms/step - loss: 2.4612\nEpoch 27/100\n1/1 [==============================] - 0s 3ms/step - loss: 2.3617\nEpoch 28/100\n1/1 [==============================] - 0s 3ms/step - loss: 2.2378\nEpoch 29/100\n1/1 [==============================] - 0s 3ms/step - loss: 2.0873\nEpoch 30/100\n1/1 [==============================] - 0s 3ms/step - loss: 1.9117\nEpoch 31/100\n1/1 [==============================] - 0s 3ms/step - loss: 1.7239\nEpoch 32/100\n1/1 [==============================] - 0s 3ms/step - loss: 1.5408\nEpoch 33/100\n1/1 [==============================] - 0s 3ms/step - loss: 1.3663\nEpoch 34/100\n1/1 [==============================] - 0s 3ms/step - loss: 1.2210\nEpoch 35/100\n1/1 [==============================] - 0s 3ms/step - loss: 1.1035\nEpoch 36/100\n1/1 [==============================] - 0s 3ms/step - loss: 1.0208\nEpoch 37/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.9766\nEpoch 38/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.9628\nEpoch 39/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.9717\nEpoch 40/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.9883\nEpoch 41/100\n1/1 [==============================] - 0s 3ms/step - loss: 1.0039\nEpoch 42/100\n1/1 [==============================] - 0s 3ms/step - loss: 1.0156\nEpoch 43/100\n1/1 [==============================] - 0s 3ms/step - loss: 1.0181\nEpoch 44/100\n1/1 [==============================] - 0s 3ms/step - loss: 1.0067\nEpoch 45/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.9808\nEpoch 46/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.9443\nEpoch 47/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.9019\nEpoch 48/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.8571\nEpoch 49/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.8146\nEpoch 50/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.7768\nEpoch 51/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.7489\nEpoch 52/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.7294\nEpoch 53/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.7186\nEpoch 54/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.7124\nEpoch 55/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.7080\nEpoch 56/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.7044\nEpoch 57/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.7002\nEpoch 58/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6949\nEpoch 59/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6884\nEpoch 60/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6806\nEpoch 61/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6715\nEpoch 62/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6615\nEpoch 63/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6510\nEpoch 64/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6404\nEpoch 65/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6302\nEpoch 66/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6209\nEpoch 67/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6127\nEpoch 68/100\n1/1 [==============================] - 0s 4ms/step - loss: 0.6060\nEpoch 69/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6006\nEpoch 70/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5963\nEpoch 71/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5924\nEpoch 72/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5888\nEpoch 73/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5853\nEpoch 74/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5816\nEpoch 75/100\n1/1 [==============================] - 0s 4ms/step - loss: 0.5778\nEpoch 76/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5736\nEpoch 77/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5691\nEpoch 78/100\n1/1 [==============================] - 0s 4ms/step - loss: 0.5644\nEpoch 79/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5595\nEpoch 80/100\n1/1 [==============================] - 0s 4ms/step - loss: 0.5547\nEpoch 81/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5501\nEpoch 82/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5457\nEpoch 83/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5417\nEpoch 84/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5379\nEpoch 85/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5343\nEpoch 86/100\n1/1 [==============================] - 0s 4ms/step - loss: 0.5309\nEpoch 87/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5276\nEpoch 88/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5242\nEpoch 89/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5208\nEpoch 90/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5174\nEpoch 91/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5140\nEpoch 92/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5107\nEpoch 93/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5075\nEpoch 94/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5044\nEpoch 95/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5014\nEpoch 96/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.4986\nEpoch 97/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.4960\nEpoch 98/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.4935\nEpoch 99/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.4909\nEpoch 100/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.4885\n\n\n<keras.callbacks.History at 0x7fe5401346a0>\n\n\n\nnp.mean((net(X)>0.5) == y)\n\n0.98125\n\n\n\nnp.mean((net(XX)>0.5) == yy)\n\n0.977\n\n\n\n\n\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='adam',loss=tf.losses.binary_crossentropy,metrics=['accuracy'])\nnet.fit(X,y,epochs=100,batch_size=12000)\n\nEpoch 1/100\n1/1 [==============================] - 0s 169ms/step - loss: 220.9145 - accuracy: 0.5000\nEpoch 2/100\n1/1 [==============================] - 0s 3ms/step - loss: 88.9451 - accuracy: 0.5073\nEpoch 3/100\n1/1 [==============================] - 0s 3ms/step - loss: 7.5899 - accuracy: 0.8208\nEpoch 4/100\n1/1 [==============================] - 0s 4ms/step - loss: 33.7523 - accuracy: 0.5972\nEpoch 5/100\n1/1 [==============================] - 0s 4ms/step - loss: 40.2278 - accuracy: 0.5723\nEpoch 6/100\n1/1 [==============================] - 0s 4ms/step - loss: 28.9657 - accuracy: 0.6442\nEpoch 7/100\n1/1 [==============================] - 0s 3ms/step - loss: 16.5118 - accuracy: 0.8061\nEpoch 8/100\n1/1 [==============================] - 0s 3ms/step - loss: 9.4905 - accuracy: 0.8947\nEpoch 9/100\n1/1 [==============================] - 0s 3ms/step - loss: 6.2025 - accuracy: 0.9355\nEpoch 10/100\n1/1 [==============================] - 0s 5ms/step - loss: 5.2417 - accuracy: 0.9404\nEpoch 11/100\n1/1 [==============================] - 0s 3ms/step - loss: 5.5173 - accuracy: 0.9270\nEpoch 12/100\n1/1 [==============================] - 0s 3ms/step - loss: 6.5902 - accuracy: 0.9021\nEpoch 13/100\n1/1 [==============================] - 0s 3ms/step - loss: 7.8607 - accuracy: 0.8788\nEpoch 14/100\n1/1 [==============================] - 0s 3ms/step - loss: 8.5884 - accuracy: 0.8647\nEpoch 15/100\n1/1 [==============================] - 0s 3ms/step - loss: 8.3990 - accuracy: 0.8664\nEpoch 16/100\n1/1 [==============================] - 0s 3ms/step - loss: 7.4674 - accuracy: 0.8793\nEpoch 17/100\n1/1 [==============================] - 0s 3ms/step - loss: 6.2580 - accuracy: 0.8982\nEpoch 18/100\n1/1 [==============================] - 0s 3ms/step - loss: 5.1273 - accuracy: 0.9156\nEpoch 19/100\n1/1 [==============================] - 0s 3ms/step - loss: 4.2381 - accuracy: 0.9302\nEpoch 20/100\n1/1 [==============================] - 0s 3ms/step - loss: 3.6032 - accuracy: 0.9426\nEpoch 21/100\n1/1 [==============================] - 0s 3ms/step - loss: 3.1860 - accuracy: 0.9509\nEpoch 22/100\n1/1 [==============================] - 0s 3ms/step - loss: 2.9232 - accuracy: 0.9551\nEpoch 23/100\n1/1 [==============================] - 0s 4ms/step - loss: 2.7559 - accuracy: 0.9574\nEpoch 24/100\n1/1 [==============================] - 0s 3ms/step - loss: 2.6420 - accuracy: 0.9594\nEpoch 25/100\n1/1 [==============================] - 0s 3ms/step - loss: 2.5490 - accuracy: 0.9599\nEpoch 26/100\n1/1 [==============================] - 0s 3ms/step - loss: 2.4612 - accuracy: 0.9603\nEpoch 27/100\n1/1 [==============================] - 0s 3ms/step - loss: 2.3617 - accuracy: 0.9608\nEpoch 28/100\n1/1 [==============================] - 0s 3ms/step - loss: 2.2378 - accuracy: 0.9612\nEpoch 29/100\n1/1 [==============================] - 0s 3ms/step - loss: 2.0873 - accuracy: 0.9619\nEpoch 30/100\n1/1 [==============================] - 0s 3ms/step - loss: 1.9117 - accuracy: 0.9630\nEpoch 31/100\n1/1 [==============================] - 0s 3ms/step - loss: 1.7239 - accuracy: 0.9641\nEpoch 32/100\n1/1 [==============================] - 0s 3ms/step - loss: 1.5408 - accuracy: 0.9657\nEpoch 33/100\n1/1 [==============================] - 0s 3ms/step - loss: 1.3663 - accuracy: 0.9670\nEpoch 34/100\n1/1 [==============================] - 0s 3ms/step - loss: 1.2210 - accuracy: 0.9685\nEpoch 35/100\n1/1 [==============================] - 0s 3ms/step - loss: 1.1035 - accuracy: 0.9688\nEpoch 36/100\n1/1 [==============================] - 0s 4ms/step - loss: 1.0208 - accuracy: 0.9696\nEpoch 37/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.9766 - accuracy: 0.9705\nEpoch 38/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.9628 - accuracy: 0.9708\nEpoch 39/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.9717 - accuracy: 0.9715\nEpoch 40/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.9883 - accuracy: 0.9706\nEpoch 41/100\n1/1 [==============================] - 0s 3ms/step - loss: 1.0039 - accuracy: 0.9699\nEpoch 42/100\n1/1 [==============================] - 0s 3ms/step - loss: 1.0156 - accuracy: 0.9685\nEpoch 43/100\n1/1 [==============================] - 0s 4ms/step - loss: 1.0181 - accuracy: 0.9681\nEpoch 44/100\n1/1 [==============================] - 0s 4ms/step - loss: 1.0067 - accuracy: 0.9686\nEpoch 45/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.9808 - accuracy: 0.9693\nEpoch 46/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.9443 - accuracy: 0.9703\nEpoch 47/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.9019 - accuracy: 0.9711\nEpoch 48/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.8571 - accuracy: 0.9722\nEpoch 49/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.8146 - accuracy: 0.9737\nEpoch 50/100\n1/1 [==============================] - 0s 4ms/step - loss: 0.7768 - accuracy: 0.9743\nEpoch 51/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.7489 - accuracy: 0.9753\nEpoch 52/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.7294 - accuracy: 0.9759\nEpoch 53/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.7186 - accuracy: 0.9767\nEpoch 54/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.7124 - accuracy: 0.9774\nEpoch 55/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.7080 - accuracy: 0.9776\nEpoch 56/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.7044 - accuracy: 0.9777\nEpoch 57/100\n1/1 [==============================] - 0s 4ms/step - loss: 0.7002 - accuracy: 0.9776\nEpoch 58/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6949 - accuracy: 0.9778\nEpoch 59/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6884 - accuracy: 0.9779\nEpoch 60/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6806 - accuracy: 0.9784\nEpoch 61/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6715 - accuracy: 0.9786\nEpoch 62/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6615 - accuracy: 0.9786\nEpoch 63/100\n1/1 [==============================] - 0s 4ms/step - loss: 0.6510 - accuracy: 0.9784\nEpoch 64/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6404 - accuracy: 0.9786\nEpoch 65/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6302 - accuracy: 0.9787\nEpoch 66/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6209 - accuracy: 0.9791\nEpoch 67/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6127 - accuracy: 0.9787\nEpoch 68/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6060 - accuracy: 0.9791\nEpoch 69/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.6006 - accuracy: 0.9792\nEpoch 70/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5963 - accuracy: 0.9795\nEpoch 71/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5924 - accuracy: 0.9793\nEpoch 72/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5888 - accuracy: 0.9791\nEpoch 73/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5853 - accuracy: 0.9790\nEpoch 74/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5816 - accuracy: 0.9793\nEpoch 75/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5778 - accuracy: 0.9794\nEpoch 76/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5736 - accuracy: 0.9795\nEpoch 77/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5691 - accuracy: 0.9794\nEpoch 78/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5644 - accuracy: 0.9794\nEpoch 79/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5595 - accuracy: 0.9796\nEpoch 80/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5547 - accuracy: 0.9796\nEpoch 81/100\n1/1 [==============================] - 0s 5ms/step - loss: 0.5501 - accuracy: 0.9798\nEpoch 82/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5457 - accuracy: 0.9800\nEpoch 83/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5417 - accuracy: 0.9800\nEpoch 84/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5379 - accuracy: 0.9804\nEpoch 85/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.9807\nEpoch 86/100\n1/1 [==============================] - 0s 2ms/step - loss: 0.5309 - accuracy: 0.9807\nEpoch 87/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5276 - accuracy: 0.9807\nEpoch 88/100\n1/1 [==============================] - 0s 2ms/step - loss: 0.5242 - accuracy: 0.9808\nEpoch 89/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5208 - accuracy: 0.9808\nEpoch 90/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5174 - accuracy: 0.9811\nEpoch 91/100\n1/1 [==============================] - 0s 2ms/step - loss: 0.5140 - accuracy: 0.9812\nEpoch 92/100\n1/1 [==============================] - 0s 5ms/step - loss: 0.5107 - accuracy: 0.9812\nEpoch 93/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.5075 - accuracy: 0.9813\nEpoch 94/100\n1/1 [==============================] - 0s 2ms/step - loss: 0.5044 - accuracy: 0.9814\nEpoch 95/100\n1/1 [==============================] - 0s 2ms/step - loss: 0.5014 - accuracy: 0.9816\nEpoch 96/100\n1/1 [==============================] - 0s 2ms/step - loss: 0.4986 - accuracy: 0.9815\nEpoch 97/100\n1/1 [==============================] - 0s 2ms/step - loss: 0.4960 - accuracy: 0.9815\nEpoch 98/100\n1/1 [==============================] - 0s 2ms/step - loss: 0.4935 - accuracy: 0.9812\nEpoch 99/100\n1/1 [==============================] - 0s 4ms/step - loss: 0.4909 - accuracy: 0.9812\nEpoch 100/100\n1/1 [==============================] - 0s 3ms/step - loss: 0.4885 - accuracy: 0.9812\n\n\n<keras.callbacks.History at 0x7fe5401ee830>\n\n\n\nnet.evaluate(X,y)\n\n375/375 [==============================] - 0s 862us/step - loss: 0.4860 - accuracy: 0.9812\n\n\n[0.4859827756881714, 0.981249988079071]\n\n\n\nnet.evaluate(XX,yy)\n\n63/63 [==============================] - 0s 1ms/step - loss: 0.4294 - accuracy: 0.9770\n\n\n[0.42935940623283386, 0.9769999980926514]\n\n\n\n\n\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='adam',loss=tf.losses.binary_crossentropy,metrics=['accuracy'])\nnet.fit(X,y,epochs=10,batch_size=120)\n\nEpoch 1/10\n100/100 [==============================] - 0s 1ms/step - loss: 5.6482 - accuracy: 0.9418\nEpoch 2/10\n100/100 [==============================] - 0s 1ms/step - loss: 0.5070 - accuracy: 0.9792\nEpoch 3/10\n100/100 [==============================] - 0s 2ms/step - loss: 0.3774 - accuracy: 0.9820\nEpoch 4/10\n100/100 [==============================] - 0s 2ms/step - loss: 0.3497 - accuracy: 0.9829\nEpoch 5/10\n100/100 [==============================] - 0s 2ms/step - loss: 0.2401 - accuracy: 0.9855\nEpoch 6/10\n100/100 [==============================] - 0s 2ms/step - loss: 0.2196 - accuracy: 0.9870\nEpoch 7/10\n100/100 [==============================] - 0s 2ms/step - loss: 0.1767 - accuracy: 0.9893\nEpoch 8/10\n100/100 [==============================] - 0s 2ms/step - loss: 0.1602 - accuracy: 0.9884\nEpoch 9/10\n100/100 [==============================] - 0s 2ms/step - loss: 0.1798 - accuracy: 0.9874\nEpoch 10/10\n100/100 [==============================] - 0s 2ms/step - loss: 0.1339 - accuracy: 0.9906\n\n\n<keras.callbacks.History at 0x7fe5246999c0>\n\n\n\nnet.evaluate(X,y)\n\n375/375 [==============================] - 0s 789us/step - loss: 0.0984 - accuracy: 0.9911\n\n\n[0.09842231124639511, 0.9910833239555359]\n\n\n\nnet.evaluate(XX,yy)\n\n63/63 [==============================] - 0s 1ms/step - loss: 0.2526 - accuracy: 0.9845\n\n\n[0.2526112496852875, 0.984499990940094]\n\n\n\n\n\n\ntf.random.set_seed(43055)\nnet = tf.keras.Sequential() \nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='adam',loss=tf.losses.binary_crossentropy,metrics=['accuracy'])\nnet.fit(X,y,epochs=10,batch_size=240)\n\nEpoch 1/10\n50/50 [==============================] - 0s 1ms/step - loss: 6.3153 - accuracy: 0.9025\nEpoch 2/10\n50/50 [==============================] - 0s 1ms/step - loss: 0.5510 - accuracy: 0.9726\nEpoch 3/10\n50/50 [==============================] - 0s 1ms/step - loss: 0.2936 - accuracy: 0.9791\nEpoch 4/10\n50/50 [==============================] - 0s 1ms/step - loss: 0.1976 - accuracy: 0.9822\nEpoch 5/10\n50/50 [==============================] - 0s 1ms/step - loss: 0.1401 - accuracy: 0.9860\nEpoch 6/10\n50/50 [==============================] - 0s 1ms/step - loss: 0.1125 - accuracy: 0.9863\nEpoch 7/10\n50/50 [==============================] - 0s 1ms/step - loss: 0.0903 - accuracy: 0.9877\nEpoch 8/10\n50/50 [==============================] - 0s 1ms/step - loss: 0.0727 - accuracy: 0.9893\nEpoch 9/10\n50/50 [==============================] - 0s 1ms/step - loss: 0.0616 - accuracy: 0.9905\nEpoch 10/10\n50/50 [==============================] - 0s 1ms/step - loss: 0.0478 - accuracy: 0.9913\n\n\n<keras.callbacks.History at 0x7fe524564e20>\n\n\n\nnet.evaluate(X,y)\n\n375/375 [==============================] - 0s 831us/step - loss: 0.0592 - accuracy: 0.9898\n\n\n[0.05924727022647858, 0.9898333549499512]\n\n\n\nnet.evaluate(XX,yy)\n\n63/63 [==============================] - 0s 1ms/step - loss: 0.1804 - accuracy: 0.9805\n\n\n[0.18036103248596191, 0.9804999828338623]"
  },
  {
    "objectID": "posts/2022-01-12-회귀분석_단순선형회귀분석.html",
    "href": "posts/2022-01-12-회귀분석_단순선형회귀분석.html",
    "title": "jisim12",
    "section": "",
    "text": "회귀분석\n\n\ntoc:false\nbranch: master\nbadges: true\ncomments: true\nauthor: 심재인\n\n\n\n\n대략적 파악 : 산점도(scatter plot)\n상관분석(correlation analysis)\n\n\n두 변수 사이의 상관관계 분석\n\n\n확률변수 \\(X,Y \\rightarrow \\rho\\) = Corr(\\(X,Y\\)) - 직선적인 관련성 파악\n\n\n회귀분석(regression analysis)\n\n\n두 변수 사이의 함수관계를 분석\n\n\n\\(x\\) : 독립변수 또는 설명변수, \\(Y\\) : 종속변수 또는 반응변수\n\n\\(Y\\) = \\(f(x) + \\epsilon,\\epsilon\\) : 오차항 \\(\\rightarrow f(x)?\\)\n\n단순선형회귀분석 - 직선관계를 모형으로 분석\n\n\\(\\quad\\) \\(\\quad\\) \\(\\quad\\) \\((f(x) = a+bx)\\)\n\n중회귀분석 - 두 개 이상의 설명변수 사용\n\n\\(\\quad\\) \\(\\quad\\) \\(\\quad\\) \\((f(x)=a+b_1x_1+\\dots+b_{k}x_k)\\)\n\n\n\n\n자료구조\n\n\n자료구조 : \\((x_1,Y_1),\\dots,(x_n,Y_n)\\) \\(\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\begin{cases}소문자:설명변수\\rightarrow 상수 \\\\ 대문자:확률변수 \\end{cases}\\)\n\\((x_1,\\dots,x_n)\\) : 설명변수(explanatory variable)(또는 독립변수)\n\n두 변수가 있을 때, 다른 한 변수에 영향을 주는 변수\n\n\\((Y_1,\\dots,Y_n)\\) : 반응변수(response variable)(또는 종속변수)\n\n두 변수가 있을 때, 다른 한 변수에 영향을 받는 변수\n\n관측값 : \\((x_1,y_1),\\dots,(x_n,y_n)\\)\n\n\nModel\n\n\n\\(Y_i = \\beta_0 + \\beta_{1}x_i + \\epsilon_i,\\quad i = 1,2,\\dots,n\\)\n\\((\\epsilon_1,\\dots,\\epsilon_n)\\) : 오차항(random error)\n\n서로 독립이면서 평균이 0, 분산이 \\(\\sigma^2\\)인 확률 변수\n\n용어\n\n\n회귀계수(regression coefficient) (or 모수, parameter)\n\n\n\\(\\beta_0\\) : 상수항 또는 절편 (constant coefficient or intercept)\n\n\n\\(\\beta_1\\) : 기울기 (slope)\n\n\n\n\n기본 가정\n\n선형성(Linearity) : \\(E(Y|x) = \\beta_0 + \\beta_1x\\)\n등분산성(Homoscedastic) : \\(Var(Y|x) = \\sigma^2\\)\n정규성(Normality) : \\(\\epsilon_i\\) ~ \\(N(0,\\sigma^2)\\)\n독립성(Independency) : \\(\\epsilon_i\\) are mutually independent \\(i = 1,\\dots,n\\)\n\n\n\n\n\n최소제곱법(method of least squares)에 의한 추정\n\n최소제곱추정량(LSE)\n\n\\(\\quad\\quad\\quad\\) \\((\\hat{\\beta_0},\\hat{\\beta_1})\\) = argmin \\(\\displaystyle\\sum_{i=1}^{n}\\{y_i - (\\beta_0 + \\beta_{1}x_i)\\}^2\\)\n\nLeast square fit : \\(\\hat{y}\\big(\\equiv E(\\hat{Y}|x_0) \\big) = \\hat{\\beta_0} + \\hat{\\beta_1}x_0\\)\n\n잔차(Residual) : \\(e_i = y_i - \\hat{y_i}\\)\n\n오차제곱합\n\\(\\quad\\quad\\quad S = \\displaystyle\\sum_{i=1}^{n}\\epsilon^2_{i}\\displaystyle\\sum_{i=1}^{n}\\{y_i - (\\beta_0 + \\beta_{1}x_i)\\}^2\\)\n정규방정식(normal equation)\n\n\\(\\frac{\\partial{S}}{\\partial\\beta_0}\\) = \\(\\displaystyle\\sum_{i=1}^{n}(y_i - \\beta_0 - \\beta_{1}x_i)\\)\n\\(\\frac{\\partial{S}}{\\partial\\beta_1}\\) = \\(-2\\displaystyle\\sum_{i=1}^{n}x_{i}(y_i - \\beta_0 - \\beta_{1}x_i)\\)\n\n최소제곱추정량 = 정규방정식의 해\n\n\\(\\quad\\quad\\quad\\begin{cases}-2\\sum_{i=1}^{n}(y_i - \\hat{\\beta_0} - \\hat\\beta_{1}x_i)=0 \\\\ -2\\sum_{i=1}^{n}x_{i}(y_i - \\hat{\\beta_0} - \\hat\\beta_{1}x_i)=0\\end{cases}\\)\n\\(\\Rightarrow\\begin{cases}n\\hat{\\beta_0} + \\hat{\\beta_1}\\sum_{i=1}^{n}x_{i} = \\sum_{i=1}^{n}y_{i} \\\\ \\sum_{i=1}^{n}x_{i}\\hat{\\beta_0} + \\sum_{i=1}^{n}x^2_{i}\\hat{\\beta_1}\\sum_{i=1}^{n}x_{i}y_i\\end{cases}\\)\n\n최소제곱추정량\n\\(\\hat{\\beta_1}\\) = \\(\\frac{\\sum_{i=1}^{n}(x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i=1}^{n}(x_i - \\overline{x})^2}\\) = \\(\\frac{S_{xy}}{S_{xx}}\\)\n\n\\(\\hat{\\beta_0}\\) = \\(\\overline{y} - \\hat{\\beta_1}\\overline{x}\\)\n\n\n\n\n\n잔차(residual) : \\(e_i = y_i - \\hat{y_i}\\), \\((\\sum_{i=1}^{n}e_i = 0, \\sum_{i=1}^{n}x_{i}e_i = 0)\\)\n> \\(e_i\\) = 오차의 관측값\\(\\quad y_i = 실제값\\quad \\hat{y_i} = 추정값\\)\n\n오차분산 \\((\\sigma^2)\\)의 추정:\n\n잔차(오차) 제곱합 (residual (or error) sum of squares) :\n\\[ SSE = \\displaystyle\\sum_{i=1}^{n}(y_i-\\hat{y_i})^2 = \\displaystyle\\sum_{i=1}^{n}e_{i}^2\\]\n\n평균제곱오차 (mean squared error) : \\(MSE = \\frac{SEE}{n-2}\\)\n\n오차분산의 추정값 : \\(\\hat\\sigma^2 = MSE\\)\n\n\n\n\n\n\n총편차의 분해\n\n\\(y_i-\\hat y = (y_i - \\hat y_i) + (\\hat y_i - \\hat y), \\quad\\forall_i\\)\n\n총편차(total deviation) = \\(y_i - \\overline y\\)\n\n추측값의 편차 = \\((\\hat y_i - \\overline{\\hat y})\\) = \\((\\hat y_i - \\overline y),\\quad\\quad\\quad\\overline{\\hat y} = \\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\hat y_i = \\hat y\\)\n\\(\\Rightarrow\\) 총편차 = 잔차 + 추측값의 편차\n\n\n\n\n\n\n제곱합의 분해 : \\(SST = SSE + SSR\\)\n\\[\\displaystyle\\sum_{i=1}^{n}(y_i - \\overline y)^2=\\displaystyle\\sum_{i=1}^{n}(y_i - \\hat y_i)^2 + \\displaystyle\\sum_{i=1}^{n}(\\hat y_i - \\overline y)^2\\]\n——————-\n\\(\\quad\\quad\\quad\\) 제곱합의 종류 \\(\\quad\\quad\\quad\\) 정의 및 기호 \\(\\quad\\quad\\quad\\) 자유도\n\n\n\n\n\n\n\n\\(\\quad\\)총제곱합 (total sum of squares)\\(\\qquad\\qquad\\qquad SST = \\displaystyle\\sum_{i=1}^{n}(y_i - \\overline y)^2\\qquad\\qquad\\quad n-1\\)\n\n\n잔차제곱합 (residual sum of squares)\\(\\qquad\\qquad\\quad SSE = \\displaystyle\\sum_{i=1}^{n}(y_i - \\hat y_i)^2 \\qquad\\qquad\\quad n-2\\)\n\n\n회귀제곱합 (regression sum of squares)\\(\\qquad\\qquad SSR = \\displaystyle\\sum_{i=1}^{n}(\\hat y_i - \\overline y)^2\\qquad\\qquad\\qquad 1\\)\n\n\n\n\n\n\n\n결정계수 (Coefficient of determination)\n\n정의 : \\(R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\\)\n\n의미 : 회귀직선의 기여울\n(총변동 가운데 회귀직선으로 설명되는 변동의 비율)\n\n성질\n> \\(0\\leq R^2\\leq1\\)\n\\(R^2\\) 값이 1에 가까울수록 회귀에 의한 설명이 잘 됨을 뜻함\n\\(R^2 = r^2 (r : sample correlation)\\qquad 표본상관계수\\)\n(단순선형회귀모형에서만 성립)\n\n\n\n\n\n\nModel : \\(Y_i = \\beta_0+\\beta_{1}x_i+\\epsilon_i,\\quad i = 1,2,\\dots,n,\\epsilon_i\\sim_{iid} N(0,\\sigma^2)\\)\n\n회귀직선의 유의성 검정 (F-test)\n\n가설 : \\(H_0 : \\beta_1 = 0\\;vs. H_1 : \\beta_1\\neq0\\)\n\n검정통계량 : \\(F = \\frac{MSR}{MSE} = \\frac{SSR/1}{SSE/(n-2)}\\sim_{H_0}\\;F(1,n-2)\\)\n\n검정통계량의 관측값 : \\(f\\)\n\n유의수준 \\(\\alpha\\)에서의 기각역 : \\(f\\geq F_{\\alpha}(1,n-2)\\)\n\n유의확률 = \\(P(F\\geq f)\\)\n\n\n\n\n\n\n\n요인 \\(\\qquad\\) 제곱합(SS) \\(\\qquad\\) 자유도(df) \\(\\qquad\\quad\\) 평균제곱(MS) \\(\\qquad\\qquad f\\qquad\\qquad\\qquad\\) 유의확률\n\n\n\n\n\n\n\n회귀 \\(\\qquad SSR\\qquad\\qquad\\quad\\) 1 \\(\\qquad\\qquad MSR=\\frac{SSR}{1}\\qquad\\quad f=\\frac{MSR}{MSE}\\qquad\\qquad\\; P(F\\geq f)\\)\n\n\n\n\n계 \\(\\qquad\\quad SST\\qquad\\qquad\\quad n-1\\)\n\n\n\n\n\n\n\n모회귀계수(기울기) \\(\\beta_1\\) 에 대한 추론\n\n\\(\\beta_1\\)의 최소제곱추정량 : \\(\\hat{\\beta_1} = \\frac{S_{xY}}{S_{xx}}\\)\n\n추정값 : \\(\\hat{\\beta_1} = \\frac{S_{xY}}{S_{xx}}\\)\n\n추정량\\(\\;\\hat{\\beta_1}\\)의 분포 : \\(\\hat{\\beta_1}\\sim N(\\beta_1,\\frac{\\sigma^2}{S_{xx}})\\)\n\nstudentized \\(\\hat{\\beta_1}\\) 의 분포 : \\(\\frac{\\hat{\\beta_1}-\\beta_1}{\\hat{\\sigma}/\\sqrt{S_{xx}}}\\sim\\; t(n-2),\\; \\hat{\\sigma} = \\sqrt{MSE}\\)\n\n\\(\\hat{\\beta_1}\\)의 \\(100(1-\\alpha)\\)% 신뢰구간 : \\(\\hat{\\beta_1}\\pm t_{\\alpha/2}(n-2)\\hat{\\sigma} = \\sqrt{S_{xx}}\\)\n\n\n\n\n\n\n모회귀계수(기울기) \\(\\beta_1\\)에 대한 추론\n\n가설검정 : \\(H_0:\\beta_1=\\beta_1^0\\)\n\n\n\\(\\qquad\\qquad\\quad\\)대립가설\\(\\qquad\\qquad\\quad\\)유의확률 \\(\\qquad\\qquad\\quad유의수준\\alpha\\) 기각역\n\n\n\n\n\n\\(\\qquad\\qquad H_1:\\beta_1>\\beta_{1}^0\\qquad\\quad P(T\\geq t)\\qquad\\qquad\\quad t\\geq t_{\\alpha} (n-2)\\)\n\n\n\\(\\qquad\\qquad H_1:\\beta_1<\\beta_{1}^0\\qquad\\quad P(T\\leq t)\\qquad\\qquad\\quad t\\geq t_{\\alpha} (n-2)\\)\n\n\n\\(\\qquad\\qquad H_1:\\beta_1\\neq\\beta_{1}^0\\qquad\\quad P(|T|\\geq |t|)\\qquad\\quad |t|\\geq t_{\\alpha/2} (n-2)\\)\n\n\n\n\n\n\n\n\n모회귀계수(절편) \\(\\beta_0\\)에 대한 추론\n\n\\(\\beta_0\\)의 최소제곱추정량 : \\(\\hat{\\beta_0}=\\overline{Y}-\\hat{\\beta_1}\\overline{x}\\)\n\n추정값 : \\(\\hat{\\beta_0}\\) = \\(\\overline y-\\hat{\\beta_1}\\overline x\\)\n\n추정량 \\(\\hat{\\beta_0}\\)의 분포 : \\(\\hat{\\beta_0}\\sim N\\big(\\beta_0,\\sigma^2(\\frac{1}{n}+\\frac{\\overline{x}^2}{S_{xx}})\\big)\\)\n\\[\\frac{\\hat{\\beta_0}-{\\beta_0}}{_{s.e.}(\\hat{\\beta_0})}\\sim t(n-2),\\;_{s.e.}(\\hat{\\beta_0})=\\hat{\\sigma}\\sqrt{\\frac{1}{n}+\\frac{\\overline x^2}{S_{xx}}}\\]\n\n\\(\\hat{\\beta_0}\\)의 \\(100(1-\\alpha)\\)% 신뢰구간 : \\(\\hat{\\beta_0}\\pm t_{\\alpha/2}(n-2)_{s.e.}(\\hat{\\beta_0})\\)\n\n\n\n\n\n\n모회귀계수(기울기) \\(\\beta_0\\)에 대한 추론\n\n가설검정 : \\(H_0:\\beta_0=\\beta_0^0\\)\n\n\n\\(\\qquad\\qquad\\quad\\)대립가설\\(\\qquad\\qquad\\quad\\)유의확률 \\(\\qquad\\qquad\\quad유의수준\\alpha\\) 기각역\n\n\n\n\n\n\\(\\qquad\\qquad H_1:\\beta_0>\\beta_{0}^0\\qquad\\quad P(T\\geq t)\\qquad\\qquad\\quad t\\geq t_{\\alpha} (n-2)\\)\n\n\n\\(\\qquad\\qquad H_1:\\beta_0<\\beta_{0}^0\\qquad\\quad P(T\\leq t)\\qquad\\qquad\\quad t\\geq t_{\\alpha} (n-2)\\)\n\n\n\\(\\qquad\\qquad H_1:\\beta_0\\neq\\beta_{0}^0\\qquad\\quad P(|T|\\geq |t|)\\qquad\\quad |t|\\geq t_{\\alpha/2} (n-2)\\)\n\n\n\n\n\n\n\n\n\\(x=x_0\\) 가 주어졌을 때 평균반응의 예측\n\n평균반응 (mean response) : \\(\\mu_0 = E(Y|x_0) = \\beta_0+\\beta_1x_0\\)\n\n평균반응 추정량 : \\(\\hat{\\mu_0} = \\hat{\\beta_0} + \\hat{\\beta_1}x_0\\)\n\n\\(\\hat{\\mu_0}\\) 의 분포 : \\(\\hat{\\mu_0}\\sim N \\big(\\mu_0,(\\frac{1}{n} + \\frac{(x_0-\\overline x)^2}{S_{xx}})\\sigma^2\\big)\\)\n\\[\\frac{\\hat{\\mu_0}-\\mu_0}{_{s.e.}(\\hat{\\mu_0})}\\sim t(n-2),\\; _{s.e.}(\\hat{\\mu_0}) = \\hat{\\sigma}\\sqrt{\\frac{1}{n}+\\frac{(x_0-\\overline x)^2}{S_{xx}}}\\]\n\n\\(\\hat{\\mu_0}\\)의 \\(100(1-\\alpha)\\)% 신뢰구간 : \\(\\hat{\\mu_0}\\pm t_{\\alpha/2}(n-2)_{s.e.}(\\hat{\\mu_0})\\)\n\n\n\n\n\n\n\\(x=x_0\\)가 주어졌을 때 \\(y=y_0\\) 예측\n\n\\(y_0 = \\beta_0+\\beta_1x_0+\\epsilon_0\\)\n\n예측값 : \\(\\hat y_0 = \\hat{\\beta_0}+\\hat{\\beta_1}x_0\\)\n\n\\(\\hat y_0\\) 의 분포 : \\(\\hat y_0\\sim N\\big(\\mu_0,(1+\\frac{1}{n} + \\frac{(x_0-\\overline x)^2}{S_{xx}})\\sigma^2\\big)\\)\n\\[\\frac{\\hat{y_0}-y_0}{_{s.e.}(\\hat{y_0})}\\sim t(n-2),\\; _{s.e.}(\\hat{y_0}) = \\hat{\\sigma}\\sqrt{1+\\frac{1}{n}+\\frac{(x_0-\\overline x)^2}{S_{xx}}}\\]\n\n\\(\\hat{y_0}\\)의 \\(100(1-\\alpha)\\)% 신뢰구간 : \\(\\hat{y_0}\\pm t_{\\alpha/2}(n-2)_{s.e.}(\\hat{y_0})\\)"
  },
  {
    "objectID": "posts/2022.09.02_발표자료.html",
    "href": "posts/2022.09.02_발표자료.html",
    "title": "jisim12",
    "section": "",
    "text": "발표자료\n\n\ntoc:false\nbranch: master\nbadges: true\ncomments: true\nauthor: 심재인\n\n\n\n\n\n2020년 3월 11일에 COVID-19는 전세계 최소 114국으로 퍼졌으며 약 4,000명이 사망하여 세계보건기구는 COVID-19가 세계적 대유행임을 선언하였다.\nCOVID-19가 전세계적으로 대유행을 하면서 우리들의 많은 일상에 변화들이 일어나게 되었다.\n그로 인해서 조금이나마 우리가 COVID-19에 예방이라도 할 수 있을까? 라는 생각을 하게 되었고,\nCOVID-19의 증상으로는, 가족 내에서 발생한 사례를 분석하였을 때 노출 후 3–7일 경에 열과 호흡기 증상이 발생하였다.\n열, 마른 기침, 피로 감이 주요 증상이며, 코막힘, 콧물, 인후통, 근육통 등은 비교적 드물게 나타난다.\nCOVID-19는 무증상 감염부터 폐렴 및 사망까지 일으킨다.\n심한 경우 폐렴으로까지 가는 COVID-19는 주로 나타나는 증상 중 기침이 있습니다.\n그래서 기침소리를 분석해서 사람들의 기침 소리로 COVID-19를 분류할 수 있을지 연구해봐야겠다고 생각하게 되었습니다.\n\n메타데이터 가져오고 녹음 오디오 올바른거 선택\n나이와 기침소리 감지 점수에 따라서 다른 등급 정의\n\nMinMaxScaler, OneHotEncoder, LabelEncoder으로 데이터값을 0과 1사이의 범위 값으로 변환 (1에 가까우면 진짜 기침 소리일 가능성 높고, 0에 가까울수록 가짜 기침 소리일 가능성이 높다)\nMinMaxScaler:데이터값을 0과 1사이의 범위 값으로 변환(음수 값이 있으면 -1에서 1값으로 변환)\nOneHotEncoder:피처 값의 유형에 따라 새로운 피처를 추가해 고유 값에 해당하는 칼럼에만 1을 표시하고 나머지 칼럼에는 0을 표시\nLabelEncoder:카테고리 피처를 코드형 숫자 값으로 변환\n\nCOVID 식별 모델 교육\n\n각 audio_class에 대해, 녹음의 오디오 속성뿐만 아니라 주요 메타데이터를 기반으로 상태를 표시하는 별도의 모델을 교육한다.\n\n모든 오디오 파일에 대한 DSR 루프 출력\n데이터 프레임에서 클래스 균형 조정\n데이터 프레임을 저장하여 값 지정\n지정된 UUID에 대한 사운드 및 스펙트로그램 시각화\n분할 열차 시험\n\n데이터 집합의 불균형으로 인해 작은 클래스를 보존하려고 ‘stratify’ 인수를 사용.\n\n데이터셋 정규화\n\n데이터 유출을 방지하기 위해 훈련 세트만 사용하여 스케일러를 장착.\n\n모형 정확도를 표시하고 평가하는 데 사용되는 도우미 함수\n로지스틱 회귀 분석 설정(멀티 클래스)\n\n로지스틱 회귀분석 대표적인 이진분류 기법으로, train 값과 라벨링(0 또는 1)을 통해 모델을 생성, 생성된 모델에 test 값을 넣어 예측해본다.\n\nXGBoost를 사용하여 BDT 분류기 교육\nXG Boost 교육 진단\n\n특정 변수의 상대적 영향도를 측정하여 분류 모델의 불순도를 더 많이 낮춰주는 변수들을 찾고, 정렬되기 전 인덱스를 리스트 형태로 반환하고, 배열을 뒤집는다.\n\n예측을 원래 데이터 프레임으로 다시 병합\n\n병합 후 XGBoost에서 잘못된 작업을 수행하는 항목을 선택하고 출력.\n이런 방식으로 사람들의 기침 소리로 COVID-19를 분류할 수 있을지 연구를 해보려고 합니다.\n감사합니다."
  },
  {
    "objectID": "posts/2022-03-21-3주차_빅데이터분석특강.html",
    "href": "posts/2022-03-21-3주차_빅데이터분석특강.html",
    "title": "jisim12",
    "section": "",
    "text": "3주차-3월 21일\n\n빅데이터분석특강\n\n\ntoc:false\nbranch: master\nbadges: true\ncomments: true\nauthor: 심재인\n\n\n강의영상\n\nyoutube:\n\n\n\nimports\n\nimport tensorflow as tf\nimport numpy as np\n\n\ntf.config.experimental.list_physical_devices('GPU')\n\n2022-04-25 14:41:14.910157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n\n\n\n\n지난강의 보충\n- max, min, sum, mean\n\na= tf.constant([1.0,2.0,3.0,4.0])\na\n\n<tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 2., 3., 4.], dtype=float32)>\n\n\n\ntf.reduce_mean(a)\n\n<tf.Tensor: shape=(), dtype=float32, numpy=2.5>\n\n\n\nconcat, stack\n- 예제: (2,3,4,5) stack (2,3,4,5) -> (?,?,?,?,?)\n\na = tf.reshape(tf.constant(range(2*3*4*5)),(2,3,4,5))\nb = -a \n\ncase1 (1,2,3,4,5) stack (1,2,3,4,5) –> (2,2,3,4,5) # axis=0\n\ntf.stack([a,b],axis=0)\n\n<tf.Tensor: shape=(2, 2, 3, 4, 5), dtype=int32, numpy=\narray([[[[[   0,    1,    2,    3,    4],\n          [   5,    6,    7,    8,    9],\n          [  10,   11,   12,   13,   14],\n          [  15,   16,   17,   18,   19]],\n\n         [[  20,   21,   22,   23,   24],\n          [  25,   26,   27,   28,   29],\n          [  30,   31,   32,   33,   34],\n          [  35,   36,   37,   38,   39]],\n\n         [[  40,   41,   42,   43,   44],\n          [  45,   46,   47,   48,   49],\n          [  50,   51,   52,   53,   54],\n          [  55,   56,   57,   58,   59]]],\n\n\n        [[[  60,   61,   62,   63,   64],\n          [  65,   66,   67,   68,   69],\n          [  70,   71,   72,   73,   74],\n          [  75,   76,   77,   78,   79]],\n\n         [[  80,   81,   82,   83,   84],\n          [  85,   86,   87,   88,   89],\n          [  90,   91,   92,   93,   94],\n          [  95,   96,   97,   98,   99]],\n\n         [[ 100,  101,  102,  103,  104],\n          [ 105,  106,  107,  108,  109],\n          [ 110,  111,  112,  113,  114],\n          [ 115,  116,  117,  118,  119]]]],\n\n\n\n       [[[[   0,   -1,   -2,   -3,   -4],\n          [  -5,   -6,   -7,   -8,   -9],\n          [ -10,  -11,  -12,  -13,  -14],\n          [ -15,  -16,  -17,  -18,  -19]],\n\n         [[ -20,  -21,  -22,  -23,  -24],\n          [ -25,  -26,  -27,  -28,  -29],\n          [ -30,  -31,  -32,  -33,  -34],\n          [ -35,  -36,  -37,  -38,  -39]],\n\n         [[ -40,  -41,  -42,  -43,  -44],\n          [ -45,  -46,  -47,  -48,  -49],\n          [ -50,  -51,  -52,  -53,  -54],\n          [ -55,  -56,  -57,  -58,  -59]]],\n\n\n        [[[ -60,  -61,  -62,  -63,  -64],\n          [ -65,  -66,  -67,  -68,  -69],\n          [ -70,  -71,  -72,  -73,  -74],\n          [ -75,  -76,  -77,  -78,  -79]],\n\n         [[ -80,  -81,  -82,  -83,  -84],\n          [ -85,  -86,  -87,  -88,  -89],\n          [ -90,  -91,  -92,  -93,  -94],\n          [ -95,  -96,  -97,  -98,  -99]],\n\n         [[-100, -101, -102, -103, -104],\n          [-105, -106, -107, -108, -109],\n          [-110, -111, -112, -113, -114],\n          [-115, -116, -117, -118, -119]]]]], dtype=int32)>\n\n\ncase2 (2,1,3,4,5) stack (2,1,3,4,5) –> (2,2,3,4,5) # axis=1\n\ntf.stack([a,b],axis=1)\n\n<tf.Tensor: shape=(2, 2, 3, 4, 5), dtype=int32, numpy=\narray([[[[[   0,    1,    2,    3,    4],\n          [   5,    6,    7,    8,    9],\n          [  10,   11,   12,   13,   14],\n          [  15,   16,   17,   18,   19]],\n\n         [[  20,   21,   22,   23,   24],\n          [  25,   26,   27,   28,   29],\n          [  30,   31,   32,   33,   34],\n          [  35,   36,   37,   38,   39]],\n\n         [[  40,   41,   42,   43,   44],\n          [  45,   46,   47,   48,   49],\n          [  50,   51,   52,   53,   54],\n          [  55,   56,   57,   58,   59]]],\n\n\n        [[[   0,   -1,   -2,   -3,   -4],\n          [  -5,   -6,   -7,   -8,   -9],\n          [ -10,  -11,  -12,  -13,  -14],\n          [ -15,  -16,  -17,  -18,  -19]],\n\n         [[ -20,  -21,  -22,  -23,  -24],\n          [ -25,  -26,  -27,  -28,  -29],\n          [ -30,  -31,  -32,  -33,  -34],\n          [ -35,  -36,  -37,  -38,  -39]],\n\n         [[ -40,  -41,  -42,  -43,  -44],\n          [ -45,  -46,  -47,  -48,  -49],\n          [ -50,  -51,  -52,  -53,  -54],\n          [ -55,  -56,  -57,  -58,  -59]]]],\n\n\n\n       [[[[  60,   61,   62,   63,   64],\n          [  65,   66,   67,   68,   69],\n          [  70,   71,   72,   73,   74],\n          [  75,   76,   77,   78,   79]],\n\n         [[  80,   81,   82,   83,   84],\n          [  85,   86,   87,   88,   89],\n          [  90,   91,   92,   93,   94],\n          [  95,   96,   97,   98,   99]],\n\n         [[ 100,  101,  102,  103,  104],\n          [ 105,  106,  107,  108,  109],\n          [ 110,  111,  112,  113,  114],\n          [ 115,  116,  117,  118,  119]]],\n\n\n        [[[ -60,  -61,  -62,  -63,  -64],\n          [ -65,  -66,  -67,  -68,  -69],\n          [ -70,  -71,  -72,  -73,  -74],\n          [ -75,  -76,  -77,  -78,  -79]],\n\n         [[ -80,  -81,  -82,  -83,  -84],\n          [ -85,  -86,  -87,  -88,  -89],\n          [ -90,  -91,  -92,  -93,  -94],\n          [ -95,  -96,  -97,  -98,  -99]],\n\n         [[-100, -101, -102, -103, -104],\n          [-105, -106, -107, -108, -109],\n          [-110, -111, -112, -113, -114],\n          [-115, -116, -117, -118, -119]]]]], dtype=int32)>\n\n\ncase3 (2,3,1,4,5) stack (2,3,1,4,5) –> (2,3,2,4,5) # axis=2\n\ntf.stack([a,b],axis=2)\n\n<tf.Tensor: shape=(2, 3, 2, 4, 5), dtype=int32, numpy=\narray([[[[[   0,    1,    2,    3,    4],\n          [   5,    6,    7,    8,    9],\n          [  10,   11,   12,   13,   14],\n          [  15,   16,   17,   18,   19]],\n\n         [[   0,   -1,   -2,   -3,   -4],\n          [  -5,   -6,   -7,   -8,   -9],\n          [ -10,  -11,  -12,  -13,  -14],\n          [ -15,  -16,  -17,  -18,  -19]]],\n\n\n        [[[  20,   21,   22,   23,   24],\n          [  25,   26,   27,   28,   29],\n          [  30,   31,   32,   33,   34],\n          [  35,   36,   37,   38,   39]],\n\n         [[ -20,  -21,  -22,  -23,  -24],\n          [ -25,  -26,  -27,  -28,  -29],\n          [ -30,  -31,  -32,  -33,  -34],\n          [ -35,  -36,  -37,  -38,  -39]]],\n\n\n        [[[  40,   41,   42,   43,   44],\n          [  45,   46,   47,   48,   49],\n          [  50,   51,   52,   53,   54],\n          [  55,   56,   57,   58,   59]],\n\n         [[ -40,  -41,  -42,  -43,  -44],\n          [ -45,  -46,  -47,  -48,  -49],\n          [ -50,  -51,  -52,  -53,  -54],\n          [ -55,  -56,  -57,  -58,  -59]]]],\n\n\n\n       [[[[  60,   61,   62,   63,   64],\n          [  65,   66,   67,   68,   69],\n          [  70,   71,   72,   73,   74],\n          [  75,   76,   77,   78,   79]],\n\n         [[ -60,  -61,  -62,  -63,  -64],\n          [ -65,  -66,  -67,  -68,  -69],\n          [ -70,  -71,  -72,  -73,  -74],\n          [ -75,  -76,  -77,  -78,  -79]]],\n\n\n        [[[  80,   81,   82,   83,   84],\n          [  85,   86,   87,   88,   89],\n          [  90,   91,   92,   93,   94],\n          [  95,   96,   97,   98,   99]],\n\n         [[ -80,  -81,  -82,  -83,  -84],\n          [ -85,  -86,  -87,  -88,  -89],\n          [ -90,  -91,  -92,  -93,  -94],\n          [ -95,  -96,  -97,  -98,  -99]]],\n\n\n        [[[ 100,  101,  102,  103,  104],\n          [ 105,  106,  107,  108,  109],\n          [ 110,  111,  112,  113,  114],\n          [ 115,  116,  117,  118,  119]],\n\n         [[-100, -101, -102, -103, -104],\n          [-105, -106, -107, -108, -109],\n          [-110, -111, -112, -113, -114],\n          [-115, -116, -117, -118, -119]]]]], dtype=int32)>\n\n\ncase4 (2,3,4,1,5) stack (2,3,4,1,5) –> (2,3,4,2,5) # axis=3\n\ntf.stack([a,b],axis=-2)\n\n<tf.Tensor: shape=(2, 3, 4, 2, 5), dtype=int32, numpy=\narray([[[[[   0,    1,    2,    3,    4],\n          [   0,   -1,   -2,   -3,   -4]],\n\n         [[   5,    6,    7,    8,    9],\n          [  -5,   -6,   -7,   -8,   -9]],\n\n         [[  10,   11,   12,   13,   14],\n          [ -10,  -11,  -12,  -13,  -14]],\n\n         [[  15,   16,   17,   18,   19],\n          [ -15,  -16,  -17,  -18,  -19]]],\n\n\n        [[[  20,   21,   22,   23,   24],\n          [ -20,  -21,  -22,  -23,  -24]],\n\n         [[  25,   26,   27,   28,   29],\n          [ -25,  -26,  -27,  -28,  -29]],\n\n         [[  30,   31,   32,   33,   34],\n          [ -30,  -31,  -32,  -33,  -34]],\n\n         [[  35,   36,   37,   38,   39],\n          [ -35,  -36,  -37,  -38,  -39]]],\n\n\n        [[[  40,   41,   42,   43,   44],\n          [ -40,  -41,  -42,  -43,  -44]],\n\n         [[  45,   46,   47,   48,   49],\n          [ -45,  -46,  -47,  -48,  -49]],\n\n         [[  50,   51,   52,   53,   54],\n          [ -50,  -51,  -52,  -53,  -54]],\n\n         [[  55,   56,   57,   58,   59],\n          [ -55,  -56,  -57,  -58,  -59]]]],\n\n\n\n       [[[[  60,   61,   62,   63,   64],\n          [ -60,  -61,  -62,  -63,  -64]],\n\n         [[  65,   66,   67,   68,   69],\n          [ -65,  -66,  -67,  -68,  -69]],\n\n         [[  70,   71,   72,   73,   74],\n          [ -70,  -71,  -72,  -73,  -74]],\n\n         [[  75,   76,   77,   78,   79],\n          [ -75,  -76,  -77,  -78,  -79]]],\n\n\n        [[[  80,   81,   82,   83,   84],\n          [ -80,  -81,  -82,  -83,  -84]],\n\n         [[  85,   86,   87,   88,   89],\n          [ -85,  -86,  -87,  -88,  -89]],\n\n         [[  90,   91,   92,   93,   94],\n          [ -90,  -91,  -92,  -93,  -94]],\n\n         [[  95,   96,   97,   98,   99],\n          [ -95,  -96,  -97,  -98,  -99]]],\n\n\n        [[[ 100,  101,  102,  103,  104],\n          [-100, -101, -102, -103, -104]],\n\n         [[ 105,  106,  107,  108,  109],\n          [-105, -106, -107, -108, -109]],\n\n         [[ 110,  111,  112,  113,  114],\n          [-110, -111, -112, -113, -114]],\n\n         [[ 115,  116,  117,  118,  119],\n          [-115, -116, -117, -118, -119]]]]], dtype=int32)>\n\n\ncase5 (2,3,4,5,1) stack (2,3,4,5,1) –> (2,3,4,5,2) # axis=4\n\ntf.stack([a,b],axis=-1)\n\n<tf.Tensor: shape=(2, 3, 4, 5, 2), dtype=int32, numpy=\narray([[[[[   0,    0],\n          [   1,   -1],\n          [   2,   -2],\n          [   3,   -3],\n          [   4,   -4]],\n\n         [[   5,   -5],\n          [   6,   -6],\n          [   7,   -7],\n          [   8,   -8],\n          [   9,   -9]],\n\n         [[  10,  -10],\n          [  11,  -11],\n          [  12,  -12],\n          [  13,  -13],\n          [  14,  -14]],\n\n         [[  15,  -15],\n          [  16,  -16],\n          [  17,  -17],\n          [  18,  -18],\n          [  19,  -19]]],\n\n\n        [[[  20,  -20],\n          [  21,  -21],\n          [  22,  -22],\n          [  23,  -23],\n          [  24,  -24]],\n\n         [[  25,  -25],\n          [  26,  -26],\n          [  27,  -27],\n          [  28,  -28],\n          [  29,  -29]],\n\n         [[  30,  -30],\n          [  31,  -31],\n          [  32,  -32],\n          [  33,  -33],\n          [  34,  -34]],\n\n         [[  35,  -35],\n          [  36,  -36],\n          [  37,  -37],\n          [  38,  -38],\n          [  39,  -39]]],\n\n\n        [[[  40,  -40],\n          [  41,  -41],\n          [  42,  -42],\n          [  43,  -43],\n          [  44,  -44]],\n\n         [[  45,  -45],\n          [  46,  -46],\n          [  47,  -47],\n          [  48,  -48],\n          [  49,  -49]],\n\n         [[  50,  -50],\n          [  51,  -51],\n          [  52,  -52],\n          [  53,  -53],\n          [  54,  -54]],\n\n         [[  55,  -55],\n          [  56,  -56],\n          [  57,  -57],\n          [  58,  -58],\n          [  59,  -59]]]],\n\n\n\n       [[[[  60,  -60],\n          [  61,  -61],\n          [  62,  -62],\n          [  63,  -63],\n          [  64,  -64]],\n\n         [[  65,  -65],\n          [  66,  -66],\n          [  67,  -67],\n          [  68,  -68],\n          [  69,  -69]],\n\n         [[  70,  -70],\n          [  71,  -71],\n          [  72,  -72],\n          [  73,  -73],\n          [  74,  -74]],\n\n         [[  75,  -75],\n          [  76,  -76],\n          [  77,  -77],\n          [  78,  -78],\n          [  79,  -79]]],\n\n\n        [[[  80,  -80],\n          [  81,  -81],\n          [  82,  -82],\n          [  83,  -83],\n          [  84,  -84]],\n\n         [[  85,  -85],\n          [  86,  -86],\n          [  87,  -87],\n          [  88,  -88],\n          [  89,  -89]],\n\n         [[  90,  -90],\n          [  91,  -91],\n          [  92,  -92],\n          [  93,  -93],\n          [  94,  -94]],\n\n         [[  95,  -95],\n          [  96,  -96],\n          [  97,  -97],\n          [  98,  -98],\n          [  99,  -99]]],\n\n\n        [[[ 100, -100],\n          [ 101, -101],\n          [ 102, -102],\n          [ 103, -103],\n          [ 104, -104]],\n\n         [[ 105, -105],\n          [ 106, -106],\n          [ 107, -107],\n          [ 108, -108],\n          [ 109, -109]],\n\n         [[ 110, -110],\n          [ 111, -111],\n          [ 112, -112],\n          [ 113, -113],\n          [ 114, -114]],\n\n         [[ 115, -115],\n          [ 116, -116],\n          [ 117, -117],\n          [ 118, -118],\n          [ 119, -119]]]]], dtype=int32)>\n\n\n- 예제: (2,3,4), (2,3,4), (2,3,4)\n\na= tf.reshape(tf.constant(range(2*3*4)),(2,3,4))\nb= -a \nc= 2*a\n\n(예시1) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (6,3,4)\n\ntf.concat([a,b,c],axis=0)\n\n<tf.Tensor: shape=(6, 3, 4), dtype=int32, numpy=\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]],\n\n       [[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]],\n\n       [[  0,   2,   4,   6],\n        [  8,  10,  12,  14],\n        [ 16,  18,  20,  22]],\n\n       [[ 24,  26,  28,  30],\n        [ 32,  34,  36,  38],\n        [ 40,  42,  44,  46]]], dtype=int32)>\n\n\n(예시2) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,9,4)\n\ntf.concat([a,b,c],axis=1)\n\n<tf.Tensor: shape=(2, 9, 4), dtype=int32, numpy=\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11],\n        [  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11],\n        [  0,   2,   4,   6],\n        [  8,  10,  12,  14],\n        [ 16,  18,  20,  22]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23],\n        [-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23],\n        [ 24,  26,  28,  30],\n        [ 32,  34,  36,  38],\n        [ 40,  42,  44,  46]]], dtype=int32)>\n\n\n(예시3) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,3,12)\n\ntf.concat([a,b,c],axis=-1)\n\n<tf.Tensor: shape=(2, 3, 12), dtype=int32, numpy=\narray([[[  0,   1,   2,   3,   0,  -1,  -2,  -3,   0,   2,   4,   6],\n        [  4,   5,   6,   7,  -4,  -5,  -6,  -7,   8,  10,  12,  14],\n        [  8,   9,  10,  11,  -8,  -9, -10, -11,  16,  18,  20,  22]],\n\n       [[ 12,  13,  14,  15, -12, -13, -14, -15,  24,  26,  28,  30],\n        [ 16,  17,  18,  19, -16, -17, -18, -19,  32,  34,  36,  38],\n        [ 20,  21,  22,  23, -20, -21, -22, -23,  40,  42,  44,  46]]],\n      dtype=int32)>\n\n\n(예시4) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (3,2,3,4)\n\ntf.stack([a,b,c],axis=0)\n\n<tf.Tensor: shape=(3, 2, 3, 4), dtype=int32, numpy=\narray([[[[  0,   1,   2,   3],\n         [  4,   5,   6,   7],\n         [  8,   9,  10,  11]],\n\n        [[ 12,  13,  14,  15],\n         [ 16,  17,  18,  19],\n         [ 20,  21,  22,  23]]],\n\n\n       [[[  0,  -1,  -2,  -3],\n         [ -4,  -5,  -6,  -7],\n         [ -8,  -9, -10, -11]],\n\n        [[-12, -13, -14, -15],\n         [-16, -17, -18, -19],\n         [-20, -21, -22, -23]]],\n\n\n       [[[  0,   2,   4,   6],\n         [  8,  10,  12,  14],\n         [ 16,  18,  20,  22]],\n\n        [[ 24,  26,  28,  30],\n         [ 32,  34,  36,  38],\n         [ 40,  42,  44,  46]]]], dtype=int32)>\n\n\n(예시5) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,3,3,4)\n\ntf.stack([a,b,c],axis=1)\n\n<tf.Tensor: shape=(2, 3, 3, 4), dtype=int32, numpy=\narray([[[[  0,   1,   2,   3],\n         [  4,   5,   6,   7],\n         [  8,   9,  10,  11]],\n\n        [[  0,  -1,  -2,  -3],\n         [ -4,  -5,  -6,  -7],\n         [ -8,  -9, -10, -11]],\n\n        [[  0,   2,   4,   6],\n         [  8,  10,  12,  14],\n         [ 16,  18,  20,  22]]],\n\n\n       [[[ 12,  13,  14,  15],\n         [ 16,  17,  18,  19],\n         [ 20,  21,  22,  23]],\n\n        [[-12, -13, -14, -15],\n         [-16, -17, -18, -19],\n         [-20, -21, -22, -23]],\n\n        [[ 24,  26,  28,  30],\n         [ 32,  34,  36,  38],\n         [ 40,  42,  44,  46]]]], dtype=int32)>\n\n\n(예시6) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,3,3,4)\n\ntf.stack([a,b,c],axis=2)\n\n<tf.Tensor: shape=(2, 3, 3, 4), dtype=int32, numpy=\narray([[[[  0,   1,   2,   3],\n         [  0,  -1,  -2,  -3],\n         [  0,   2,   4,   6]],\n\n        [[  4,   5,   6,   7],\n         [ -4,  -5,  -6,  -7],\n         [  8,  10,  12,  14]],\n\n        [[  8,   9,  10,  11],\n         [ -8,  -9, -10, -11],\n         [ 16,  18,  20,  22]]],\n\n\n       [[[ 12,  13,  14,  15],\n         [-12, -13, -14, -15],\n         [ 24,  26,  28,  30]],\n\n        [[ 16,  17,  18,  19],\n         [-16, -17, -18, -19],\n         [ 32,  34,  36,  38]],\n\n        [[ 20,  21,  22,  23],\n         [-20, -21, -22, -23],\n         [ 40,  42,  44,  46]]]], dtype=int32)>\n\n\n(예시7) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,3,4,3)\n\ntf.stack([a,b,c],axis=-1)\n\n<tf.Tensor: shape=(2, 3, 4, 3), dtype=int32, numpy=\narray([[[[  0,   0,   0],\n         [  1,  -1,   2],\n         [  2,  -2,   4],\n         [  3,  -3,   6]],\n\n        [[  4,  -4,   8],\n         [  5,  -5,  10],\n         [  6,  -6,  12],\n         [  7,  -7,  14]],\n\n        [[  8,  -8,  16],\n         [  9,  -9,  18],\n         [ 10, -10,  20],\n         [ 11, -11,  22]]],\n\n\n       [[[ 12, -12,  24],\n         [ 13, -13,  26],\n         [ 14, -14,  28],\n         [ 15, -15,  30]],\n\n        [[ 16, -16,  32],\n         [ 17, -17,  34],\n         [ 18, -18,  36],\n         [ 19, -19,  38]],\n\n        [[ 20, -20,  40],\n         [ 21, -21,  42],\n         [ 22, -22,  44],\n         [ 23, -23,  46]]]], dtype=int32)>\n\n\n- 예제: (2,3,4) (4,3,4) \\(\\to\\) (6,3,4)\n\na=tf.reshape(tf.constant(range(2*3*4)),(2,3,4))\nb=tf.reshape(-tf.constant(range(4*3*4)),(4,3,4))\n\n\ntf.concat([a,b],axis=0)\n\n<tf.Tensor: shape=(6, 3, 4), dtype=int32, numpy=\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]],\n\n       [[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]],\n\n       [[-24, -25, -26, -27],\n        [-28, -29, -30, -31],\n        [-32, -33, -34, -35]],\n\n       [[-36, -37, -38, -39],\n        [-40, -41, -42, -43],\n        [-44, -45, -46, -47]]], dtype=int32)>\n\n\n\ntf.concat([a,b],axis=1)\n\nInvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat\n\n\n\ntf.concat([a,b],axis=2)\n\nInvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat\n\n\n- (2,2) @ (2,) 의 연산?\nnumpy\n\nnp.array([[1,0],[0,1]]) @ np.array([77,-88])\n\narray([ 77, -88])\n\n\n\nnp.array([77,-88]) @ np.array([[1,0],[0,1]])\n\narray([ 77, -88])\n\n\n\nnp.array([[1,0],[0,1]]) @ np.array([77,-88]).reshape(2,1)\n\narray([[ 77],\n       [-88]])\n\n\n\nnp.array([77,-88]).reshape(2,1) @ np.array([[1,0],[0,1]]) \n\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 1)\n\n\n\nnp.array([77,-88]).reshape(1,2) @ np.array([[1,0],[0,1]]) \n\narray([[ 77, -88]])\n\n\ntensorflow\n\nI = tf.constant([[1.0,0.0],[0.0,1.0]]) \nx = tf.constant([77.0,-88.0]) \n\n\nI @ x \n\nInvalidArgumentError: In[0] and In[1] has different ndims: [2,2] vs. [2] [Op:MatMul]\n\n\n\nx @ I\n\nInvalidArgumentError: In[0] and In[1] has different ndims: [2] vs. [2,2] [Op:MatMul]\n\n\n\nI @ tf.reshape(x,(2,1))\n\n<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\narray([[ 77.],\n       [-88.]], dtype=float32)>\n\n\n\ntf.reshape(x,(1,2)) @ I \n\n<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 77., -88.]], dtype=float32)>\n\n\n\n\n\n\ntf.Variable\n\n선언\n- tf.Variable()로 선언\n\ntf.Variable([1,2,3,4])\n\n<tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>\n\n\n\ntf.Variable([1.0,2.0,3.0,4.0])\n\n<tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([1., 2., 3., 4.], dtype=float32)>\n\n\n- tf.constant() 선언후 변환\n\ntf.Variable(tf.constant([1,2,3,4]))\n\n<tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>\n\n\n- np 등으로 선언후 변환\n\ntf.Variable(np.array([1,2,3,4]))\n\n<tf.Variable 'Variable:0' shape=(4,) dtype=int64, numpy=array([1, 2, 3, 4])>\n\n\n\n\n타입\n\ntype(tf.Variable([1,2,3,4]))\n\ntensorflow.python.ops.resource_variable_ops.ResourceVariable\n\n\n\n\n인덱싱\n\na=tf.Variable([1,2,3,4])\na\n\n<tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>\n\n\n\na[:2]\n\n<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>\n\n\n\n\n연산가능\n\na=tf.Variable([1,2,3,4])\nb=tf.Variable([-1,-2,-3,-4])\n\n\na+b\n\n<tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>\n\n\n\n\ntf.Variable도 쓰기 불편함\n\ntf.Variable([1,2])+tf.Variable([3.14,3.14])\n\nInvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:AddV2]\n\n\n\n\ntnp의 은총도 일부만 가능\n\nimport tensorflow.experimental.numpy as tnp \ntnp.experimental_enable_numpy_behavior() \n\n- 알아서 형 변환\n\ntf.Variable([1,2])+tf.Variable([3.14,3.14])\n\n<tf.Tensor: shape=(2,), dtype=float64, numpy=array([4.1400001, 5.1400001])>\n\n\n- .reshape 메소드\n\ntf.Variable([1,2,3,4]).reshape(2,2)\n\nAttributeError: 'ResourceVariable' object has no attribute 'reshape'\n\n\n\n\n대부분의 동작은 tf.constant랑 큰 차이를 모르겠음\n- tf.concat\n\na= tf.Variable([[1,2],[3,4]]) \nb= tf.Variable([[-1,-2],[-3,-4]]) \ntf.concat([a,b],axis=0)\n\n<tf.Tensor: shape=(4, 2), dtype=int32, numpy=\narray([[ 1,  2],\n       [ 3,  4],\n       [-1, -2],\n       [-3, -4]], dtype=int32)>\n\n\n- tf.stack\n\na= tf.Variable([[1,2],[3,4]]) \nb= tf.Variable([[-1,-2],[-3,-4]]) \ntf.stack([a,b],axis=0)\n\n<tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\narray([[[ 1,  2],\n        [ 3,  4]],\n\n       [[-1, -2],\n        [-3, -4]]], dtype=int32)>\n\n\n\n\n변수값변경가능(?)\n\na= tf.Variable([1,2,3,4])\nid(a)\n\n140135926541824\n\n\n\na.assign_add([-1,-2,-3,-4])\nid(a)\n\n140135926541824\n\n\n\n\n요약\n- tf.Variable()로 만들어야 하는 뚜렷한 차이는 모르겠음.\n- 애써 tf.Variable()로 만들어도 간단한연산을 하면 그 결과는 tf.constant()로 만든 오브젝트와 동일해짐.\n\n\n\n미분\n\n모티브\n- 예제: 컴퓨터를 이용하여 \\(x=2\\)에서 \\(y=3x^2\\)의 접선의 기울기를 구해보자.\n(손풀이)\n\\[\\frac{dy}{dx}=6x\\]\n이므로 \\(x=2\\)를 대입하면 12이다.\n(컴퓨터를 이용한 풀이)\n단계1\n\nx1=2 \ny1= 3*x1**2 \n\n\nx2=2+0.000000001\ny2= 3*x2**2\n\n\n(y2-y1)/(x2-x1)\n\n12.0\n\n\n단계2\n\ndef f(x):\n    return(3*x**2)\n\n\nf(3)\n\n27\n\n\n\ndef d(f,x):\n    return (f(x+0.000000001)-f(x))/0.000000001\n\n\nd(f,2)\n\n12.000000992884452\n\n\n단계3\n\nd(lambda x: 3*x**2 ,2)\n\n12.000000992884452\n\n\n\nd(lambda x: x**2 ,0)\n\n1e-09\n\n\n단계4\n\\[f(x,y)= x^2 +3y\\]\n\ndef f(x,y):\n    return(x**2 +3*y)\n\n\nd(f,(2,3))\n\nTypeError: can only concatenate tuple (not \"float\") to tuple\n\n\n\n\ntf.GradientTape() 사용방법\n- 예제1: \\(x=2\\)에서 \\(y=3x^2\\)의 도함수값을 구하라.\n\nx=tf.Variable(2.0)\na=tf.constant(3.0)\n\n\nmytape=tf.GradientTape()\nmytape.__enter__() # 기록 시작 \ny=a*x**2 # y=ax^2 = 3x^2\nmytape.__exit__(None,None,None) # 기록 끝 \n\n\nmytape.gradient(y,x) # y를 x로 미분하라. \n\n<tf.Tensor: shape=(), dtype=float32, numpy=12.0>\n\n\n- 예제2: 조금 다른예제\n\nx=tf.Variable(2.0)\n#a=tf.constant(3.0)\n\nmytape=tf.GradientTape()\nmytape.__enter__() # 기록 시작 \na=(x/2)*3 ## a=(3/2)x \ny=a*x**2  ## y=ax^2 = (3/2)x^3\nmytape.__exit__(None,None,None) # 기록 끝 \n\nmytape.gradient(y,x) # y를 x로 미분하라. \n\n<tf.Tensor: shape=(), dtype=float32, numpy=18.0>\n\n\n\\[a=\\frac{3}{2}x\\] \\[y=ax^2=\\frac{3}{2}x^3\\]\n\\[\\frac{dy}{dx}=\\frac{3}{2} 3x^2\\]\n\n3/2*3*4\n\n18.0\n\n\n- 테이프의 개념 (\\(\\star\\))\n(상황)\n우리가 어려운 미분계산을 컴퓨터에게 부탁하는 상황임. (예를들면 \\(y=3x^2\\)) 컴퓨터에게 부탁을 하기 위해서는 연습장(=테이프)에 \\(y=3x^2\\)이라는 수식을 써서 보여줘야하는데 이때 컴퓨터에게 target이 무엇인지 그리고 무엇으로 미분하고 싶은 것인지를 명시해야함.\n\nmytape = tf.GradientTape(): tf.GradientTape()는 연습장을 만드는 명령어, 만들어진 연습장을 mytape라고 이름을 붙인다.\nmytape.__enter__(): 만들어진 공책을 연다 (=기록할수 있는 상태로 만든다)\na=x/2*3; y=a*x**2: 컴퓨터에게 전달할 수식을 쓴다\nmytape.__exit__(None,None,None): 공책을 닫는다.\nmytape.gradient(y,x): \\(y\\)를 \\(x\\)로 미분하라는 메모를 남기고 컴퓨터에게 전달한다.\n\n- 예제3: 연습장을 언제 열고 닫을지 결정하는건 중요하다.\n\nx=tf.Variable(2.0)\na=(x/2)*3 ## a=(3/2)x\n\nmytape=tf.GradientTape()\nmytape.__enter__() # 기록 시작 \ny=a*x**2  ## y=ax^2 = (3/2)x^3\nmytape.__exit__(None,None,None) # 기록 끝 \n\nmytape.gradient(y,x) # y를 x로 미분하라. \n\n<tf.Tensor: shape=(), dtype=float32, numpy=12.0>\n\n\n- 예제4: with문과 함께 쓰는 tf.GradientTape()\n\nx=tf.Variable(2.0)\na=(x/2)*3 \n\n\nwith tf.GradientTape() as mytape:\n    ## with문 시작 \n    y=a*x**2 \n    ## with문 끝 \n\n\nmytape.gradient(y,x) # y를 x로 미분하라.\n\n<tf.Tensor: shape=(), dtype=float32, numpy=12.0>\n\n\n(문법해설)\n아래와 같이 쓴다.\nwith expression as myname:\n    ## with문 시작: myname.__enter__() \n    blabla ~ \n    yadiyadi !! \n    ## with문 끝: myname.__exit__()\n\nexpression 의 실행결과 오브젝트가 생성, 생성된 오브젝트는 myname라고 이름붙임. 이 오브젝트는 .__enter__()와 .__exit__()를 숨겨진 기능으로 포함해야 한다.\nwith문이 시작되면서 myname.__enter__()이 실행된다.\n블라블라와 야디야디가 실행된다.\nwith문이 종료되면서 myname.__exit__()이 실행된다.\n\n- 예제5: 예제2를 with문과 함께 구현\n\nx=tf.Variable(2.0)\n\nwith tf.GradientTape() as mytape:\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\nmytape.gradient(y,x) # y를 x로 미분하라. \n\n<tf.Tensor: shape=(), dtype=float32, numpy=18.0>\n\n\n- 예제6: persistent = True\n(관찰1)\n\nx=tf.Variable(2.0)\n\nwith tf.GradientTape() as mytape:\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nmytape.gradient(y,x) # 2번이상 실행해서 에러를 관측하라\n\n<tf.Tensor: shape=(), dtype=float32, numpy=18.0>\n\n\n(관찰2)\n\nx=tf.Variable(2.0)\n\nwith tf.GradientTape(persistent=True) as mytape:\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nmytape.gradient(y,x) # 2번이상실행해도 에러가 나지않음 \n\n<tf.Tensor: shape=(), dtype=float32, numpy=18.0>\n\n\n- 예제7: watch\n(관찰1)\n\nx=tf.constant(2.0)\n\nwith tf.GradientTape(persistent=True) as mytape:\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\nNone\n\n\n(관찰2)\n\nx=tf.constant(2.0)\nwith tf.GradientTape(persistent=True) as mytape:\n    mytape.watch(x) # 수동감시\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\ntf.Tensor(18.0, shape=(), dtype=float32)\n\n\n(관찰3)\n\nx=tf.Variable(2.0)\nwith tf.GradientTape(persistent=True,watch_accessed_variables=False) as mytape: # 자동감시 모드 해제 \n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\nNone\n\n\n(관찰4)\n\nx=tf.Variable(2.0)\nwith tf.GradientTape(persistent=True,watch_accessed_variables=False) as mytape: # 자동감시 모드 해제\n    mytape.watch(x)\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\ntf.Tensor(18.0, shape=(), dtype=float32)\n\n\n(관찰5)\n\nx=tf.Variable(2.0)\nwith tf.GradientTape(persistent=True) as mytape: \n    mytape.watch(x)\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\ntf.Tensor(18.0, shape=(), dtype=float32)\n\n\n- 예제9: 카페예제로 돌아오자.\n- 예제10: 카페예제의 매트릭스 버전\n- 예제11: 위의 예제에서 이론적인 \\(\\boldsymbol{\\beta}\\)의 최적값을 찾아보고 (즉 \\(\\hat{\\boldsymbol{\\beta}}\\)을 찾고) 그곳에서 loss의 미분을 구하라. 구한결과가 \\(\\begin{bmatrix}0 \\\\ 0 \\end{bmatrix}\\) 임을 확인하라."
  },
  {
    "objectID": "posts/2022-01-14-회귀분석-변수선택.html",
    "href": "posts/2022-01-14-회귀분석-변수선택.html",
    "title": "jisim12",
    "section": "",
    "text": "회귀분석\n\n\ntoc:false\nbranch: master\nbadges: true\ncomments: true\nauthor: 심재인\n\n\n\n\nGoal : explain the response with minimum number of explanatory variables\n\nFull Model\n\n\\(y_i = \\beta_0+\\beta_1x_{i1}+\\dots+\\beta_px_{ip}+\\dots+\\beta_qx_{iq}+\\epsilon_i\\)\n\n\\(\\epsilon_i\\sim_{idd}N(0,\\epsilon^2)\\)\n\n\\(SSE_q = ||Y-X\\hat{\\beta^*}||^2,\\quad\\hat{\\sigma_{q}^2}=SSE_q/(n-p-1)\\)\n-Current Model\n\n\\(y_i = \\beta_0+\\beta_1x_{i1}+\\dots+\\beta_px_{ip}\\epsilon_i\\),\n\n\\(\\epsilon_i\\sim_{idd}N(0,\\epsilon_{p}^2)\\)\n\n\\(SSE_p = ||Y-X_{p}\\hat{\\beta^p}||^2,\\quad\\hat{\\sigma_{p}^2}=SSE_p/(n-p-1)\\)\n\n\nStatistics used in model selection\n\nResidual mean squares error (MSE) : \\(MSE = \\frac{SSE_P}{(n-p-1)}\\)\n\ncoefficient of determination : \\(R^2 = \\frac{SSR}{SST}=1-\\frac{SSE_p}{SST}\\)\n\nAdjusted \\(R^2 : R^2_{adj} = 1-\\frac{SSE_p/(n-p-1)}{SST/(n-1)}\\)\n\nPartial F-test statistics\n\n\n\n\n\n\n부분 F-검정통계량\n\\(H_0:\\beta_p=0_{|\\beta_0,\\beta_1,\\dots,\\beta_{p-1}}\\;vs. H_1:\\beta_p\\neq 0_{|\\beta_0,\\beta_1,\\dots,\\beta_{p-1}}\\)\n\n부분 검정통계량 : \\(F_0 = \\frac{SSR_{FM}-SSR_{RM}}{MSE_{FM}}\\qquad 유의확률\\begin{cases}낮으면\\;추가 \\\\ 높으면\\;제거 \\end{cases}\\)\n\n\\(FM\\) : 변수 \\(p\\)개, \\(RM\\) : 변수 \\((p-1)\\)개\n\n\\(F_0\\) ~ \\(F(1,n-p-1), under\\;H_0\\)\n\n\\(F_0 \\geq F_c = F_{\\alpha}(1,n-p-1) \\Rightarrow H_0\\) 기각 \\(\\qquad f_0\\begin{cases}높으면\\;추가 \\\\ 낮으면\\;제거 \\end{cases}\\)\n\n\n\n\n\n\nVariable selection.\n\nAll possible regression : 모든 가능한 회귀 \\(\\Rightarrow\\) 수가 많아지면 너무 오래 걸린다\n\nBackward Elimination : 후진 제거법 \\(\\Rightarrow\\) 필요없는것부터 제거 \\(\\qquad\\;\\) (단, 한번 제거되면 무조건 제거)\n\nForward Selection : 전진 선택법 \\(\\Rightarrow\\) 하나씩 추가해보며 하는 것 \\(\\qquad\\) (단, 한번 추가되면 무조건 추가)\n\nStepwise regression : 단계별 회귀 \\(\\Rightarrow\\) 후진 제거법 + 전진 선택법\n\n\nAll possible regression\n\n모든 가능한 변수들의 조합 \\((2^p)\\)을 회귀분석하여 결과 비교\n\n시간이 오래 걸림\n\n\\(R^2\\)또는 \\(MSE\\) 사용\n\n\nBackward Elimination\n(step 0) 모든 변수를 포함한 회귀방정식 적합 (Full Model).\n(step 1) 변수 하나하나씩에 대한 부분 F-검정통계량 \\(F_0\\) 구함\n(step 2) 가장 작은 부분 F-검정통계량 \\(F_L\\)과 \\(F_c\\)를 구함\n(step 3) \\(F_L < F_c\\) 이면 \\(x_L\\) 제거 \\(\\Rightarrow\\) (step 2)로\n\\(\\qquad\\quad F_L\\geq F_c\\) 이면 멈춘 후 최종모형으로 선택\nForward Selection\n(step 0) 변수 하나하나씩에 대한 회귀모형 적합 후 \\(R^2\\) 를 가장 크게 하는 설명변수 선택\n(step 1) 변수를 하나하나씩 추가하여 \\(R^2\\)를 가장 크게 하는 변수 선택 \\((x_s)\\)\n(step 2) 위에서 추가된 변수 \\(x_s\\)에 대해 부분 F-검정 수행\n(step 3) 위의 결과가 유의하면 $x_s$ 추가하고 (step 1)으로, 유의하지 않으면 멈춘 후 $x_s$를 제외한 모형을 최종모형으로 선택\n\nStepwise regression\n(step 0) 전진선택법의 (step 0)와 동일\n(step 1) 변수를 하나하나씩 추가하여 \\(R^2\\)를 가장 크게 하는 변수 선택 \\(x_s\\)\n(step 2) 위에서 추가된 변수 \\(x_s\\)에 대해 부분 F-검정 수행\n(step 3) 위의 결과가 유의하면 \\(x_s\\) 포함하고 (step 4)로, 유의하지 않으면 멈춘 후 \\(x_s\\)를 제외한 모형을 최종모형으로 선택\n(step 4) 포함된 변수에 대해 부분 F-검정을 실시하여 유의하지 않은 변수가 있으면 제거하고 (step 1)로"
  },
  {
    "objectID": "posts/2022-04-30-중간고사해설.html",
    "href": "posts/2022-04-30-중간고사해설.html",
    "title": "jisim12",
    "section": "",
    "text": "중간고사해설\n\n빅데이터분석특강\n\n\ntoc:false\nbranch: master\nbadges: true\ncomments: true\nauthor: 심재인\n\n\nimports\n\nimport numpy as np\nimport tensorflow as tf \nimport tensorflow.experimental.numpy as tnp\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport matplotlib.pyplot as plt \n\n\n\n1. 경사하강법과 tf.GradientTape()의 사용방법 (30점)\n(1) 아래는 \\(X_i \\overset{iid}{\\sim} N(3,2^2)\\) 를 생성하는 코드이다.\n\ntf.random.set_seed(43052)\nx= tnp.random.randn(10000)*2+3\nx\n\n2022-05-04 14:57:40.284732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n<tf.Tensor: shape=(10000,), dtype=float64, numpy=\narray([ 4.12539849,  5.46696729,  5.27243374, ...,  2.89712332,\n        5.01072291, -1.13050477])>\n\n\n함수 \\(L(\\mu,\\sigma)\\)을 최대화하는 \\((\\mu,\\sigma)\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\(\\mu\\)의 초기값은 2로 \\(\\sigma\\)의 초기값은 3으로 설정할 것)\n\\[L(\\mu,\\sigma)=\\prod_{i=1}^{n}f(x_i), \\quad f(x_i)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x_i-\\mu}{\\sigma})^2}\\]\n(풀이)\n\nsigma = tf.Variable(3.0) \nmu = tf.Variable(2.0)\n\n\nwith tf.GradientTape() as tape: \n    pdf = 1/sigma * tnp.exp(-0.5*((x-mu)/sigma)**2)\n    logL = tf.reduce_sum(tnp.log(pdf) ) \ntape.gradient(logL,[mu,sigma]) \n\n[<tf.Tensor: shape=(), dtype=float32, numpy=1129.3353>,\n <tf.Tensor: shape=(), dtype=float32, numpy=-1488.3431>]\n\n\n\nfor i in range(1000):\n    with tf.GradientTape() as tape: \n        pdf = 1/sigma * tnp.exp(-0.5*((x-mu)/sigma)**2)\n        logL = tf.reduce_sum(tnp.log(pdf) ) \n    slope1, slope2 = tape.gradient(logL,[mu,sigma]) \n    mu.assign_add(slope1* 0.1/10000) # N=10000 \n    sigma.assign_add(slope2* 0.1/10000) \n\n\nmu,sigma\n\n(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.0163972>,\n <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9870595>)\n\n\n(2) 아래는 \\(X_i \\overset{iid}{\\sim} Ber(0.8)\\)을 생성하는 코드이다.\n\ntf.random.set_seed(43052)\nx= tf.constant(np.random.binomial(1,0.8,(10000,)))\nx\n\n<tf.Tensor: shape=(10000,), dtype=int64, numpy=array([1, 1, 1, ..., 1, 1, 1])>\n\n\n함수 \\(L(p)\\)을 최대화하는 \\(p\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\(p\\)의 초기값은 0.3으로 설정할 것)\n\\[L(\\mu,\\sigma)=\\prod_{i=1}^{n}f(x_i), \\quad f(x_i)=p^{x_i}(1-p)^{1-x_i}\\]\n(풀이)\n\np=tf.Variable(0.3) \nfor i in range(1000):\n    with tf.GradientTape() as tape: \n        pdf = p**x * (1-p)**(1-x) \n        logL = tf.reduce_sum(tnp.log(pdf)) \n    slope = tape.gradient(logL,p) \n    p.assign_add(slope* 0.1/10000) # N=10000 \n\n\np\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.8002>\n\n\n(3) 아래의 모형에 따라서 \\(\\{Y_i\\}_{i=1}^{10000}\\)를 생성하는 코드를 작성하라. - \\(Y_i \\overset{iid}{\\sim} N(\\mu_i,1)\\) - \\(\\mu_i = \\beta_0 + \\beta_1 x_i = 0.5 + 2 x_i\\) , where \\(x_i = \\frac{i}{10000}\\)\n(풀이)\n\nx= tf.constant(np.arange(1,10001)/10000)\ny= tnp.random.randn(10000) + (0.5 + 2*x) \n\n\nbeta0= tf.Variable(1.0)\nbeta1= tf.Variable(1.0)\nfor i in range(2000):\n    with tf.GradientTape() as tape:\n        mu = beta0 + beta1*x\n        pdf = tnp.exp(-0.5*(y-mu)**2)\n        logL = tf.reduce_sum(tnp.log(pdf))\n    slope1, slope2 = tape.gradient(logL,[beta0,beta1])\n    beta0.assign_add(slope1* 0.1/10000) # N=10000\n    beta1.assign_add(slope2* 0.1/10000)\n\n\nbeta0, beta1\n\n(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.5316726>,\n <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9530618>)\n\n\n\n\n2. 회귀분석의 이론적해와 tf.keras.optimizer 이용방법 (20점)\n아래와 같은 선형모형을 고려하자.\n\\[y_i = \\beta_0 + \\beta_1 x_i +\\epsilon_i.\\]\n이때 오차항은 정규분포로 가정한다. 즉 \\(\\epsilon_i \\overset{iid}{\\sim} N(0,\\sigma^2)\\)라고 가정한다.\n관측데이터가 아래와 같을때 아래의 물음에 답하라.\n\nx= tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4])\n\nX= tnp.array([[1.0, 20.1], [1.0, 22.2], [1.0, 22.7], [1.0, 23.3], [1.0, 24.4],\n              [1.0, 25.1], [1.0, 26.2], [1.0, 27.3], [1.0, 28.4], [1.0, 30.4]])\ny= tnp.array([55.4183651 , 58.19427589, 61.23082496, 62.31255873, 63.1070028 , \n              63.69569103, 67.24704918, 71.43650092, 73.10130336, 77.84988286]).reshape(10,1)\n\n(1) MSE loss를 최소화 하는 \\(\\beta_0,\\beta_1\\)의 해석해를 구하라.\n(풀이)\n\ntf.linalg.inv(X.T @ X ) @ X.T @ y\n\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[9.94457323],\n       [2.21570461]])>\n\n\n(2) 경사하강법과 MSE loss의 도함수를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\n주의 tf.GradeintTape()를 이용하지 말고 MSE loss의 해석적 도함수를 사용할 것.\n(풀이)\n\nbeta= tnp.array([5,10]).reshape(2,1)\n\n\nfor i in range(50000): \n    beta = beta - 0.0015 * (-2*X.T @y + 2*X.T@X@beta)/10\n\n\nbeta\n\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[9.28579424],\n       [2.24168098]])>\n\n\n(3) tf.keras.optimizers의 apply_gradients()를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\n(풀이)\n\nbeta = tf.Variable(tnp.array([5.0,10.0]).reshape(2,1)) \nopt = tf.optimizers.SGD(0.0015) \nfor i in range(50000): \n    with tf.GradientTape() as tape: \n        loss = (y-X@beta).T @ (y-X@beta) / 10 \n    slope = tape.gradient(loss,beta) \n    opt.apply_gradients([(slope,beta)])\n\n\nbeta\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[9.28579425],\n       [2.24168098]])>\n\n\n(4) tf.keras.optimizers의 minimize()를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\n(풀이)\n\nbeta = tf.Variable(tnp.array([5.0,10.0]).reshape(2,1)) \nopt = tf.optimizers.SGD(0.0015) \nloss_fn = lambda: (y-X@beta).T @ (y-X@beta) / 10 \nfor i in range(50000): \n    opt.minimize(loss_fn,beta)  \n\n\nbeta\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[9.28579425],\n       [2.24168098]])>\n\n\n\n\n3. keras를 이용한 풀이 (30점)\n(1) 아래와 같은 모형을 고려하자.\n\\[y_i= \\beta_0 + \\sum_{k=1}^{5} \\beta_k \\cos(k t_i)+\\epsilon_i, \\quad i=0,1,\\dots, 999\\]\n여기에서 \\(t_i=\\frac{2\\pi i}{1000}\\) 이다. 그리고 \\(\\epsilon_i \\sim i.i.d~ N(0,\\sigma^2)\\), 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자.\n\nnp.random.seed(43052)\nt= np.array(range(1000))* np.pi/1000\ny = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2\nplt.plot(t,y,'.',alpha=0.2)\n\n\n\n\ntf.keras를 이용하여 \\(\\beta_0,\\dots,\\beta_5\\)를 추정하라. (\\(\\beta_0,\\dots,\\beta_5\\)의 참값은 각각 -2,3,1,0,0,0.5 이다)\n(풀이)\n\ny = y.reshape(1000,1)\nx1 = np.cos(t) \nx2 = np.cos(2*t)\nx3 = np.cos(3*t)\nx4 = np.cos(4*t)\nx5 = np.cos(5*t)\nX = tf.stack([x1,x2,x3,x4,x5],axis=1)\n\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1)) \nnet.compile(loss='mse',optimizer='sgd') \nnet.fit(X,y,batch_size=1000, epochs = 1000, verbose=0)\n\n<keras.callbacks.History at 0x7f8544703100>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense/kernel:0' shape=(5, 1) dtype=float32, numpy=\n array([[ 3.0008404e+00],\n        [ 1.0067019e+00],\n        [ 1.8562055e-03],\n        [-3.8460968e-03],\n        [ 4.9710521e-01]], dtype=float32)>,\n <tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([-2.0122595], dtype=float32)>]\n\n\n(2) 아래와 같은 모형을 고려하자.\n\\[y_i \\sim Ber(\\pi_i), ~ \\text{where} ~ \\pi_i=\\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\]\n위의 모형에서 관측한 데이터는 아래와 같다.\n\ntf.random.set_seed(43052)\nx = tnp.linspace(-1,1,2000) \ny = tf.constant(np.random.binomial(1, tf.nn.sigmoid(-1+5*x)),dtype=tf.float64) \nplt.plot(x,y,'.',alpha=0.05)\n\n\n\n\ntf.keras를 이용하여 \\(w_0,w_1\\)을 추정하라. (참고: \\(w_0, w_1\\)에 대한 참값은 -1과 5이다.)\n(풀이)\n\nx= x.reshape(2000,1)\ny= y.reshape(2000,1)\n\n\nnet= tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='sgd', loss= tf.losses.binary_crossentropy) \nnet.fit(x,y,epochs=10000,batch_size=2000, verbose=0)\n\n<keras.callbacks.History at 0x7f854458fee0>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense_1/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[4.232856]], dtype=float32)>,\n <tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32, numpy=array([-0.90837014], dtype=float32)>]\n\n\n\nplt.plot(y,'.')\nplt.plot(net(x),'--')\n\n\n\n\n\n\n4. Piecewise-linear regression (15점)\n아래의 모형을 고려하자.\nmodel: \\(y_i=\\begin{cases} x_i +0.3\\epsilon_i & x\\leq 0 \\\\ 3.5x_i +0.3\\epsilon_i & x>0 \\end{cases}\\)\n아래는 위의 모형에서 생성한 샘플이다.\n\nnp.random.seed(43052)\nN=100\nx= np.linspace(-1,1,N).reshape(N,1)\ny= np.array(list(map(lambda x: x*1+np.random.normal()*0.3 if x<0 else x*3.5+np.random.normal()*0.3,x))).reshape(N,1)\n\n(1) 다음은 \\((x_i,y_i)\\)를 아래와 같은 아키텍처로 적합시키는 코드이다.\n\n$ = _0+_1x $\n\n\ntf.random.set_seed(43054) \nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1)) \nnet.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\nnet.fit(x,y,batch_size=N,epochs=1000,verbose=0) # numpy로 해도 돌아감\n\n<keras.callbacks.History at 0x7f85c004b2e0>\n\n\n케라스에 의해 추정된 \\(\\hat{\\beta}_0,\\hat{\\beta}_1\\)을 구하라.\n\nnet.weights\n\n[<tf.Variable 'dense_2/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[2.2616348]], dtype=float32)>,\n <tf.Variable 'dense_2/bias:0' shape=(1,) dtype=float32, numpy=array([0.6069048], dtype=float32)>]\n\n\n(풀이)\n\n\\(\\hat{\\beta}_0= 0.6069048\\)\n\\(\\hat{\\beta}_1= 2.2616348\\)\n\n(2) 다음은 \\((x_i,y_i)\\)를 아래와 같은 아키텍처로 적합시키는 코드이다.\n\n\\(\\boldsymbol{u}= x\\boldsymbol{W}^{(1)}+\\boldsymbol{b}^{(1)}\\)\n\\(\\boldsymbol{v}= \\text{relu}(u)\\)\n\\(yhat= \\boldsymbol{v}\\boldsymbol{W}^{(2)}+b^{(2)}\\)\n\n\ntf.random.set_seed(43056) \n## 1단계\nnet = tf.keras.Sequential() \nnet.add(tf.keras.layers.Dense(2))\nnet.add(tf.keras.layers.Activation('relu')) \nnet.add(tf.keras.layers.Dense(1))\nnet.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n<keras.callbacks.History at 0x7f86aefa36a0>\n\n\n\\({\\boldsymbol u}\\)를 이용하여 \\({\\boldsymbol v}\\)를 만드는 코드와 \\({\\boldsymbol v}\\)를 이용하여 \\(yhat\\)를 만드는 코드를 작성하라.\n(풀이)\n\nu=net.layers[0](x)\nv=net.layers[1](u) \nyhat=net.layers[2](v)\n\n(3) 아래는 (1)-(2)번 모형에 대한 discussion이다. 올바른 것을 모두 골라라.\n(곤이) (2) 모형은 활성화함수로 relu를 사용하였다.\n(철용) (1) 모형에서 추정해야할 파라메터의 수는 2개이다.\n(아귀) (2) 모형이 (1) 모형보다 복잡한 모형이다.\n(짝귀) (1) 의 모형은 오버피팅의 위험이 있다.\n\n\n5. 다음을 잘 읽고 참과 거짓을 판단하라. (5점)\n(1) 적절한 학습률이 선택된다면, 경사하강법은 손실함수가 convex일때 언제나 전역최소해를 찾을 수 있다.\n(2) tf.GradeintTape()는 경사하강법을 이용하여 최적점을 찾아주는 tool이다.\n(3) 학습률이 크다는 것은 파라메터는 1회 업데이트 하는 양이 크다는 것을 의미한다.\n(4) 학습률이 크면 학습파라메터의 수렴속도가 빨라지지만 때때로 과적합에 빠질 수도 있다.\n(5) 단순회귀분석에서 MSE loss를 최소화 하는 해는 경사하강법을 이용하지 않아도 해석적으로 구할 수 있다."
  },
  {
    "objectID": "posts/2021-12-20-데이터시각화_기말고사풀이.html",
    "href": "posts/2021-12-20-데이터시각화_기말고사풀이.html",
    "title": "jisim12",
    "section": "",
    "text": "toc:false\nbranch: master\nbadges: true\ncomments: true\nauthor: 심재인\n\n\n#hide\nimport requests\nfrom IPython.display import HTML\nimport plotly.express as px\n\n\n\n아래의 그림은 COVID19 예방접종의 시도별 현황을 캡쳐한 것이다.\n\n이 정보는 특정 주기로 업데이트 되며 아래의 웹페이지 2번째 테이블에서 확인할 수 있다.\nhttps://ncv.kdca.go.kr/mainStatus.es?mid=a11702000000\n판다스의 pd.read_html() 함수를 이용해 위의 페이지에서 그림1의 테이블을 읽어오라. 그리고 folium의 choroplethmap을 활용하여 시도별 2차접종의 접종률을 시각화 하라. 시각화 예시는 아래와 같다.\n\n(풀이)\n- 데이터프레임 읽어오기\n\nimport pandas as pd\n\n\ndf=pd.read_html('https://ncv.kdca.go.kr/mainStatus.es?mid=a11702000000',encoding='utf-8')[1]\ndf\n\n\n\n\n\n  \n    \n      \n      구분\n      1차접종\n      2차접종\n      3차접종\n    \n    \n      \n      구분\n      당일 실적\n      당일 누계\n      당일 실적\n      당일 누계\n      당일 실적\n      당일 누계\n    \n  \n  \n    \n      0\n      합계\n      5851\n      43493325\n      1743\n      42070660\n      29240\n      11565083\n    \n    \n      1\n      서울\n      1092\n      8081511\n      327\n      7848156\n      6576\n      2173116\n    \n    \n      2\n      부산\n      323\n      2785658\n      96\n      2686254\n      1691\n      766316\n    \n    \n      3\n      대구\n      173\n      1938156\n      37\n      1867302\n      596\n      443843\n    \n    \n      4\n      인천\n      525\n      2480025\n      168\n      2400474\n      2436\n      632558\n    \n    \n      5\n      광주\n      72\n      1216505\n      14\n      1173233\n      267\n      316210\n    \n    \n      6\n      대전\n      197\n      1199583\n      42\n      1157132\n      582\n      293704\n    \n    \n      7\n      울산\n      80\n      928762\n      19\n      896509\n      388\n      207834\n    \n    \n      8\n      세종\n      54\n      279728\n      15\n      268580\n      166\n      64957\n    \n    \n      9\n      경기\n      2272\n      11384986\n      705\n      11016264\n      11819\n      2817113\n    \n    \n      10\n      강원\n      77\n      1303677\n      21\n      1263853\n      269\n      385809\n    \n    \n      11\n      충북\n      113\n      1376943\n      34\n      1334316\n      805\n      382193\n    \n    \n      12\n      충남\n      245\n      1825333\n      77\n      1765603\n      1057\n      521523\n    \n    \n      13\n      전북\n      111\n      1545850\n      29\n      1497144\n      584\n      508956\n    \n    \n      14\n      전남\n      101\n      1603261\n      28\n      1554519\n      481\n      544769\n    \n    \n      15\n      경북\n      106\n      2201519\n      41\n      2126360\n      338\n      618715\n    \n    \n      16\n      경남\n      218\n      2778313\n      62\n      2673402\n      833\n      750044\n    \n    \n      17\n      제주\n      92\n      563515\n      28\n      541559\n      352\n      137423\n    \n  \n\n\n\n\n- json 파일\n\nimport json\nimport requests\n\n\nglobal_distriction_jsonurl='https://raw.githubusercontent.com/southkorea/southkorea-maps/master/kostat/2018/json/skorea-provinces-2018-geo.json'\nglobal_dict = json.loads(requests.get(global_distriction_jsonurl).text)\n\n\nprov=[global_dict['features'][i]['properties']['name'] for i in range(17)]\nprov \n\n['서울특별시',\n '부산광역시',\n '대구광역시',\n '인천광역시',\n '광주광역시',\n '대전광역시',\n '울산광역시',\n '세종특별자치시',\n '경기도',\n '강원도',\n '충청북도',\n '충청남도',\n '전라북도',\n '전라남도',\n '경상북도',\n '경상남도',\n '제주특별자치도']\n\n\n- 변형\n\ndf.iloc[1:,1:].assign(prov=prov)\n\n\n\n\n\n  \n    \n      \n      1차접종\n      2차접종\n      3차접종\n      prov\n    \n    \n      \n      당일 실적\n      당일 누계\n      당일 실적\n      당일 누계\n      당일 실적\n      당일 누계\n      \n    \n  \n  \n    \n      1\n      1092\n      8081511\n      327\n      7848156\n      6576\n      2173116\n      서울특별시\n    \n    \n      2\n      323\n      2785658\n      96\n      2686254\n      1691\n      766316\n      부산광역시\n    \n    \n      3\n      173\n      1938156\n      37\n      1867302\n      596\n      443843\n      대구광역시\n    \n    \n      4\n      525\n      2480025\n      168\n      2400474\n      2436\n      632558\n      인천광역시\n    \n    \n      5\n      72\n      1216505\n      14\n      1173233\n      267\n      316210\n      광주광역시\n    \n    \n      6\n      197\n      1199583\n      42\n      1157132\n      582\n      293704\n      대전광역시\n    \n    \n      7\n      80\n      928762\n      19\n      896509\n      388\n      207834\n      울산광역시\n    \n    \n      8\n      54\n      279728\n      15\n      268580\n      166\n      64957\n      세종특별자치시\n    \n    \n      9\n      2272\n      11384986\n      705\n      11016264\n      11819\n      2817113\n      경기도\n    \n    \n      10\n      77\n      1303677\n      21\n      1263853\n      269\n      385809\n      강원도\n    \n    \n      11\n      113\n      1376943\n      34\n      1334316\n      805\n      382193\n      충청북도\n    \n    \n      12\n      245\n      1825333\n      77\n      1765603\n      1057\n      521523\n      충청남도\n    \n    \n      13\n      111\n      1545850\n      29\n      1497144\n      584\n      508956\n      전라북도\n    \n    \n      14\n      101\n      1603261\n      28\n      1554519\n      481\n      544769\n      전라남도\n    \n    \n      15\n      106\n      2201519\n      41\n      2126360\n      338\n      618715\n      경상북도\n    \n    \n      16\n      218\n      2778313\n      62\n      2673402\n      833\n      750044\n      경상남도\n    \n    \n      17\n      92\n      563515\n      28\n      541559\n      352\n      137423\n      제주특별자치도\n    \n  \n\n\n\n\n\ndf.iloc[1:,1:].assign(prov=prov).\\\nset_index('prov').stack().stack().reset_index().rename(columns={0:'value'})\n\n\n\n\n\n  \n    \n      \n      prov\n      level_1\n      level_2\n      value\n    \n  \n  \n    \n      0\n      서울특별시\n      당일 누계\n      1차접종\n      8081511\n    \n    \n      1\n      서울특별시\n      당일 누계\n      2차접종\n      7848156\n    \n    \n      2\n      서울특별시\n      당일 누계\n      3차접종\n      2173116\n    \n    \n      3\n      서울특별시\n      당일 실적\n      1차접종\n      1092\n    \n    \n      4\n      서울특별시\n      당일 실적\n      2차접종\n      327\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      97\n      제주특별자치도\n      당일 누계\n      2차접종\n      541559\n    \n    \n      98\n      제주특별자치도\n      당일 누계\n      3차접종\n      137423\n    \n    \n      99\n      제주특별자치도\n      당일 실적\n      1차접종\n      92\n    \n    \n      100\n      제주특별자치도\n      당일 실적\n      2차접종\n      28\n    \n    \n      101\n      제주특별자치도\n      당일 실적\n      3차접종\n      352\n    \n  \n\n102 rows × 4 columns\n\n\n\n\ndf.iloc[1:,1:].assign(prov=prov).\\\nset_index('prov').stack().stack().reset_index().rename(columns={0:'value'}).\\\nquery('level_1 == \"당일 누계\" and level_2 == \"2차접종\" ' )\n\n\n\n\n\n  \n    \n      \n      prov\n      level_1\n      level_2\n      value\n    \n  \n  \n    \n      1\n      서울특별시\n      당일 누계\n      2차접종\n      7848156\n    \n    \n      7\n      부산광역시\n      당일 누계\n      2차접종\n      2686254\n    \n    \n      13\n      대구광역시\n      당일 누계\n      2차접종\n      1867302\n    \n    \n      19\n      인천광역시\n      당일 누계\n      2차접종\n      2400474\n    \n    \n      25\n      광주광역시\n      당일 누계\n      2차접종\n      1173233\n    \n    \n      31\n      대전광역시\n      당일 누계\n      2차접종\n      1157132\n    \n    \n      37\n      울산광역시\n      당일 누계\n      2차접종\n      896509\n    \n    \n      43\n      세종특별자치시\n      당일 누계\n      2차접종\n      268580\n    \n    \n      49\n      경기도\n      당일 누계\n      2차접종\n      11016264\n    \n    \n      55\n      강원도\n      당일 누계\n      2차접종\n      1263853\n    \n    \n      61\n      충청북도\n      당일 누계\n      2차접종\n      1334316\n    \n    \n      67\n      충청남도\n      당일 누계\n      2차접종\n      1765603\n    \n    \n      73\n      전라북도\n      당일 누계\n      2차접종\n      1497144\n    \n    \n      79\n      전라남도\n      당일 누계\n      2차접종\n      1554519\n    \n    \n      85\n      경상북도\n      당일 누계\n      2차접종\n      2126360\n    \n    \n      91\n      경상남도\n      당일 누계\n      2차접종\n      2673402\n    \n    \n      97\n      제주특별자치도\n      당일 누계\n      2차접종\n      541559\n    \n  \n\n\n\n\n- 머지할 데이터프레임을 찾자\n\npd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-prov.csv')\n\n\n\n\n\n  \n    \n      \n      행정구역(시군구)별\n      총인구수 (명)\n    \n  \n  \n    \n      0\n      서울특별시\n      9532428\n    \n    \n      1\n      부산광역시\n      3356311\n    \n    \n      2\n      대구광역시\n      2390721\n    \n    \n      3\n      인천광역시\n      2945009\n    \n    \n      4\n      광주광역시\n      1442454\n    \n    \n      5\n      대전광역시\n      1454228\n    \n    \n      6\n      울산광역시\n      1122566\n    \n    \n      7\n      세종특별자치시\n      368276\n    \n    \n      8\n      경기도\n      13549577\n    \n    \n      9\n      강원도\n      1537717\n    \n    \n      10\n      충청북도\n      1596948\n    \n    \n      11\n      충청남도\n      2118977\n    \n    \n      12\n      전라북도\n      1789770\n    \n    \n      13\n      전라남도\n      1834653\n    \n    \n      14\n      경상북도\n      2627925\n    \n    \n      15\n      경상남도\n      3318161\n    \n    \n      16\n      제주특별자치도\n      676569\n    \n  \n\n\n\n\n\npd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-prov.csv').\\\nrename(columns={'행정구역(시군구)별':'prov','총인구수 (명)':'pop'}) ## 머지할 df <-- 이름 줄 가치가 X \n\n\n\n\n\n  \n    \n      \n      prov\n      pop\n    \n  \n  \n    \n      0\n      서울특별시\n      9532428\n    \n    \n      1\n      부산광역시\n      3356311\n    \n    \n      2\n      대구광역시\n      2390721\n    \n    \n      3\n      인천광역시\n      2945009\n    \n    \n      4\n      광주광역시\n      1442454\n    \n    \n      5\n      대전광역시\n      1454228\n    \n    \n      6\n      울산광역시\n      1122566\n    \n    \n      7\n      세종특별자치시\n      368276\n    \n    \n      8\n      경기도\n      13549577\n    \n    \n      9\n      강원도\n      1537717\n    \n    \n      10\n      충청북도\n      1596948\n    \n    \n      11\n      충청남도\n      2118977\n    \n    \n      12\n      전라북도\n      1789770\n    \n    \n      13\n      전라남도\n      1834653\n    \n    \n      14\n      경상북도\n      2627925\n    \n    \n      15\n      경상남도\n      3318161\n    \n    \n      16\n      제주특별자치도\n      676569\n    \n  \n\n\n\n\n- 머지\n\ndf.iloc[1:,1:].assign(prov=prov).\\\nset_index('prov').stack().stack().reset_index().rename(columns={0:'value'}).\\\nquery('level_1 == \"당일 누계\" and level_2 == \"2차접종\" ' ).\\\nmerge(pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-prov.csv').\\\nrename(columns={'행정구역(시군구)별':'prov','총인구수 (명)':'pop'}))\n\n\n\n\n\n  \n    \n      \n      prov\n      level_1\n      level_2\n      value\n      pop\n    \n  \n  \n    \n      0\n      서울특별시\n      당일 누계\n      2차접종\n      7848156\n      9532428\n    \n    \n      1\n      부산광역시\n      당일 누계\n      2차접종\n      2686254\n      3356311\n    \n    \n      2\n      대구광역시\n      당일 누계\n      2차접종\n      1867302\n      2390721\n    \n    \n      3\n      인천광역시\n      당일 누계\n      2차접종\n      2400474\n      2945009\n    \n    \n      4\n      광주광역시\n      당일 누계\n      2차접종\n      1173233\n      1442454\n    \n    \n      5\n      대전광역시\n      당일 누계\n      2차접종\n      1157132\n      1454228\n    \n    \n      6\n      울산광역시\n      당일 누계\n      2차접종\n      896509\n      1122566\n    \n    \n      7\n      세종특별자치시\n      당일 누계\n      2차접종\n      268580\n      368276\n    \n    \n      8\n      경기도\n      당일 누계\n      2차접종\n      11016264\n      13549577\n    \n    \n      9\n      강원도\n      당일 누계\n      2차접종\n      1263853\n      1537717\n    \n    \n      10\n      충청북도\n      당일 누계\n      2차접종\n      1334316\n      1596948\n    \n    \n      11\n      충청남도\n      당일 누계\n      2차접종\n      1765603\n      2118977\n    \n    \n      12\n      전라북도\n      당일 누계\n      2차접종\n      1497144\n      1789770\n    \n    \n      13\n      전라남도\n      당일 누계\n      2차접종\n      1554519\n      1834653\n    \n    \n      14\n      경상북도\n      당일 누계\n      2차접종\n      2126360\n      2627925\n    \n    \n      15\n      경상남도\n      당일 누계\n      2차접종\n      2673402\n      3318161\n    \n    \n      16\n      제주특별자치도\n      당일 누계\n      2차접종\n      541559\n      676569\n    \n  \n\n\n\n\n\ndf.iloc[1:,1:].assign(prov=prov).\\\nset_index('prov').stack().stack().reset_index().rename(columns={0:'value'}).\\\nquery('level_1 == \"당일 누계\" and level_2 == \"2차접종\" ' ).\\\nmerge(pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-prov.csv').\\\nrename(columns={'행정구역(시군구)별':'prov','총인구수 (명)':'pop'})).\\\neval('prop= value / pop')\n\n\n\n\n\n  \n    \n      \n      prov\n      level_1\n      level_2\n      value\n      pop\n      prop\n    \n  \n  \n    \n      0\n      서울특별시\n      당일 누계\n      2차접종\n      7848156\n      9532428\n      0.823311\n    \n    \n      1\n      부산광역시\n      당일 누계\n      2차접종\n      2686254\n      3356311\n      0.800359\n    \n    \n      2\n      대구광역시\n      당일 누계\n      2차접종\n      1867302\n      2390721\n      0.781062\n    \n    \n      3\n      인천광역시\n      당일 누계\n      2차접종\n      2400474\n      2945009\n      0.815099\n    \n    \n      4\n      광주광역시\n      당일 누계\n      2차접종\n      1173233\n      1442454\n      0.813359\n    \n    \n      5\n      대전광역시\n      당일 누계\n      2차접종\n      1157132\n      1454228\n      0.795702\n    \n    \n      6\n      울산광역시\n      당일 누계\n      2차접종\n      896509\n      1122566\n      0.798625\n    \n    \n      7\n      세종특별자치시\n      당일 누계\n      2차접종\n      268580\n      368276\n      0.729290\n    \n    \n      8\n      경기도\n      당일 누계\n      2차접종\n      11016264\n      13549577\n      0.813034\n    \n    \n      9\n      강원도\n      당일 누계\n      2차접종\n      1263853\n      1537717\n      0.821902\n    \n    \n      10\n      충청북도\n      당일 누계\n      2차접종\n      1334316\n      1596948\n      0.835541\n    \n    \n      11\n      충청남도\n      당일 누계\n      2차접종\n      1765603\n      2118977\n      0.833234\n    \n    \n      12\n      전라북도\n      당일 누계\n      2차접종\n      1497144\n      1789770\n      0.836501\n    \n    \n      13\n      전라남도\n      당일 누계\n      2차접종\n      1554519\n      1834653\n      0.847310\n    \n    \n      14\n      경상북도\n      당일 누계\n      2차접종\n      2126360\n      2627925\n      0.809140\n    \n    \n      15\n      경상남도\n      당일 누계\n      2차접종\n      2673402\n      3318161\n      0.805688\n    \n    \n      16\n      제주특별자치도\n      당일 누계\n      2차접종\n      541559\n      676569\n      0.800449\n    \n  \n\n\n\n\n\n_df=df.iloc[1:,1:].assign(prov=prov).\\\nset_index('prov').stack().stack().reset_index().rename(columns={0:'value'}).\\\nquery('level_1 == \"당일 누계\" and level_2 == \"2차접종\" ' ).\\\nmerge(pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-prov.csv').\\\nrename(columns={'행정구역(시군구)별':'prov','총인구수 (명)':'pop'})).\\\neval('prop= value / pop')\n\n- 저장\n\nimport folium\n\n\nm = folium.Map([36,128],zoom_start=7,scrollWheelZoom=False)\nchoro = folium.Choropleth(\n    data = _df, \n    geo_data= global_dict, \n    columns=['prov','prop'],\n    key_on = 'feature.properties.name'\n)\nchoro.add_to(m)   \n#m\n\n<folium.features.Choropleth at 0x7fecc0a489a0>\n\n\n\n\n\n아래는 COVID19 확진자수를 지역별로 매일 기록한 자료이다.\nhttps://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/covid19_20211202.csv\n자료를 판다스로 불러온 결과는 아래와 같다.\n\n#hide_input\nHTML('<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>일자</th>\\n      <th>계(명)</th>\\n      <th>서울</th>\\n      <th>부산</th>\\n      <th>대구</th>\\n      <th>인천</th>\\n      <th>광주</th>\\n      <th>대전</th>\\n      <th>울산</th>\\n      <th>세종</th>\\n      <th>경기</th>\\n      <th>강원</th>\\n      <th>충북</th>\\n      <th>충남</th>\\n      <th>전북</th>\\n      <th>전남</th>\\n      <th>경북</th>\\n      <th>경남</th>\\n      <th>제주</th>\\n      <th>검역</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>누적(명)</td>\\n      <td>457,612</td>\\n      <td>158,774</td>\\n      <td>16,555</td>\\n      <td>19,114</td>\\n      <td>25,299</td>\\n      <td>6,353</td>\\n      <td>8,809</td>\\n      <td>5,675</td>\\n      <td>1,588</td>\\n      <td>136,546</td>\\n      <td>8,889</td>\\n      <td>8,942</td>\\n      <td>13,174</td>\\n      <td>6,453</td>\\n      <td>4,498</td>\\n      <td>11,471</td>\\n      <td>15,236</td>\\n      <td>3,762</td>\\n      <td>6,474</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>2020-01-20</td>\\n      <td>1</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>1</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>2020-01-21</td>\\n      <td>0</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>2020-01-22</td>\\n      <td>0</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>2020-01-23</td>\\n      <td>0</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n    </tr>\\n    <tr>\\n      <th>...</th>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n    </tr>\\n    <tr>\\n      <th>679</th>\\n      <td>2021-11-28</td>\\n      <td>3,925</td>\\n      <td>1,673</td>\\n      <td>148</td>\\n      <td>106</td>\\n      <td>278</td>\\n      <td>52</td>\\n      <td>53</td>\\n      <td>4</td>\\n      <td>5</td>\\n      <td>1,090</td>\\n      <td>63</td>\\n      <td>25</td>\\n      <td>121</td>\\n      <td>45</td>\\n      <td>25</td>\\n      <td>103</td>\\n      <td>89</td>\\n      <td>35</td>\\n      <td>10</td>\\n    </tr>\\n    <tr>\\n      <th>680</th>\\n      <td>2021-11-29</td>\\n      <td>3,308</td>\\n      <td>1,393</td>\\n      <td>144</td>\\n      <td>88</td>\\n      <td>233</td>\\n      <td>61</td>\\n      <td>43</td>\\n      <td>2</td>\\n      <td>15</td>\\n      <td>910</td>\\n      <td>56</td>\\n      <td>33</td>\\n      <td>52</td>\\n      <td>49</td>\\n      <td>28</td>\\n      <td>68</td>\\n      <td>86</td>\\n      <td>44</td>\\n      <td>3</td>\\n    </tr>\\n    <tr>\\n      <th>681</th>\\n      <td>2021-11-30</td>\\n      <td>3,032</td>\\n      <td>1,186</td>\\n      <td>79</td>\\n      <td>78</td>\\n      <td>192</td>\\n      <td>52</td>\\n      <td>43</td>\\n      <td>3</td>\\n      <td>22</td>\\n      <td>909</td>\\n      <td>84</td>\\n      <td>59</td>\\n      <td>81</td>\\n      <td>50</td>\\n      <td>36</td>\\n      <td>68</td>\\n      <td>60</td>\\n      <td>22</td>\\n      <td>8</td>\\n    </tr>\\n    <tr>\\n      <th>682</th>\\n      <td>2021-12-01</td>\\n      <td>5,123</td>\\n      <td>2,222</td>\\n      <td>143</td>\\n      <td>86</td>\\n      <td>326</td>\\n      <td>29</td>\\n      <td>88</td>\\n      <td>17</td>\\n      <td>20</td>\\n      <td>1,582</td>\\n      <td>105</td>\\n      <td>48</td>\\n      <td>96</td>\\n      <td>50</td>\\n      <td>40</td>\\n      <td>97</td>\\n      <td>127</td>\\n      <td>27</td>\\n      <td>20</td>\\n    </tr>\\n    <tr>\\n      <th>683</th>\\n      <td>2021-12-02</td>\\n      <td>5,266</td>\\n      <td>2,268</td>\\n      <td>158</td>\\n      <td>70</td>\\n      <td>355</td>\\n      <td>39</td>\\n      <td>166</td>\\n      <td>18</td>\\n      <td>8</td>\\n      <td>1,495</td>\\n      <td>145</td>\\n      <td>49</td>\\n      <td>149</td>\\n      <td>71</td>\\n      <td>39</td>\\n      <td>106</td>\\n      <td>94</td>\\n      <td>31</td>\\n      <td>5</td>\\n    </tr>\\n  </tbody>\\n</table>')\n\n\n\n  \n    \n      \n      일자\n      계(명)\n      서울\n      부산\n      대구\n      인천\n      광주\n      대전\n      울산\n      세종\n      경기\n      강원\n      충북\n      충남\n      전북\n      전남\n      경북\n      경남\n      제주\n      검역\n    \n  \n  \n    \n      0\n      누적(명)\n      457,612\n      158,774\n      16,555\n      19,114\n      25,299\n      6,353\n      8,809\n      5,675\n      1,588\n      136,546\n      8,889\n      8,942\n      13,174\n      6,453\n      4,498\n      11,471\n      15,236\n      3,762\n      6,474\n    \n    \n      1\n      2020-01-20\n      1\n      -\n      -\n      -\n      1\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n    \n    \n      2\n      2020-01-21\n      0\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n    \n    \n      3\n      2020-01-22\n      0\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n    \n    \n      4\n      2020-01-23\n      0\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      679\n      2021-11-28\n      3,925\n      1,673\n      148\n      106\n      278\n      52\n      53\n      4\n      5\n      1,090\n      63\n      25\n      121\n      45\n      25\n      103\n      89\n      35\n      10\n    \n    \n      680\n      2021-11-29\n      3,308\n      1,393\n      144\n      88\n      233\n      61\n      43\n      2\n      15\n      910\n      56\n      33\n      52\n      49\n      28\n      68\n      86\n      44\n      3\n    \n    \n      681\n      2021-11-30\n      3,032\n      1,186\n      79\n      78\n      192\n      52\n      43\n      3\n      22\n      909\n      84\n      59\n      81\n      50\n      36\n      68\n      60\n      22\n      8\n    \n    \n      682\n      2021-12-01\n      5,123\n      2,222\n      143\n      86\n      326\n      29\n      88\n      17\n      20\n      1,582\n      105\n      48\n      96\n      50\n      40\n      97\n      127\n      27\n      20\n    \n    \n      683\n      2021-12-02\n      5,266\n      2,268\n      158\n      70\n      355\n      39\n      166\n      18\n      8\n      1,495\n      145\n      49\n      149\n      71\n      39\n      106\n      94\n      31\n      5\n    \n  \n\n\n\n일별로 기록된 COVID19 확진자수를 월별로 통합한 뒤 2021-01 ~ 2021-10 기간의 발생률을 계산하여 시각화하라. 시각화는 plotly의 choropleth_mapbox를 이용하며 시간의 추이를 표현하기 위해 animation_frame 옵션을 사용한다. 시각화 예시는 생략함.\n(풀이)\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/covid19_20211202.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      일자\n      계(명)\n      서울\n      부산\n      대구\n      인천\n      광주\n      대전\n      울산\n      세종\n      경기\n      강원\n      충북\n      충남\n      전북\n      전남\n      경북\n      경남\n      제주\n      검역\n    \n  \n  \n    \n      0\n      누적(명)\n      457,612\n      158,774\n      16,555\n      19,114\n      25,299\n      6,353\n      8,809\n      5,675\n      1,588\n      136,546\n      8,889\n      8,942\n      13,174\n      6,453\n      4,498\n      11,471\n      15,236\n      3,762\n      6,474\n    \n    \n      1\n      2020-01-20\n      1\n      -\n      -\n      -\n      1\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n    \n    \n      2\n      2020-01-21\n      0\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n    \n    \n      3\n      2020-01-22\n      0\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n    \n    \n      4\n      2020-01-23\n      0\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      679\n      2021-11-28\n      3,925\n      1,673\n      148\n      106\n      278\n      52\n      53\n      4\n      5\n      1,090\n      63\n      25\n      121\n      45\n      25\n      103\n      89\n      35\n      10\n    \n    \n      680\n      2021-11-29\n      3,308\n      1,393\n      144\n      88\n      233\n      61\n      43\n      2\n      15\n      910\n      56\n      33\n      52\n      49\n      28\n      68\n      86\n      44\n      3\n    \n    \n      681\n      2021-11-30\n      3,032\n      1,186\n      79\n      78\n      192\n      52\n      43\n      3\n      22\n      909\n      84\n      59\n      81\n      50\n      36\n      68\n      60\n      22\n      8\n    \n    \n      682\n      2021-12-01\n      5,123\n      2,222\n      143\n      86\n      326\n      29\n      88\n      17\n      20\n      1,582\n      105\n      48\n      96\n      50\n      40\n      97\n      127\n      27\n      20\n    \n    \n      683\n      2021-12-02\n      5,266\n      2,268\n      158\n      70\n      355\n      39\n      166\n      18\n      8\n      1,495\n      145\n      49\n      149\n      71\n      39\n      106\n      94\n      31\n      5\n    \n  \n\n684 rows × 20 columns\n\n\n\n\ndf.iloc[1:].set_index('일자').iloc[:,1:-1].applymap(lambda x: int(x.replace(',','') if x!='-' else 0))\n\n\n\n\n\n  \n    \n      \n      서울\n      부산\n      대구\n      인천\n      광주\n      대전\n      울산\n      세종\n      경기\n      강원\n      충북\n      충남\n      전북\n      전남\n      경북\n      경남\n      제주\n    \n    \n      일자\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2020-01-20\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2020-01-21\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2020-01-22\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2020-01-23\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2020-01-24\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2021-11-28\n      1673\n      148\n      106\n      278\n      52\n      53\n      4\n      5\n      1090\n      63\n      25\n      121\n      45\n      25\n      103\n      89\n      35\n    \n    \n      2021-11-29\n      1393\n      144\n      88\n      233\n      61\n      43\n      2\n      15\n      910\n      56\n      33\n      52\n      49\n      28\n      68\n      86\n      44\n    \n    \n      2021-11-30\n      1186\n      79\n      78\n      192\n      52\n      43\n      3\n      22\n      909\n      84\n      59\n      81\n      50\n      36\n      68\n      60\n      22\n    \n    \n      2021-12-01\n      2222\n      143\n      86\n      326\n      29\n      88\n      17\n      20\n      1582\n      105\n      48\n      96\n      50\n      40\n      97\n      127\n      27\n    \n    \n      2021-12-02\n      2268\n      158\n      70\n      355\n      39\n      166\n      18\n      8\n      1495\n      145\n      49\n      149\n      71\n      39\n      106\n      94\n      31\n    \n  \n\n683 rows × 17 columns\n\n\n\n\ndf.iloc[1:].set_index('일자').iloc[:,1:-1].applymap(lambda x: int(x.replace(',','') if x!='-' else 0)).\\\nT.reset_index().rename(columns={'index':'prov'}).assign(prov=prov).set_index('prov').T.reset_index()\n\n\n\n\n\n  \n    \n      prov\n      일자\n      서울특별시\n      부산광역시\n      대구광역시\n      인천광역시\n      광주광역시\n      대전광역시\n      울산광역시\n      세종특별자치시\n      경기도\n      강원도\n      충청북도\n      충청남도\n      전라북도\n      전라남도\n      경상북도\n      경상남도\n      제주특별자치도\n    \n  \n  \n    \n      0\n      2020-01-20\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      2020-01-21\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      2020-01-22\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      2020-01-23\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      2020-01-24\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      678\n      2021-11-28\n      1673\n      148\n      106\n      278\n      52\n      53\n      4\n      5\n      1090\n      63\n      25\n      121\n      45\n      25\n      103\n      89\n      35\n    \n    \n      679\n      2021-11-29\n      1393\n      144\n      88\n      233\n      61\n      43\n      2\n      15\n      910\n      56\n      33\n      52\n      49\n      28\n      68\n      86\n      44\n    \n    \n      680\n      2021-11-30\n      1186\n      79\n      78\n      192\n      52\n      43\n      3\n      22\n      909\n      84\n      59\n      81\n      50\n      36\n      68\n      60\n      22\n    \n    \n      681\n      2021-12-01\n      2222\n      143\n      86\n      326\n      29\n      88\n      17\n      20\n      1582\n      105\n      48\n      96\n      50\n      40\n      97\n      127\n      27\n    \n    \n      682\n      2021-12-02\n      2268\n      158\n      70\n      355\n      39\n      166\n      18\n      8\n      1495\n      145\n      49\n      149\n      71\n      39\n      106\n      94\n      31\n    \n  \n\n683 rows × 18 columns\n\n\n\n\ndf.iloc[1:].set_index('일자').iloc[:,1:-1].applymap(lambda x: int(x.replace(',','') if x!='-' else 0)).\\\nT.reset_index().rename(columns={'index':'prov'}).assign(prov=prov).set_index('prov').T.reset_index()\n\n\n\n\n\n  \n    \n      prov\n      일자\n      서울특별시\n      부산광역시\n      대구광역시\n      인천광역시\n      광주광역시\n      대전광역시\n      울산광역시\n      세종특별자치시\n      경기도\n      강원도\n      충청북도\n      충청남도\n      전라북도\n      전라남도\n      경상북도\n      경상남도\n      제주특별자치도\n    \n  \n  \n    \n      0\n      2020-01-20\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      2020-01-21\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      2020-01-22\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      2020-01-23\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      2020-01-24\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      678\n      2021-11-28\n      1673\n      148\n      106\n      278\n      52\n      53\n      4\n      5\n      1090\n      63\n      25\n      121\n      45\n      25\n      103\n      89\n      35\n    \n    \n      679\n      2021-11-29\n      1393\n      144\n      88\n      233\n      61\n      43\n      2\n      15\n      910\n      56\n      33\n      52\n      49\n      28\n      68\n      86\n      44\n    \n    \n      680\n      2021-11-30\n      1186\n      79\n      78\n      192\n      52\n      43\n      3\n      22\n      909\n      84\n      59\n      81\n      50\n      36\n      68\n      60\n      22\n    \n    \n      681\n      2021-12-01\n      2222\n      143\n      86\n      326\n      29\n      88\n      17\n      20\n      1582\n      105\n      48\n      96\n      50\n      40\n      97\n      127\n      27\n    \n    \n      682\n      2021-12-02\n      2268\n      158\n      70\n      355\n      39\n      166\n      18\n      8\n      1495\n      145\n      49\n      149\n      71\n      39\n      106\n      94\n      31\n    \n  \n\n683 rows × 18 columns\n\n\n\n- 여기가 어려움. 아래의 변환이 필요함\n\n'2020-01-20', '2020-01'\n\n('2020-01-20', '2020-01')\n\n\n\n'2020-01-20'[:7]\n\n'2020-01'\n\n\n\n'2020-01-20'.split('-')[0]+ '-' + '2020-01-20'.split('-')[1]\n\n'2020-01'\n\n\n- 방법1\n\n_df=df.iloc[1:].set_index('일자').iloc[:,1:-1].applymap(lambda x: int(x.replace(',','') if x!='-' else 0)).\\\nT.reset_index().rename(columns={'index':'prov'}).assign(prov=prov).set_index('prov').T.reset_index()\n_df.assign(일자=list(map(lambda x: x[:7] , _df['일자'])))\n\n\n\n\n\n  \n    \n      prov\n      일자\n      서울특별시\n      부산광역시\n      대구광역시\n      인천광역시\n      광주광역시\n      대전광역시\n      울산광역시\n      세종특별자치시\n      경기도\n      강원도\n      충청북도\n      충청남도\n      전라북도\n      전라남도\n      경상북도\n      경상남도\n      제주특별자치도\n    \n  \n  \n    \n      0\n      2020-01\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      2020-01\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      2020-01\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      2020-01\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      2020-01\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      678\n      2021-11\n      1673\n      148\n      106\n      278\n      52\n      53\n      4\n      5\n      1090\n      63\n      25\n      121\n      45\n      25\n      103\n      89\n      35\n    \n    \n      679\n      2021-11\n      1393\n      144\n      88\n      233\n      61\n      43\n      2\n      15\n      910\n      56\n      33\n      52\n      49\n      28\n      68\n      86\n      44\n    \n    \n      680\n      2021-11\n      1186\n      79\n      78\n      192\n      52\n      43\n      3\n      22\n      909\n      84\n      59\n      81\n      50\n      36\n      68\n      60\n      22\n    \n    \n      681\n      2021-12\n      2222\n      143\n      86\n      326\n      29\n      88\n      17\n      20\n      1582\n      105\n      48\n      96\n      50\n      40\n      97\n      127\n      27\n    \n    \n      682\n      2021-12\n      2268\n      158\n      70\n      355\n      39\n      166\n      18\n      8\n      1495\n      145\n      49\n      149\n      71\n      39\n      106\n      94\n      31\n    \n  \n\n683 rows × 18 columns\n\n\n\n- 방법2\n\ndf.assign(일자=list(map(lambda x: x[:7], df.일자 )))\n\n\n\n\n\n  \n    \n      \n      일자\n      계(명)\n      서울\n      부산\n      대구\n      인천\n      광주\n      대전\n      울산\n      세종\n      경기\n      강원\n      충북\n      충남\n      전북\n      전남\n      경북\n      경남\n      제주\n      검역\n    \n  \n  \n    \n      0\n      누적(명)\n      457,612\n      158,774\n      16,555\n      19,114\n      25,299\n      6,353\n      8,809\n      5,675\n      1,588\n      136,546\n      8,889\n      8,942\n      13,174\n      6,453\n      4,498\n      11,471\n      15,236\n      3,762\n      6,474\n    \n    \n      1\n      2020-01\n      1\n      -\n      -\n      -\n      1\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n    \n    \n      2\n      2020-01\n      0\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n    \n    \n      3\n      2020-01\n      0\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n    \n    \n      4\n      2020-01\n      0\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      679\n      2021-11\n      3,925\n      1,673\n      148\n      106\n      278\n      52\n      53\n      4\n      5\n      1,090\n      63\n      25\n      121\n      45\n      25\n      103\n      89\n      35\n      10\n    \n    \n      680\n      2021-11\n      3,308\n      1,393\n      144\n      88\n      233\n      61\n      43\n      2\n      15\n      910\n      56\n      33\n      52\n      49\n      28\n      68\n      86\n      44\n      3\n    \n    \n      681\n      2021-11\n      3,032\n      1,186\n      79\n      78\n      192\n      52\n      43\n      3\n      22\n      909\n      84\n      59\n      81\n      50\n      36\n      68\n      60\n      22\n      8\n    \n    \n      682\n      2021-12\n      5,123\n      2,222\n      143\n      86\n      326\n      29\n      88\n      17\n      20\n      1,582\n      105\n      48\n      96\n      50\n      40\n      97\n      127\n      27\n      20\n    \n    \n      683\n      2021-12\n      5,266\n      2,268\n      158\n      70\n      355\n      39\n      166\n      18\n      8\n      1,495\n      145\n      49\n      149\n      71\n      39\n      106\n      94\n      31\n      5\n    \n  \n\n684 rows × 20 columns\n\n\n\n\n현실적으로 이정도까지 앞을 내다보는건 힘든것 같아요..\n\n- 방법3\n\ndf.iloc[1:].set_index('일자').iloc[:,1:-1].applymap(lambda x: int(x.replace(',','') if x!='-' else 0)).\\\nT.reset_index().rename(columns={'index':'prov'}).assign(prov=prov).set_index('prov').T.reset_index().\\\nassign(일자=lambda df: list(map(lambda x: x[:7] , df['일자'])))\n\n\n\n\n\n  \n    \n      prov\n      일자\n      서울특별시\n      부산광역시\n      대구광역시\n      인천광역시\n      광주광역시\n      대전광역시\n      울산광역시\n      세종특별자치시\n      경기도\n      강원도\n      충청북도\n      충청남도\n      전라북도\n      전라남도\n      경상북도\n      경상남도\n      제주특별자치도\n    \n  \n  \n    \n      0\n      2020-01\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      2020-01\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      2020-01\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      2020-01\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      2020-01\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      678\n      2021-11\n      1673\n      148\n      106\n      278\n      52\n      53\n      4\n      5\n      1090\n      63\n      25\n      121\n      45\n      25\n      103\n      89\n      35\n    \n    \n      679\n      2021-11\n      1393\n      144\n      88\n      233\n      61\n      43\n      2\n      15\n      910\n      56\n      33\n      52\n      49\n      28\n      68\n      86\n      44\n    \n    \n      680\n      2021-11\n      1186\n      79\n      78\n      192\n      52\n      43\n      3\n      22\n      909\n      84\n      59\n      81\n      50\n      36\n      68\n      60\n      22\n    \n    \n      681\n      2021-12\n      2222\n      143\n      86\n      326\n      29\n      88\n      17\n      20\n      1582\n      105\n      48\n      96\n      50\n      40\n      97\n      127\n      27\n    \n    \n      682\n      2021-12\n      2268\n      158\n      70\n      355\n      39\n      166\n      18\n      8\n      1495\n      145\n      49\n      149\n      71\n      39\n      106\n      94\n      31\n    \n  \n\n683 rows × 18 columns\n\n\n\n- 방법3을 택하도록 하자. 위에서 일자를 ym으로 바꾸고 tidydata를 만들자.\n\ndf.iloc[1:].set_index('일자').iloc[:,1:-1].applymap(lambda x: int(x.replace(',','') if x!='-' else 0)).\\\nT.reset_index().rename(columns={'index':'prov'}).assign(prov=prov).set_index('prov').T.reset_index().\\\nassign(일자=lambda df: list(map(lambda x: x[:7] , df['일자']))).\\\nrename(columns={'일자':'ym'}).set_index('ym').stack().reset_index().rename(columns={0:'value'})\n\n\n\n\n\n  \n    \n      \n      ym\n      prov\n      value\n    \n  \n  \n    \n      0\n      2020-01\n      서울특별시\n      0\n    \n    \n      1\n      2020-01\n      부산광역시\n      0\n    \n    \n      2\n      2020-01\n      대구광역시\n      0\n    \n    \n      3\n      2020-01\n      인천광역시\n      1\n    \n    \n      4\n      2020-01\n      광주광역시\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      11606\n      2021-12\n      전라북도\n      71\n    \n    \n      11607\n      2021-12\n      전라남도\n      39\n    \n    \n      11608\n      2021-12\n      경상북도\n      106\n    \n    \n      11609\n      2021-12\n      경상남도\n      94\n    \n    \n      11610\n      2021-12\n      제주특별자치도\n      31\n    \n  \n\n11611 rows × 3 columns\n\n\n\n- groupby 적용\n\ndf.iloc[1:].set_index('일자').iloc[:,1:-1].applymap(lambda x: int(x.replace(',','') if x!='-' else 0)).\\\nT.reset_index().rename(columns={'index':'prov'}).assign(prov=prov).set_index('prov').T.reset_index().\\\nassign(일자=lambda df: list(map(lambda x: x[:7] , df['일자']))).\\\nrename(columns={'일자':'ym'}).set_index('ym').stack().reset_index().rename(columns={0:'value'}).\\\ngroupby(['ym','prov']).agg({'value':sum}).reset_index()\n\n\n\n\n\n  \n    \n      \n      ym\n      prov\n      value\n    \n  \n  \n    \n      0\n      2020-01\n      강원도\n      0\n    \n    \n      1\n      2020-01\n      경기도\n      2\n    \n    \n      2\n      2020-01\n      경상남도\n      0\n    \n    \n      3\n      2020-01\n      경상북도\n      0\n    \n    \n      4\n      2020-01\n      광주광역시\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      403\n      2021-12\n      전라남도\n      79\n    \n    \n      404\n      2021-12\n      전라북도\n      121\n    \n    \n      405\n      2021-12\n      제주특별자치도\n      58\n    \n    \n      406\n      2021-12\n      충청남도\n      245\n    \n    \n      407\n      2021-12\n      충청북도\n      97\n    \n  \n\n408 rows × 3 columns\n\n\n\n- query\n\ndf.iloc[1:].set_index('일자').iloc[:,1:-1].applymap(lambda x: int(x.replace(',','') if x!='-' else 0)).\\\nT.reset_index().rename(columns={'index':'prov'}).assign(prov=prov).set_index('prov').T.reset_index().\\\nassign(일자=lambda df: list(map(lambda x: x[:7] , df['일자']))).\\\nrename(columns={'일자':'ym'}).set_index('ym').stack().reset_index().rename(columns={0:'value'}).\\\ngroupby(['ym','prov']).agg({'value':sum}).reset_index().\\\nquery('ym >= \"2021-01\" and ym<=\"2021-10\"')\n\n\n\n\n\n  \n    \n      \n      ym\n      prov\n      value\n    \n  \n  \n    \n      204\n      2021-01\n      강원도\n      488\n    \n    \n      205\n      2021-01\n      경기도\n      5353\n    \n    \n      206\n      2021-01\n      경상남도\n      686\n    \n    \n      207\n      2021-01\n      경상북도\n      593\n    \n    \n      208\n      2021-01\n      광주광역시\n      702\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      369\n      2021-10\n      전라남도\n      459\n    \n    \n      370\n      2021-10\n      전라북도\n      672\n    \n    \n      371\n      2021-10\n      제주특별자치도\n      225\n    \n    \n      372\n      2021-10\n      충청남도\n      1467\n    \n    \n      373\n      2021-10\n      충청북도\n      1556\n    \n  \n\n170 rows × 3 columns\n\n\n\n- merge\n\ndf.iloc[1:].set_index('일자').iloc[:,1:-1].applymap(lambda x: int(x.replace(',','') if x!='-' else 0)).\\\nT.reset_index().rename(columns={'index':'prov'}).assign(prov=prov).set_index('prov').T.reset_index().\\\nassign(일자=lambda df: list(map(lambda x: x[:7] , df['일자']))).\\\nrename(columns={'일자':'ym'}).set_index('ym').stack().reset_index().rename(columns={0:'value'}).\\\ngroupby(['ym','prov']).agg({'value':sum}).reset_index().\\\nquery('ym >= \"2021-01\" and ym<=\"2021-10\"').\\\nmerge(pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-prov.csv').\\\nrename(columns={'행정구역(시군구)별':'prov','총인구수 (명)':'pop'})) ## 머지할 df <-- 이름 줄 가치가 X \n\n\n\n\n\n  \n    \n      \n      ym\n      prov\n      value\n      pop\n    \n  \n  \n    \n      0\n      2021-01\n      강원도\n      488\n      1537717\n    \n    \n      1\n      2021-02\n      강원도\n      169\n      1537717\n    \n    \n      2\n      2021-03\n      강원도\n      466\n      1537717\n    \n    \n      3\n      2021-04\n      강원도\n      354\n      1537717\n    \n    \n      4\n      2021-05\n      강원도\n      501\n      1537717\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      165\n      2021-06\n      충청북도\n      363\n      1596948\n    \n    \n      166\n      2021-07\n      충청북도\n      544\n      1596948\n    \n    \n      167\n      2021-08\n      충청북도\n      1302\n      1596948\n    \n    \n      168\n      2021-09\n      충청북도\n      1192\n      1596948\n    \n    \n      169\n      2021-10\n      충청북도\n      1556\n      1596948\n    \n  \n\n170 rows × 4 columns\n\n\n\n- prop = value / pop 을 계산하자\n\ndf.iloc[1:].set_index('일자').iloc[:,1:-1].applymap(lambda x: int(x.replace(',','') if x!='-' else 0)).\\\nT.reset_index().rename(columns={'index':'prov'}).assign(prov=prov).set_index('prov').T.reset_index().\\\nassign(일자=lambda df: list(map(lambda x: x[:7] , df['일자']))).\\\nrename(columns={'일자':'ym'}).set_index('ym').stack().reset_index().rename(columns={0:'value'}).\\\ngroupby(['ym','prov']).agg({'value':sum}).reset_index().\\\nquery('ym >= \"2021-01\" and ym<=\"2021-10\"').\\\nmerge(pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-prov.csv').\\\nrename(columns={'행정구역(시군구)별':'prov','총인구수 (명)':'pop'})).\\\neval('prop = value/pop')\n\n\n\n\n\n  \n    \n      \n      ym\n      prov\n      value\n      pop\n      prop\n    \n  \n  \n    \n      0\n      2021-01\n      강원도\n      488\n      1537717\n      0.000317\n    \n    \n      1\n      2021-02\n      강원도\n      169\n      1537717\n      0.000110\n    \n    \n      2\n      2021-03\n      강원도\n      466\n      1537717\n      0.000303\n    \n    \n      3\n      2021-04\n      강원도\n      354\n      1537717\n      0.000230\n    \n    \n      4\n      2021-05\n      강원도\n      501\n      1537717\n      0.000326\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      165\n      2021-06\n      충청북도\n      363\n      1596948\n      0.000227\n    \n    \n      166\n      2021-07\n      충청북도\n      544\n      1596948\n      0.000341\n    \n    \n      167\n      2021-08\n      충청북도\n      1302\n      1596948\n      0.000815\n    \n    \n      168\n      2021-09\n      충청북도\n      1192\n      1596948\n      0.000746\n    \n    \n      169\n      2021-10\n      충청북도\n      1556\n      1596948\n      0.000974\n    \n  \n\n170 rows × 5 columns\n\n\n\n\ndf2=df.iloc[1:].set_index('일자').iloc[:,1:-1].applymap(lambda x: int(x.replace(',','') if x!='-' else 0)).\\\nT.reset_index().rename(columns={'index':'prov'}).assign(prov=prov).set_index('prov').T.reset_index().\\\nassign(일자=lambda df: list(map(lambda x: x[:7] , df['일자']))).\\\nrename(columns={'일자':'ym'}).set_index('ym').stack().reset_index().rename(columns={0:'value'}).\\\ngroupby(['ym','prov']).agg({'value':sum}).reset_index().\\\nquery('ym >= \"2021-01\" and ym<=\"2021-10\"').\\\nmerge(pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-prov.csv').\\\nrename(columns={'행정구역(시군구)별':'prov','총인구수 (명)':'pop'})).\\\neval('prop = value/pop')\n\n\n# import plotly.express as px\n# from IPython.display import HTML\n# fig=px.choropleth_mapbox(df2, \n#                      geojson=global_dict, \n#                      color='prop',\n#                      locations='prov', \n#                      animation_frame='ym',\n#                      featureidkey='properties.name',\n#                      center={\"lat\": 36, \"lon\": 128},\n#                      mapbox_style=\"carto-positron\", \n#                      range_color=(0, df2.prop.max()),\n#                      height=1200,\n#                      zoom=6.5)\n# fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n# HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))"
  },
  {
    "objectID": "posts/2022-06-30-기말고사예상문제_빅데이터분석특강.html",
    "href": "posts/2022-06-30-기말고사예상문제_빅데이터분석특강.html",
    "title": "jisim12",
    "section": "",
    "text": "빅데이터분석특강\n\n\n기말고사 예상문제 - toc:false - branch: master - badges: true - comments: true - author: 심재인\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport tensorflow as tf \nimport tensorflow.experimental.numpy as tnp\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\n%load_ext tensorboard\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+ s + ';}')\n\n\n#!conda install -c conda-forge python-graphviz -y\n\n\n\n\n(1) tf.keras.datasets.fashion_mnist.load_data()을 이용하여 fashion_mnist 자료를 불러온 뒤 아래의 네트워크를 이용하여 적합하라.\n\n평가지표로 accuracy를 이용할 것\nepoch은 10으로 설정할 것\noptimizer는 adam을 이용할 것\n\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x1\"\n    \"x2\"\n    \"..\"\n    \"x784\"\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x1\" -> \"node1\"\n    \"x2\" -> \"node1\"\n    \"..\" -> \"node1\"\n    \"x784\" -> \"node1\"\n    \n    \"x1\" -> \"node2\"\n    \"x2\" -> \"node2\"\n    \"..\" -> \"node2\"\n    \"x784\" -> \"node2\"\n    \n    \"x1\" -> \"...\"\n    \"x2\" -> \"...\"\n    \"..\" -> \"...\"\n    \"x784\" -> \"...\"\n\n    \"x1\" -> \"node20\"\n    \"x2\" -> \"node20\"\n    \"..\" -> \"node20\"\n    \"x784\" -> \"node20\"\n\n\n    label = \"Layer 1: relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -> \"node1 \"\n    \"node2\" -> \"node1 \"\n    \"...\" -> \"node1 \"\n    \"node20\" -> \"node1 \"\n    \n    \"node1\" -> \"node2 \"\n    \"node2\" -> \"node2 \"\n    \"...\" -> \"node2 \"\n    \"node20\" -> \"node2 \"\n    \n    \"node1\" -> \"... \"\n    \"node2\" -> \"... \"\n    \"...\" -> \"... \"\n    \"node20\" -> \"... \"\n\n    \"node1\" -> \"node30 \"\n    \"node2\" -> \"node30 \"\n    \"...\" -> \"node30 \"\n    \"node20\" -> \"node30 \"\n\n\n    label = \"Layer 2: relu\"\n}\nsubgraph cluster_4{\n    style=filled;\n    color=lightgrey;\n\n    \"node1 \" -> \"y10\"\n    \"node2 \" -> \"y10\"\n    \"... \" -> \"y10\"\n    \"node30 \" -> \"y10\"\n    \n    \"node1 \" -> \"y1\"\n    \"node2 \" -> \"y1\"\n    \"... \" -> \"y1\"\n    \"node30 \" -> \"y1\"\n    \n    \"node1 \" -> \".\"\n    \"node2 \" -> \".\"\n    \"... \" -> \".\"\n    \"node30 \" -> \".\"\n    \n    label = \"Layer 3: softmax\"\n}\n''')\n\n\n\n\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\n\nX = tf.constant(x_train.reshape(-1,28,28,1),dtype=tf.float64)\ny = tf.keras.utils.to_categorical(y_train)\nXX = tf.constant(x_test.reshape(-1,28,28,1),dtype=tf.float64)\nyy = tf.keras.utils.to_categorical(y_test)\n\n2022-06-13 18:07:06.073347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n\ntf.random.set_seed(4305)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(20,activation='relu'))\nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy'])\nnet.fit(X,y,epochs=10)\n\nEpoch 1/10\n1875/1875 [==============================] - 3s 1ms/step - loss: 1.5931 - accuracy: 0.5048\nEpoch 2/10\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.9580 - accuracy: 0.6069\nEpoch 3/10\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.9079 - accuracy: 0.6217\nEpoch 4/10\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.8931 - accuracy: 0.6271\nEpoch 5/10\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.8732 - accuracy: 0.6290\nEpoch 6/10\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.8751 - accuracy: 0.6310\nEpoch 7/10\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.8634 - accuracy: 0.6353\nEpoch 8/10\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.8578 - accuracy: 0.6340\nEpoch 9/10\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.8427 - accuracy: 0.6371\nEpoch 10/10\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.8528 - accuracy: 0.6336\n\n\n<keras.callbacks.History at 0x7f00665d3c70>\n\n\n(2) (1)에서 적합된 네트워크를 이용하여 test data의 accuracy를 구하라.\n\nnet.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 1ms/step - loss: 0.8463 - accuracy: 0.6357\n\n\n[0.8462827205657959, 0.635699987411499]\n\n\n(3) train set에서 20%의 자료를 validation 으로 분리하여 50에폭동안 학습하라. 텐서보드를 이용하여 train accuracy와 validation accuracy를 시각화 하고 결과를 해석하라. 오버피팅이라고 볼 수 있는가?\n\ntf.random.set_seed(4305)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(20,activation='relu'))\nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy'])\n\n\n#collapse_output\ncb1 = tf.keras.callbacks.TensorBoard()\nnet.fit(X,y,epochs=50,batch_size=200,validation_split=0.2,callbacks=cb1,verbose=1) \n\nEpoch 1/50\n240/240 [==============================] - 1s 2ms/step - loss: 3.7604 - accuracy: 0.2533 - val_loss: 1.8268 - val_accuracy: 0.3212\nEpoch 2/50\n240/240 [==============================] - 0s 1ms/step - loss: 1.7592 - accuracy: 0.3275 - val_loss: 1.6927 - val_accuracy: 0.3509\nEpoch 3/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.6008 - accuracy: 0.3767 - val_loss: 1.5118 - val_accuracy: 0.4139\nEpoch 4/50\n240/240 [==============================] - 1s 3ms/step - loss: 1.4380 - accuracy: 0.4215 - val_loss: 1.3867 - val_accuracy: 0.4374\nEpoch 5/50\n240/240 [==============================] - 1s 2ms/step - loss: 1.3066 - accuracy: 0.4505 - val_loss: 1.2980 - val_accuracy: 0.4444\nEpoch 6/50\n240/240 [==============================] - 1s 2ms/step - loss: 1.2581 - accuracy: 0.4582 - val_loss: 1.2748 - val_accuracy: 0.4487\nEpoch 7/50\n240/240 [==============================] - 1s 3ms/step - loss: 1.2330 - accuracy: 0.4642 - val_loss: 1.2586 - val_accuracy: 0.4619\nEpoch 8/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.2193 - accuracy: 0.4665 - val_loss: 1.2448 - val_accuracy: 0.4613\nEpoch 9/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.2105 - accuracy: 0.4708 - val_loss: 1.2377 - val_accuracy: 0.4622\nEpoch 10/50\n240/240 [==============================] - 1s 2ms/step - loss: 1.2070 - accuracy: 0.4655 - val_loss: 1.2371 - val_accuracy: 0.4642\nEpoch 11/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.1965 - accuracy: 0.4741 - val_loss: 1.2254 - val_accuracy: 0.4652\nEpoch 12/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.2010 - accuracy: 0.4695 - val_loss: 1.2485 - val_accuracy: 0.4633\nEpoch 13/50\n240/240 [==============================] - 1s 3ms/step - loss: 1.1945 - accuracy: 0.4700 - val_loss: 1.2370 - val_accuracy: 0.4622\nEpoch 14/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.1909 - accuracy: 0.4745 - val_loss: 1.2393 - val_accuracy: 0.4631\nEpoch 15/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.1846 - accuracy: 0.4765 - val_loss: 1.2120 - val_accuracy: 0.4613\nEpoch 16/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.1775 - accuracy: 0.4799 - val_loss: 1.2313 - val_accuracy: 0.4661\nEpoch 17/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.0790 - accuracy: 0.5331 - val_loss: 0.9083 - val_accuracy: 0.6162\nEpoch 18/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.8537 - accuracy: 0.6369 - val_loss: 0.8944 - val_accuracy: 0.6488\nEpoch 19/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.8094 - accuracy: 0.6599 - val_loss: 0.8285 - val_accuracy: 0.6495\nEpoch 20/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.7425 - accuracy: 0.7044 - val_loss: 0.8067 - val_accuracy: 0.6590\nEpoch 21/50\n240/240 [==============================] - 1s 2ms/step - loss: 0.7089 - accuracy: 0.7297 - val_loss: 0.7473 - val_accuracy: 0.7247\nEpoch 22/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.6667 - accuracy: 0.7424 - val_loss: 0.6920 - val_accuracy: 0.7377\nEpoch 23/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.6437 - accuracy: 0.7521 - val_loss: 0.6612 - val_accuracy: 0.7607\nEpoch 24/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.6165 - accuracy: 0.7617 - val_loss: 0.6459 - val_accuracy: 0.7591\nEpoch 25/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.6074 - accuracy: 0.7645 - val_loss: 0.6200 - val_accuracy: 0.7754\nEpoch 26/50\n240/240 [==============================] - 1s 2ms/step - loss: 0.5958 - accuracy: 0.7732 - val_loss: 0.6508 - val_accuracy: 0.7409\nEpoch 27/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.5861 - accuracy: 0.7760 - val_loss: 0.6398 - val_accuracy: 0.7695\nEpoch 28/50\n240/240 [==============================] - 1s 2ms/step - loss: 0.5731 - accuracy: 0.7811 - val_loss: 0.6187 - val_accuracy: 0.7620\nEpoch 29/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.5785 - accuracy: 0.7821 - val_loss: 0.6690 - val_accuracy: 0.7592\nEpoch 30/50\n240/240 [==============================] - 1s 2ms/step - loss: 0.5699 - accuracy: 0.7838 - val_loss: 0.6264 - val_accuracy: 0.7689\nEpoch 31/50\n240/240 [==============================] - 1s 3ms/step - loss: 0.5614 - accuracy: 0.7887 - val_loss: 0.6336 - val_accuracy: 0.7703\nEpoch 32/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.5735 - accuracy: 0.7826 - val_loss: 0.6390 - val_accuracy: 0.7651\nEpoch 33/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.5586 - accuracy: 0.7869 - val_loss: 0.6163 - val_accuracy: 0.7673\nEpoch 34/50\n240/240 [==============================] - 1s 2ms/step - loss: 0.5639 - accuracy: 0.7859 - val_loss: 0.6234 - val_accuracy: 0.7632\nEpoch 35/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.5505 - accuracy: 0.7945 - val_loss: 0.6131 - val_accuracy: 0.7771\nEpoch 36/50\n240/240 [==============================] - 1s 2ms/step - loss: 0.5520 - accuracy: 0.7976 - val_loss: 0.6372 - val_accuracy: 0.7622\nEpoch 37/50\n240/240 [==============================] - 1s 2ms/step - loss: 0.5506 - accuracy: 0.8038 - val_loss: 0.5787 - val_accuracy: 0.8054\nEpoch 38/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.5323 - accuracy: 0.8130 - val_loss: 0.5897 - val_accuracy: 0.8002\nEpoch 39/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.5180 - accuracy: 0.8217 - val_loss: 0.5652 - val_accuracy: 0.8137\nEpoch 40/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.8242 - val_loss: 0.5604 - val_accuracy: 0.8138\nEpoch 41/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.5154 - accuracy: 0.8216 - val_loss: 0.5749 - val_accuracy: 0.8083\nEpoch 42/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.5144 - accuracy: 0.8220 - val_loss: 0.5619 - val_accuracy: 0.8148\nEpoch 43/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.5058 - accuracy: 0.8260 - val_loss: 0.5536 - val_accuracy: 0.8163\nEpoch 44/50\n240/240 [==============================] - 1s 2ms/step - loss: 0.5018 - accuracy: 0.8279 - val_loss: 0.5928 - val_accuracy: 0.8047\nEpoch 45/50\n240/240 [==============================] - 1s 2ms/step - loss: 0.4976 - accuracy: 0.8291 - val_loss: 0.5759 - val_accuracy: 0.8092\nEpoch 46/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.5027 - accuracy: 0.8270 - val_loss: 0.5770 - val_accuracy: 0.8013\nEpoch 47/50\n240/240 [==============================] - 1s 2ms/step - loss: 0.4967 - accuracy: 0.8284 - val_loss: 0.5560 - val_accuracy: 0.8148\nEpoch 48/50\n240/240 [==============================] - 1s 4ms/step - loss: 0.4986 - accuracy: 0.8263 - val_loss: 0.5802 - val_accuracy: 0.8119\nEpoch 49/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.5021 - accuracy: 0.8277 - val_loss: 0.6201 - val_accuracy: 0.7898\nEpoch 50/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.4893 - accuracy: 0.8325 - val_loss: 0.5654 - val_accuracy: 0.8139\n\n\n<keras.callbacks.History at 0x7f00580a7070>\n\n\n\n%tensorboard --logdir logs --host 0.0.0.0 \n\n\n      \n      \n      \n    \n\n\n(4) (3)에서 적합된 네트워크를 이용하여 test data의 accuracy를 구하라. (2)의 결과와 비교하라.\n\nnet.evaluate(XX,yy)\n\n313/313 [==============================] - 1s 1ms/step - loss: 0.6004 - accuracy: 0.8104\n\n\n[0.6003509759902954, 0.8104000091552734]\n\n\n(5) 조기종료기능을 이용하여 (3)의 네트워크를 다시 학습하라. 학습결과를 텐서보드를 이용하여 시각화 하라. - patience=3 으로 설정할 것\n\ntf.random.set_seed(4305)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(20,activation='relu'))\nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy'])\n\n\ncb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=3)\nnet.fit(X,y,epochs=50,batch_size=200,validation_split=0.2,callbacks=[cb1,cb2]) \n\nEpoch 1/50\n240/240 [==============================] - 1s 2ms/step - loss: 3.7604 - accuracy: 0.2533 - val_loss: 1.8268 - val_accuracy: 0.3212\nEpoch 2/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.7592 - accuracy: 0.3275 - val_loss: 1.6927 - val_accuracy: 0.3509\nEpoch 3/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.6008 - accuracy: 0.3767 - val_loss: 1.5118 - val_accuracy: 0.4139\nEpoch 4/50\n240/240 [==============================] - 1s 3ms/step - loss: 1.4379 - accuracy: 0.4222 - val_loss: 1.3775 - val_accuracy: 0.4302\nEpoch 5/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.3140 - accuracy: 0.4486 - val_loss: 1.3151 - val_accuracy: 0.4382\nEpoch 6/50\n240/240 [==============================] - 1s 2ms/step - loss: 1.2642 - accuracy: 0.4572 - val_loss: 1.2761 - val_accuracy: 0.4493\nEpoch 7/50\n240/240 [==============================] - 1s 4ms/step - loss: 1.2326 - accuracy: 0.4636 - val_loss: 1.2428 - val_accuracy: 0.4622\nEpoch 8/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.2164 - accuracy: 0.4680 - val_loss: 1.2297 - val_accuracy: 0.4630\nEpoch 9/50\n240/240 [==============================] - 1s 3ms/step - loss: 1.2113 - accuracy: 0.4680 - val_loss: 1.2636 - val_accuracy: 0.4579\nEpoch 10/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.2073 - accuracy: 0.4672 - val_loss: 1.2401 - val_accuracy: 0.4648\nEpoch 11/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.1960 - accuracy: 0.4719 - val_loss: 1.2252 - val_accuracy: 0.4662\nEpoch 12/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.1938 - accuracy: 0.4719 - val_loss: 1.2334 - val_accuracy: 0.4652\nEpoch 13/50\n240/240 [==============================] - 1s 2ms/step - loss: 1.1888 - accuracy: 0.4730 - val_loss: 1.2353 - val_accuracy: 0.4635\nEpoch 14/50\n240/240 [==============================] - 1s 2ms/step - loss: 1.1124 - accuracy: 0.5195 - val_loss: 1.0564 - val_accuracy: 0.5371\nEpoch 15/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.0059 - accuracy: 0.5556 - val_loss: 1.0487 - val_accuracy: 0.5492\nEpoch 16/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.9738 - accuracy: 0.5659 - val_loss: 0.9910 - val_accuracy: 0.5705\nEpoch 17/50\n240/240 [==============================] - 1s 2ms/step - loss: 0.9336 - accuracy: 0.5824 - val_loss: 0.9485 - val_accuracy: 0.5882\nEpoch 18/50\n240/240 [==============================] - 1s 3ms/step - loss: 0.9061 - accuracy: 0.5903 - val_loss: 0.9322 - val_accuracy: 0.5903\nEpoch 19/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.8916 - accuracy: 0.5959 - val_loss: 0.9291 - val_accuracy: 0.6063\nEpoch 20/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.8764 - accuracy: 0.6091 - val_loss: 0.9857 - val_accuracy: 0.5641\nEpoch 21/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.8798 - accuracy: 0.6152 - val_loss: 0.9307 - val_accuracy: 0.5952\nEpoch 22/50\n240/240 [==============================] - 1s 2ms/step - loss: 0.8610 - accuracy: 0.6280 - val_loss: 0.8747 - val_accuracy: 0.6604\nEpoch 23/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.8173 - accuracy: 0.6770 - val_loss: 0.8339 - val_accuracy: 0.7003\nEpoch 24/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.7832 - accuracy: 0.6968 - val_loss: 0.7947 - val_accuracy: 0.7060\nEpoch 25/50\n240/240 [==============================] - 1s 2ms/step - loss: 0.7679 - accuracy: 0.7033 - val_loss: 0.7749 - val_accuracy: 0.7098\nEpoch 26/50\n240/240 [==============================] - 1s 2ms/step - loss: 0.7428 - accuracy: 0.7130 - val_loss: 0.7888 - val_accuracy: 0.7041\nEpoch 27/50\n240/240 [==============================] - 1s 3ms/step - loss: 0.7450 - accuracy: 0.7131 - val_loss: 0.8130 - val_accuracy: 0.6842\nEpoch 28/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.7344 - accuracy: 0.7173 - val_loss: 0.7820 - val_accuracy: 0.7192\n\n\n<keras.callbacks.History at 0x7f0050636980>\n\n\n\n%tensorboard --logdir logs --host 0.0.0.0 \n\nReusing TensorBoard on port 6006 (pid 1475105), started 0:00:15 ago. (Use '!kill 1475105' to kill it.)\n\n\n\n      \n      \n      \n    \n\n\n\n\n\n(1) tf.keras.datasets.fashion_mnist.load_data()을 이용하여 fashion_mnist 자료를 불러온 뒤 아래의 네트워크를 이용하여 적합하라.\n\n이때 n1=6, n2=16, n3=120 으로 설정한다, 드랍아웃비율은 20%로 설정한다.\nnet.summary()를 출력하여 설계결과를 확인하라.\n\n\n\ntf.random.set_seed(4305)\nnet1 = tf.keras.Sequential()\nnet1.add(tf.keras.layers.Conv2D(6,(4,4),activation='relu'))\nnet1.add(tf.keras.layers.MaxPool2D()) \nnet1.add(tf.keras.layers.Conv2D(16,(4,4),activation='relu'))\nnet1.add(tf.keras.layers.MaxPool2D()) \nnet1.add(tf.keras.layers.Flatten())\nnet1.add(tf.keras.layers.Dense(120,activation='relu'))\nnet1.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet1.add(tf.keras.layers.Dropout(0.2))\nnet1.compile(optimizer='adam', loss=tf.losses.categorical_crossentropy,metrics='accuracy')\nnet1.fit(X,y,epochs=5,batch_size=200)\n\nEpoch 1/5\n300/300 [==============================] - 2s 2ms/step - loss: 4.7347 - accuracy: 0.5660\nEpoch 2/5\n300/300 [==============================] - 1s 2ms/step - loss: 3.5843 - accuracy: 0.6647\nEpoch 3/5\n300/300 [==============================] - 1s 3ms/step - loss: 3.5181 - accuracy: 0.6823\nEpoch 4/5\n300/300 [==============================] - 2s 5ms/step - loss: 3.4991 - accuracy: 0.6916\nEpoch 5/5\n300/300 [==============================] - 1s 2ms/step - loss: 3.4950 - accuracy: 0.6990\n\n\n<keras.callbacks.History at 0x7f00505c73a0>\n\n\n\nnet1.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 1ms/step - loss: 0.4629 - accuracy: 0.8334\n\n\n[0.46292147040367126, 0.8334000110626221]\n\n\n\nnet1.summary()\n\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 25, 25, 6)         102       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 12, 12, 6)        0         \n )                                                               \n                                                                 \n conv2d_1 (Conv2D)           (None, 9, 9, 16)          1552      \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 4, 4, 16)         0         \n 2D)                                                             \n                                                                 \n flatten_3 (Flatten)         (None, 256)               0         \n                                                                 \n dense_9 (Dense)             (None, 120)               30840     \n                                                                 \n dense_10 (Dense)            (None, 10)                1210      \n                                                                 \n dropout (Dropout)           (None, 10)                0         \n                                                                 \n=================================================================\nTotal params: 33,704\nTrainable params: 33,704\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nc1, m1, c2, m2, flttn, dns1, dns2, dropout = net1.layers\n\n\nprint(X.shape)\nprint(c1(X).shape) \nprint(m1(c1(X)).shape)\nprint(c2(m1(c1(X))).shape)\nprint(m2(c2(m1(c1(X)))).shape)\nprint(flttn(m2(c2(m1(c1(X))))).shape)\nprint(dns1(flttn(m2(c2(m1(c1(X)))))).shape)\nprint(dns2(dns1(flttn(m2(c2(m1(c1(X))))))).shape)\nprint(dropout(dns2(dns1(flttn(m2(c2(m1(c1(X)))))))).shape)\n\n(60000, 28, 28, 1)\n(60000, 25, 25, 6)\n(60000, 12, 12, 6)\n(60000, 9, 9, 16)\n(60000, 4, 4, 16)\n(60000, 256)\n(60000, 120)\n(60000, 10)\n(60000, 10)\n\n\n(2) n1=(6,64,128), n2=(16,256)에 대하여 test set의 loss가 최소화되는 조합을 찾아라. 결과를 텐서보드로 시각화하는 코드를 작성하라. - epoc은 3회로 한정한다. - validation_split은 0.2로 설정한다.\n\nfrom tensorboard.plugins.hparams import api as hp\n\n\n!rm -rf logs\nfor u in [6,64,128]: \n    for d in [16,256]: \n        logdir = 'logs/hpguebin_{}_{}'.format(u,d)\n        with tf.summary.create_file_writer(logdir).as_default():\n            tf.random.set_seed(4305)\n            net1 = tf.keras.Sequential()\n            net1.add(tf.keras.layers.Conv2D(6,(4,4),activation='relu'))\n            net1.add(tf.keras.layers.MaxPool2D()) \n            net1.add(tf.keras.layers.Conv2D(16,(4,4),activation='relu'))\n            net1.add(tf.keras.layers.MaxPool2D()) \n            net1.add(tf.keras.layers.Flatten())\n            net1.add(tf.keras.layers.Dense(120,activation='relu'))\n            net1.add(tf.keras.layers.Dense(10,activation='softmax'))\n            net1.add(tf.keras.layers.Dropout(0.2))\n            net1.compile(optimizer='adam', loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n            cb3 = hp.KerasCallback(logdir, {'n1':u, 'n2':d})\n            net1.fit(X,y,epochs=3,batch_size=200,callbacks=cb3,validation_split=0.2)\n            _rslt=net.evaluate(XX,yy) \n            tf.summary.scalar('test set loss', _rslt[0], step=1)\n\nEpoch 1/3\n240/240 [==============================] - 1s 3ms/step - loss: 4.5247 - accuracy: 0.5634 - val_loss: 0.6971 - val_accuracy: 0.7641\nEpoch 2/3\n240/240 [==============================] - 1s 2ms/step - loss: 3.6050 - accuracy: 0.6616 - val_loss: 0.5841 - val_accuracy: 0.7986\nEpoch 3/3\n240/240 [==============================] - 1s 3ms/step - loss: 3.5395 - accuracy: 0.6785 - val_loss: 0.5066 - val_accuracy: 0.8183\n313/313 [==============================] - 0s 1ms/step - loss: 0.7856 - accuracy: 0.7178\nEpoch 1/3\n240/240 [==============================] - 1s 2ms/step - loss: 4.5250 - accuracy: 0.5634 - val_loss: 0.6955 - val_accuracy: 0.7667\nEpoch 2/3\n240/240 [==============================] - 0s 2ms/step - loss: 3.6047 - accuracy: 0.6609 - val_loss: 0.5796 - val_accuracy: 0.8022\nEpoch 3/3\n240/240 [==============================] - 1s 2ms/step - loss: 3.5403 - accuracy: 0.6794 - val_loss: 0.5066 - val_accuracy: 0.8194\n313/313 [==============================] - 0s 1ms/step - loss: 0.7856 - accuracy: 0.7178\nEpoch 1/3\n240/240 [==============================] - 1s 2ms/step - loss: 4.5250 - accuracy: 0.5634 - val_loss: 0.6955 - val_accuracy: 0.7667\nEpoch 2/3\n240/240 [==============================] - 0s 2ms/step - loss: 3.6043 - accuracy: 0.6614 - val_loss: 0.5811 - val_accuracy: 0.8035\nEpoch 3/3\n240/240 [==============================] - 0s 2ms/step - loss: 3.5402 - accuracy: 0.6796 - val_loss: 0.5060 - val_accuracy: 0.8189\n313/313 [==============================] - 0s 1ms/step - loss: 0.7856 - accuracy: 0.7178\nEpoch 1/3\n240/240 [==============================] - 1s 2ms/step - loss: 4.5264 - accuracy: 0.5641 - val_loss: 0.6979 - val_accuracy: 0.7617\nEpoch 2/3\n240/240 [==============================] - 1s 2ms/step - loss: 3.6080 - accuracy: 0.6594 - val_loss: 0.5781 - val_accuracy: 0.7992\nEpoch 3/3\n240/240 [==============================] - 1s 2ms/step - loss: 3.5403 - accuracy: 0.6784 - val_loss: 0.5089 - val_accuracy: 0.8193\n313/313 [==============================] - 0s 1ms/step - loss: 0.7856 - accuracy: 0.7178\nEpoch 1/3\n240/240 [==============================] - 1s 2ms/step - loss: 4.5266 - accuracy: 0.5646 - val_loss: 0.7037 - val_accuracy: 0.7628\nEpoch 2/3\n240/240 [==============================] - 1s 2ms/step - loss: 3.6056 - accuracy: 0.6609 - val_loss: 0.5811 - val_accuracy: 0.8027\nEpoch 3/3\n240/240 [==============================] - 1s 2ms/step - loss: 3.5394 - accuracy: 0.6799 - val_loss: 0.5044 - val_accuracy: 0.8208\n313/313 [==============================] - 0s 1ms/step - loss: 0.7856 - accuracy: 0.7178\nEpoch 1/3\n240/240 [==============================] - 1s 2ms/step - loss: 4.5266 - accuracy: 0.5643 - val_loss: 0.7022 - val_accuracy: 0.7646\nEpoch 2/3\n240/240 [==============================] - 0s 2ms/step - loss: 3.6060 - accuracy: 0.6614 - val_loss: 0.5794 - val_accuracy: 0.8032\nEpoch 3/3\n240/240 [==============================] - 1s 3ms/step - loss: 3.5388 - accuracy: 0.6797 - val_loss: 0.5140 - val_accuracy: 0.8155\n313/313 [==============================] - 0s 1ms/step - loss: 0.7856 - accuracy: 0.7178\n\n\n\n%tensorboard --logdir logs --host 0.0.0.0\n\nReusing TensorBoard on port 6006 (pid 1475105), started 0:00:37 ago. (Use '!kill 1475105' to kill it.)\n\n\n\n      \n      \n      \n    \n\n\n\n\n\ntf.keras.datasets.cifar10.load_data()을 이용하여 CIFAR10을 불러온 뒤 적당한 네트워크를 사용하여 적합하라.\n\n결과를 텐서보드로 시각화할 필요는 없다.\n자유롭게 모형을 설계하여 적합하라.\ntest set의 accuracy가 70%이상인 경우만 정답으로 인정한다.\n\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n\n\nx_train.shape\n\n(50000, 32, 32, 3)\n\n\n\nX = tf.constant(x_train.reshape(-1,32,32,3),dtype=tf.float64)\ny = tf.keras.utils.to_categorical(y_train)\nXX = tf.constant(x_test.reshape(-1,32,32,3),dtype=tf.float64)\nyy = tf.keras.utils.to_categorical(y_test)\n\n\nprint(X.shape)\nprint(y.shape)\nprint(XX.shape)\nprint(yy.shape)\n\n(50000, 32, 32, 3)\n(50000, 10)\n(10000, 32, 32, 3)\n(10000, 10)\n\n\n\nnet2 = tf.keras.Sequential()\nnet2.add(tf.keras.layers.Conv2D(512,(2,2),activation='relu'))\nnet2.add(tf.keras.layers.Conv2D(512,(2,2),activation='relu'))\nnet2.add(tf.keras.layers.Dropout(0.5))\nnet2.add(tf.keras.layers.MaxPool2D()) \nnet2.add(tf.keras.layers.Conv2D(512,(2,2),activation='relu'))\nnet2.add(tf.keras.layers.Conv2D(512,(2,2),activation='relu'))\nnet2.add(tf.keras.layers.Dropout(0.5))\nnet2.add(tf.keras.layers.MaxPool2D()) \nnet2.add(tf.keras.layers.Flatten())\nnet2.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet2.compile(optimizer='adam', loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n\n\nnet2.fit(X,y,epochs=5,validation_split=0.2)\n\nEpoch 1/5\n1250/1250 [==============================] - 44s 35ms/step - loss: 2.2100 - accuracy: 0.4334 - val_loss: 1.4963 - val_accuracy: 0.5213\nEpoch 2/5\n1250/1250 [==============================] - 43s 35ms/step - loss: 1.2402 - accuracy: 0.5626 - val_loss: 1.3536 - val_accuracy: 0.5743\nEpoch 3/5\n1250/1250 [==============================] - 43s 35ms/step - loss: 1.1110 - accuracy: 0.6104 - val_loss: 1.2189 - val_accuracy: 0.6069\nEpoch 4/5\n1250/1250 [==============================] - 43s 35ms/step - loss: 1.0174 - accuracy: 0.6427 - val_loss: 1.1409 - val_accuracy: 0.6466\nEpoch 5/5\n1250/1250 [==============================] - 43s 35ms/step - loss: 0.9725 - accuracy: 0.6606 - val_loss: 1.0891 - val_accuracy: 0.6455\n\n\n<keras.callbacks.History at 0x7f005037d930>\n\n\n\nnet2.fit(X,y,epochs=10,validation_split=0.2)\n\nEpoch 1/10\n1250/1250 [==============================] - 44s 35ms/step - loss: 0.9142 - accuracy: 0.6825 - val_loss: 1.0787 - val_accuracy: 0.6388\nEpoch 2/10\n1250/1250 [==============================] - 44s 35ms/step - loss: 0.8797 - accuracy: 0.6935 - val_loss: 1.0044 - val_accuracy: 0.6667\nEpoch 3/10\n1250/1250 [==============================] - 44s 35ms/step - loss: 0.8509 - accuracy: 0.7039 - val_loss: 0.9295 - val_accuracy: 0.6799\nEpoch 4/10\n1250/1250 [==============================] - 44s 35ms/step - loss: 0.8338 - accuracy: 0.7113 - val_loss: 0.9554 - val_accuracy: 0.6747\nEpoch 5/10\n1250/1250 [==============================] - 44s 35ms/step - loss: 0.8201 - accuracy: 0.7141 - val_loss: 1.0631 - val_accuracy: 0.6435\nEpoch 6/10\n1250/1250 [==============================] - 44s 35ms/step - loss: 0.8030 - accuracy: 0.7235 - val_loss: 0.9158 - val_accuracy: 0.6929\nEpoch 7/10\n1250/1250 [==============================] - 44s 35ms/step - loss: 0.7904 - accuracy: 0.7266 - val_loss: 0.9175 - val_accuracy: 0.6878\nEpoch 8/10\n1250/1250 [==============================] - 44s 35ms/step - loss: 0.7732 - accuracy: 0.7323 - val_loss: 0.9135 - val_accuracy: 0.6860\nEpoch 9/10\n1250/1250 [==============================] - 44s 35ms/step - loss: 0.7665 - accuracy: 0.7327 - val_loss: 0.9677 - val_accuracy: 0.6756\nEpoch 10/10\n1250/1250 [==============================] - 44s 35ms/step - loss: 0.7622 - accuracy: 0.7346 - val_loss: 0.8576 - val_accuracy: 0.7060\n\n\n<keras.callbacks.History at 0x7f00cda3aaa0>\n\n\n\nnet2.evaluate(XX,yy)\n\n313/313 [==============================] - 4s 11ms/step - loss: 0.8638 - accuracy: 0.7041\n\n\n[0.8638339042663574, 0.7041000127792358]\n\n\n\n\n\n\n(1,128,128,3)의 shape을 가진 텐서가 tf.keras.layers.Conv2D(5,(2,2))으로 만들어진 커널을 통과할시 나오는 shape은?\n\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(5,(2,2))\nXXX = tnp.array([1]*1*128*128*3,dtype=tf.float64).reshape(1,128,128,3)\n\n\ncnv(XXX)\n\n<tf.Tensor: shape=(1, 127, 127, 5), dtype=float32, numpy=\narray([[[[-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         ...,\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703]],\n\n        [[-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         ...,\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703]],\n\n        [[-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         ...,\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703]],\n\n        ...,\n\n        [[-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         ...,\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703]],\n\n        [[-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         ...,\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703]],\n\n        [[-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         ...,\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703]]]], dtype=float32)>\n\n\n답 : (1, 127, 127, 5)\n\n(1,24,24,16)의 shape을 가진 텐서가 tf.keras.layers.Flatten()을 통과할때 나오는 텐서의 shape은?\n\n\n24*24*16\n\n9216\n\n\n답 : (1, 9216)"
  },
  {
    "objectID": "posts/2022-01-13-회귀분석_중회귀모형.html",
    "href": "posts/2022-01-13-회귀분석_중회귀모형.html",
    "title": "jisim12",
    "section": "",
    "text": "회귀분석\n\n\ntoc:false\nbranch: master\nbadges: true\ncomments: true\nauthor: 심재인\n\n\n\n\n자료구조\n\nresponse\\(\\quad\\) explanatory\n\\(\\begin{pmatrix}  y_{1} \\\\  y_{2} \\\\  \\vdots \\\\  y_{n}  \\end{pmatrix}\\) \\(\\begin{pmatrix}  x_{11} & \\cdots & x_{1p} \\\\  x_{21} & \\cdots & x_{2p} \\\\  \\vdots & \\ddots & \\vdots \\\\  x_{n1} & \\cdots & x_{np}  \\end{pmatrix}\\)\n\n\n\n\n설명변수가 \\(p\\)개인 다중(선형)회귀모형\n\\(\\qquad\\qquad y_i=\\beta_0+\\beta_1x_{1i}+\\dots+\\beta_{p}x_{ip}+\\epsilon_i\\quad i = 1,2,\\dots,n\\)\n\n회귀모수 : \\(\\beta_0,\\beta_1,\\dots,\\beta_p\\qquad\\longrightarrow (p+1)개\\)\n\n설명변수(독립변수) :\n\\(X_1 = (x_{11},\\dots,x_{n1})^T,\\dots,X_p = (x_{1p},\\dots,x_{np})^T\\)\n\n반응변수(종속변수) : \\(Y=(y_1,\\dots,y_n)^T\\)\n\n오차항 : \\(\\epsilon_1,\\dots,\\epsilon_n,\\;(\\sim_{i.i.d}N(0,\\sigma^2))\\)\n\n설명변수가 \\(p\\)개인 다중(선형)회귀모형 : 행렬형식\n\\(\\qquad\\qquad Y=X\\beta +\\epsilon\\)\n\n회귀모수 : \\(\\beta = (\\beta_0,\\beta_1,\\dots,\\beta_p)^T, (p+1)\\) vetor\n\n설명변수(독립변수) :\n\\(X = (1,X_1,\\dots,X_p)^T, (n\\times(P+1))\\) matrix\n\n반응변수(종속변수) : \\(Y = (y_1,\\dots,y_n)^T,\\;n\\) vector\n\n오차항 : \\(\\epsilon = (\\epsilon_1,\\dots,\\epsilon_n)^T\\;n\\) vector\n\n자료구조\n\n\\(\\begin{pmatrix} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{n}  \\end{pmatrix}\\) = \\(\\begin{pmatrix} 1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\ 1 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & x_{n2} & \\cdots & x_{np}  \\end{pmatrix}\\) \\(\\begin{pmatrix} \\beta_{0} \\\\ \\beta_{1} \\\\ \\vdots \\\\ \\beta_{p}  \\end{pmatrix}\\) + \\(\\begin{pmatrix} \\epsilon_{1} \\\\ \\epsilon_{2} \\\\ \\vdots \\\\ \\epsilon_{n}  \\end{pmatrix}\\)\n\n\\(\\quad\\; n\\times 1\\qquad\\qquad n\\times(p+1)\\qquad\\;(p+1)\\times 1\\quad n\\times 1\\)\n\n\n\n\n최소제곱추정량 :\n\\((\\hat{\\beta_0},\\hat{\\beta_1},\\dots,\\hat{\\beta_p})\\) =\n\\(_{({\\beta_0},\\dots,{\\beta_p})\\in R^{p+1}} argmin\\displaystyle\\sum_{i=1}^{n}\\{y_i - (\\beta_0+\\beta_1x_{i1}+\\dots +\\beta_px_{ip})\\}^2\\)\n\n또는\n\\(\\qquad\\hat{\\beta} = _{\\beta\\in R^{p+1}} argmin\\;||Y - X\\beta||^2 = (X^T X)^{-1}X^T Y\\)\n\n\n\n\n잔차 (residual) : \\(e_i = y_i - \\hat{y_i}\\) - 오차분산 \\((\\sigma^2)\\)의 추정 :\n\n잔차(오차) 제곱합 (residual (or error) sum of squares) :\n\\[SSE = \\displaystyle\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2=\\displaystyle\\sum_{i=1}^{n}e_i^2\\]\n\n평균제곱오차 (mean squared error) : \\(MSE=\\frac{SSE}{n-(p+1)}\\)\n\n오차분산의 추정값 : \\(\\hat{\\sigma}^2 = MSE\\)\n\n\n\n\n\n\n총편차의 분해\n\n\\(y_i - \\overline y = (y_i-\\hat{y_i})+(\\hat{y_i}-\\overline y),\\;\\forall i\\)\n\n총편차(total deviation) = \\(y_i - \\overline y\\)\n\n추측값의 편차 = \\((\\hat{y_i}-\\overline{\\hat{y_i}})+(\\hat{y_i}-\\overline y)\\)\n\\(\\Rightarrow\\) 총편차 = 잔차 + 추측값의 편차\n\n\n제곱합의 분해 : SST = SSE + SSR\n\\[\\displaystyle\\sum_{i=1}^{n}(y_i - \\overline y)^2 = \\displaystyle\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2 + \\displaystyle\\sum_{i=1}^{n}(\\hat{y_i} - \\overline y)^2\\]\n—————- 제곱합의 종류 \\(\\qquad\\qquad\\) 정의 및 기호 \\(\\qquad\\qquad\\) 자유도\n\n\n\n\n\n\n\n총제곱함 \\(\\qquad\\qquad SST = \\sum_{i=1}^{n}(y_i - \\overline y)^2\\qquad n-1\\) 잔차제곱합 \\(\\qquad\\quad\\; SSE = \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2\\quad n-(p+1)\\) 회귀제곱합 \\(\\qquad\\quad\\; SSR = \\sum_{i=1}^{n}(\\hat{y_i} - \\overline y)^2\\qquad\\quad p\\)\n\n\n\n\n\n\n\n결정계수 (Coefficient of determination)\n\\[R^2 = \\frac{SSR}{SST} = 1-\\frac{SSE}{SST}\\]\n\n수정된 결정계수 (Adjusted multiple correlation coefficient)\n\\[R_{adj}^2 = 1-\\frac{SSE/(n-p-1)}{SST/(n-1)}\\]\n\n\n\n\n\n회귀직선의 유의성 검정 (F-test)\n\n가설 : \\(H_0 : \\beta_1 = \\dots = \\beta_p=0_{vs.}H_1:notH_0\\)\n\n검정통계량 : \\(F = \\frac{MSR}{MSE}=\\frac{SSR/p}{SSE/(n-(p+1))}\\sim_{H_0}F(p,n-p-1)\\)\n\n검정통계량의 관측값 : \\(f\\).\n\n유의수준 \\(\\alpha\\)에서의 기각역 : \\(f\\geq F_{\\alpha}(p,n-p-1)\\)\n\n유의확률 = \\(P(F\\geq f)\\)\n\n\n\n요인 \\(\\qquad\\) 제곱합 \\(\\qquad\\) 자유도(df) \\(\\qquad\\qquad\\) 평균제곱(MS) \\(\\qquad\\qquad f\\qquad\\quad\\) 유의확률\n\n\n\n\n\n\n\n회귀 \\(\\qquad\\quad SSR\\qquad\\quad p\\qquad\\qquad\\quad MSR=\\frac{SSR}{P}\\qquad\\; f=\\frac{MSR}{MSE}\\qquad P(F\\geq f)\\) 잔차 \\(\\qquad\\quad SSE\\qquad\\quad n-(p+1)\\quad\\; MSE=\\frac{SSE}{n-p-1}\\)\n\n\n\n\n계 \\(\\qquad\\qquad SST\\qquad\\quad n-1\\)\n\n\n\n\nReduced model(RM)\\(_{vs.}\\) Full model(FM)\n\\(\\qquad FM : y_i = \\beta_0+\\beta_1x_{i1}+\\beta_qx_{iq}+\\dots+\\beta_p x_{ip}+\\epsilon_i\\)\n\\(\\qquad RM : y_i = \\beta_0+\\beta_1x_{i1}+\\beta_qx_{iq}+\\epsilon_i\\)\n\n가설 : \\(H_0:\\beta_{q+1} =\\dots= \\beta_p=0\\;_{vs.}\\;H_1:not\\;H_0\\)\n\n검정통계량\n\\[F = \\frac{SSR_{FM}-SSR_{RM}/(p-q)}{SSE_{FM}/(n-p-1)}\\sim_{H_o}\\;F(p-q,n-p-1)\\]\n\n검정통계량의 관측값 : \\(f\\)\n\n유의수준 \\(\\alpha\\)에서의 기각역 : \\(f\\geq F_{\\alpha}(p-q,n-p-1)\\)\n\n유의확률 = \\(P(F\\geq f)\\)\n\n\nGeneral Linear Hypothesis\n\\[H_0:H\\beta=0\\;_{vs.}\\;H_1:not\\;H_0\\]\n\n\\(H:r\\times(p+1)\\;matrix, rank(H) = r\\)\n\n\\(\\beta = (\\beta_0,\\beta_1,\\dots,\\beta_p)^T\\)\n\n검정통계량\n\\[F=\\frac{(SSR_{FM}-SSR_{RM})/(p-q)}{SSE_{FM}/(n-p-1)}\\sim_{H_0} F(r,n-p-1)\\]\n\n\n\n\n\n\n\\(\\beta_1,\\beta_2,\\dots,\\beta_p\\) 에 대한 추론\n\n\\(\\hat{\\beta}=X(X^TX)^{-1}Y\\)\n\n\\(\\frac{\\hat{\\beta_i}-\\beta_i}{_{s.e.}(\\hat{\\beta_i})}\\sim t(n-p-1),\\;_{s.e.}(\\hat{\\beta_i})=d_{ii}\\hat{\\sigma}\\)\n\n\\(d_{ii}\\) ; diagonal elements of \\(D^{-1}, i=1,\\dots,p,\\)\n\\(D = \\begin{pmatrix} s_{11} & \\cdots & s_{1p} \\\\ \\vdots & \\ddots & \\vdots \\\\ s_{p1} & \\cdots & s_{pp}  \\end{pmatrix}\\),\n\\(s_{ij} = \\sum_{k=11}^{n}(x_{ki}-\\overline x_i)(x_{kj}-\\overline x_j)\\)\n\n\n가설검정 : \\(H_0 : \\beta_i=\\beta_i^0\\).\n\n\n대립가설 \\(\\qquad\\qquad\\;\\) 유의확률 \\(\\qquad\\quad\\) 유의수준 \\(\\alpha\\) 기각역\n\n\n\n\n\n\n\n\\(H_1:\\beta_i>\\beta_i^0\\qquad P(T\\geq t)\\qquad\\quad t\\geq t_{\\alpha}(n-p-1)\\) \\(H_1:\\beta_i<\\beta_i^0\\qquad P(T\\leq t)\\qquad\\quad t\\geq t_{\\alpha}(n-p-1)\\) \\(H_1:\\beta_i\\neq\\beta_i^0\\qquad P(|T|\\geq |t|)\\qquad |t|\\geq t_{\\alpha/2}(n-p-1)\\)\n\n\n\n\n모회귀계수(절편)\\(\\beta_0\\) 에 대한 추론\n\n\\(\\frac{\\hat{\\beta_0}-\\beta_0}{_{s.e.}(\\hat{\\beta_0})}\\sim t(n-p-1)\\),\n\n\\(_{s.e.}(\\hat{\\beta}_0) = \\hat{\\sigma}(\\frac{1}{n}+\\displaystyle\\sum_{i=1}^{p}\\displaystyle\\sum_{j=1}^{p}\\overline x_i d_{ij}\\overline x_{j})^{1/2}\\)\n\n\n\n\n\n\n$X=X_0=(x_{01},,x_{0p})^T 가 주어졌을 때 평균반응의 예측\n\n평균반응 (mean response) :\n\\(\\mu_0 = E(Y|X_0) = \\beta_0+\\beta_1x_{01}+\\dots+\\beta_px_{0p}\\)\n\n평균반응 추정량 : \\(\\hat{\\mu_0}=\\hat{\\beta_0}+\\hat{\\beta_1}x_{01}+\\dots+\\hat{\\beta_p}x_{op}\\)\n\n\\(\\frac{\\hat{\\mu_0}-\\mu_0}{_{s.e.}(\\hat{\\mu_0})}\\sim t(n-p-1)\\)\n\n\\(_{s.e.}(\\hat{mu_{0}}) = \\hat{\\sigma}\\bigg(\\frac{1}{n}+\\displaystyle\\sum_{i=1}^{p}\\displaystyle\\sum_{j=1}^{p}(x_{0i}-\\overline x_i)d_{ij}(x_{0j}-\\overline x_{j})\\bigg)^{1/2}\\)\n\n\\(\\hat{\\mu_0}의\\;100(1-\\alpha)\\)% 신뢰구간 : \\(\\hat{\\mu_{0}}\\pm t_{\\alpha/2}(n-p-1)_{s.e.}(\\hat{\\mu_{0}})\\)\n\n\n\n\n\n\n\\(X=X_0\\) 가 주어졌을 때 \\(y=y_0\\) 예측\n\n\\(y_0=\\beta_0+\\beta_1 x_{01}+\\dots+\\beta_p x_{0p}+\\epsilon_0\\)\n\n예측값 : \\(\\hat{y_0}=\\hat{\\beta_0}+\\hat{\\beta_1}x_{01}+\\dots+\\hat{\\beta_{p}}x_{0p}\\)\n\n\\(\\frac{\\hat{y_0}-y_0}{_{s.e.}(\\hat{y_0})}\\sim t(n-p-1)\\)\n\n\\(_{s.e.}(\\hat{y_0}) = \\hat{\\sigma}\\bigg(1+\\frac{1}{n}+\\displaystyle\\sum_{i=1}^{p}\\displaystyle\\sum_{j=1}^{p}(x_{0i}-\\overline x_i)d_{ij}(x_{0j}-\\overline x_{j})\\bigg)^{1/2}\\)\n\n\\(\\hat{y_0}\\)의 \\(100(1-\\alpha)\\) % 신뢰구간 : \\(\\hat{y_0}\\pm t_{\\alpha/2}(n-p-1)_{s.e.}(\\hat{y_{0}})\\)"
  },
  {
    "objectID": "posts/2022-04-11-6주차_빅데이터분석특강.html",
    "href": "posts/2022-04-11-6주차_빅데이터분석특강.html",
    "title": "jisim12",
    "section": "",
    "text": "6주차 4월11일\n\n빅데이터분석특강\n\n\ntoc:false\nbranch: master\nbadges: true\ncomments: true\nauthor: 심재인\n\n\nimports\n\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport tensorflow as tf \nimport tensorflow.experimental.numpy as tnp \n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+s + '; }')\n\n\n\n\\(x \\to \\hat{y}\\) 가 되는 과정을 그림으로 그리기\n- 단순회귀분석의 예시 - \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i, \\quad i=1,2,\\dots,n\\)\n(표현1)\n\ngv(''' \n    \"1\" -> \"β̂₀ + xₙ*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"xₙ\" -> \"β̂₀ + xₙ*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + xₙ*β̂₁,    bias=False\" -> \"ŷₙ\"[label=\"identity\"]\n\n    \".\" -> \"....................................\"[label=\"* β̂₀\"]\n    \"..\" -> \"....................................\"[label=\"* β̂₁\"]\n    \"....................................\" -> \"...\"[label=\" \"]\n\n    \"1 \" -> \"β̂₀ + x₂*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"x₂\" -> \"β̂₀ + x₂*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + x₂*β̂₁,    bias=False\" -> \"ŷ₂\"[label=\"identity\"]\n    \n    \"1  \" -> \"β̂₀ + x₁*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"x₁\" -> \"β̂₀ + x₁*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + x₁*β̂₁,    bias=False\" -> \"ŷ₁\"[label=\"identity\"]\n''')\n\nExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\n\n\n<graphviz.sources.Source at 0x7f9cd4fa9540>\n\n\n- 표현1의 소감? - 교수님이 고생해서 만든것 같음 - 그런데 그냥 다 똑같은 그림의 반복이라 사실 고생한 의미가 없음.\n(표현2)\n- 그냥 아래와 같이 그리고 “모든 \\(i=1,2,3,\\dots,n\\)에 대하여 \\(\\hat{y}_i\\)을 아래의 그림과 같이 그린다”고 하면 될것 같다.\n\ngv(''' \n    \"1\" -> \"β̂₀ + xᵢ*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"xᵢ\" -> \"β̂₀ + xᵢ*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + xᵢ*β̂₁,    bias=False\" -> \"ŷᵢ\"[label=\"identity\"]\n\n''')\n\nExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\n\n\n<graphviz.sources.Source at 0x7f9cd49c2830>\n\n\n(표현3)\n- 그런데 “모든 \\(i=1,2,3,\\dots,n\\)에 대하여 \\(\\hat{y}_i\\)을 아래의 그림과 같이 그린다” 라는 언급자체도 반복할 필요가 없을 것 같다. (어차피 당연히 그럴테니까) 그래서 단순히 아래와 같이 그려도 무방할듯 하다.\n\ngv(''' \n    \"1\" -> \"β̂₀ + x*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"x\" -> \"β̂₀ + x*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + x*β̂₁,    bias=False\" -> \"ŷ\"[label=\"identity\"]\n\n''')\n\nExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\n\n\n<graphviz.sources.Source at 0x7f9cd4a199f0>\n\n\n(표현4)\n- 위의 모델은 아래와 같이 쓸 수 있다. (\\(\\beta_0\\)를 바이어스로 표현)\n\ngv('''\n\"x\" -> \"x*β̂₁,    bias=True\"[label=\"*β̂₁\"] ;\n\"x*β̂₁,    bias=True\" -> \"ŷ\"[label=\"indentity\"] ''')\n\nExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\n\n\n<graphviz.sources.Source at 0x7f9cd487dab0>\n\n\n\n실제로는 이 표현을 많이 사용함\n\n(표현5)\n- 벡터버전으로 표현하면 아래와 같다. 이 경우에는 \\({\\bf X}=[1,x]\\)에 포함된 1이 bias의 역할을 해주므로 bias = False 임.\n\ngv('''\n\"X\" -> \"X@β̂,    bias=False\"[label=\"@β̂\"] ;\n\"X@β̂,    bias=False\" -> \"ŷ\"[label=\"indentity\"] ''')\n\nExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\n\n\n<graphviz.sources.Source at 0x7f9cd4885270>\n\n\n\n저는 이걸 좋아해요\n\n(표현5)’\n- 딥러닝에서는 \\(\\hat{\\boldsymbol{\\beta}}\\) 대신에 \\(\\hat{{\\bf W}}\\)을 라고 표현한다.\n\n#collapse\ngv('''\n\"X\" -> \"X@Ŵ,    bias=False\"[label=\"@Ŵ\"] ;\n\"X@Ŵ,    bias=False\" -> \"ŷ\"[label=\"identity\"] ''')\n\nExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\n\n\n<graphviz.sources.Source at 0x7f9cd4751ff0>\n\n\n- 실제로는 표현4 혹은 표현5를 외우면 된다.\n\n\nLayer의 개념\n- (표현4) 혹은 (표현5)의 그림은 레이어로 설명할 수 있다.\n- 레이어는 항상 아래와 같은 규칙을 가진다. - 첫 동그라미는 레이어의 입력이다. - 첫번째 화살표는 선형변환을 의미한다. - 두번째 동그라미는 선형변환의 결과이다. (이때 bias가 false인지 true인지에 따라서 실제 수식이 조금 다름) - 두번째 화살표는 두번째 동그라미에 어떠한 함수 \\(f\\)를 취하는 과정을 의미한다. (우리의 그림에서는 \\(f(x)=x\\)) - 세번째 동그라미는 레이어의 최종출력이다.\n- 엄청 복잡한데, 결국 레이어를 만들때 위의 그림들을 의미하도록 하려면 아래의 4개의 요소만 필요하다. 1. 레이어의 입력차원 2. 선형변환의 결과로 얻어지는 차원 3. 선형변환에서 바이어스를 쓸지? 안쓸지? 4. 함수 \\(f\\)\n- 주목: 1,2가 결정되면 자동으로 \\(\\hat{{\\bf W}}\\)의 차원이 결정된다.\n(예시) - 레이어의 입력차원=2, 선형변환의 결과로 얻어지는 차원=1: \\(\\hat{\\bf W}\\)는 (2,1) 매트릭스 - 레이어의 입력차원=20, 선형변환의 결과로 얻어지는 차원=5: \\(\\hat{\\bf W}\\)는 (20,5) 매트릭스 - 레이어의 입력차원=2, 선형변환의 결과로 얻어지는 차원=50: \\(\\hat{\\bf W}\\)는 (2,50) 매트릭스\n- 주목2: 이중에서 절대 생략불가능 것은 “2. 선형변환의 결과로 얻어지는 차원” 이다. - 레이어의 입력차원: 실제 레이어에 데이터가 들어올 때 데이터의 입력차원을 컴퓨터 스스로 체크하여 \\(\\hat{\\bf W}\\)의 차원을 결정할 수 있음. - 바이어스를 쓸지? 안쓸지? 기본적으로 쓴다고 가정한다. - 함수 \\(f\\): 기본적으로 항등함수를 가정하면 된다.\n\n\nKeras를 이용한 풀이\n- 기본뼈대: net생성 \\(\\to\\) add(layer) \\(\\to\\) compile(opt,loss) \\(\\to\\) fit(data,epochs)\n- 데이터정리\n\\[{\\bf y}\\approx 2.5 +4*x\\]\n\ntnp.random.seed(43052)\nN= 200 \nx= tnp.linspace(0,1,N)\nepsilon= tnp.random.randn(N)*0.5 \ny= 2.5+4*x +epsilon\n\n2022-04-18 13:26:23.084963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n\nX=tf.stack([tf.ones(N,dtype='float64'),x],axis=1)\n\n\n풀이1: 스칼라버전\n(0단계) 데이터정리\n\ny=y.reshape(N,1)\nx=x.reshape(N,1)\nx.shape,y.shape\n\n(TensorShape([200, 1]), TensorShape([200, 1]))\n\n\n(1단계) net 생성\n\nnet = tf.keras.Sequential() \n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1) \nnet.add(layer)\n\n(3단계) net.compile(opt,loss_fn)\n\nnet.compile(tf.keras.optimizers.SGD(0.1), tf.keras.losses.MSE) \n\n(4단계) net.fit(x,y,epochs)\n\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N) # batch_size=N 일 경우에 경사하강법이 적용, batch_size!=N 이면 확률적 경사하강법 적용\n\n<keras.callbacks.History at 0x7f9cd3fc3af0>\n\n\n(결과확인)\n\nnet.weights\n\n[<tf.Variable 'dense/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[3.9330251]], dtype=float32)>,\n <tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([2.5836723], dtype=float32)>]\n\n\n\n\n풀이2: 벡터버전\n(0단계) 데이터정리\n\nX.shape,y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net 생성\n\nnet = tf.keras.Sequential() \n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1,use_bias=False) \nnet.add(layer)\n\n(3단계) net.compile(opt,loss_fn)\n\nnet.compile(tf.keras.optimizers.SGD(0.1), tf.keras.losses.MSE) \n\n(4단계) net.fit(x,y,epochs)\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) # batch_size=N 일 경우에 경사하강법이 적용, batch_size!=N 이면 확률적 경사하강법 적용 \n\n<keras.callbacks.History at 0x7f9c6426a2c0>\n\n\n(결과확인)\n\nnet.weights\n\n[<tf.Variable 'dense_1/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5836723],\n        [3.9330251]], dtype=float32)>]\n\n\n\n\n잠시문법정리\n- 잠깐 Dense layer를 만드는 코드를 정리해보자.\n\n아래는 모두 같은 코드이다.\n\n\ntf.keras.layers.Dense(1)\ntf.keras.layers.Dense(units=1)\ntf.keras.layers.Dense(units=1,activation=‘linear’) // identity 가 더 맞는것 같은데..\ntf.keras.layers.Dense(units=1,activation=‘linear’,use_bias=True)\n\n\n아래의 코드1,2는 (1)의 코드들과 살짝 다른코드이다. (코드1과 코드2는 같은코드임)\n\n\ntf.keras.layers.Dense(1,input_dim=2) # 코드1\ntf.keras.layers.Dense(1,input_shape=(2,)) # 코드2\n\n\n아래는 사용불가능한 코드이다.\n\n\ntf.keras.layers.Dense(1,input_dim=(2,)) # 코드1\ntf.keras.layers.Dense(1,input_shape=2) # 코드2\n\n- 왜 input_dim이 필요한가?\n\nnet1 = tf.keras.Sequential()\nnet1.add(tf.keras.layers.Dense(1,use_bias=False)) \n\n\nnet2 = tf.keras.Sequential()\nnet2.add(tf.keras.layers.Dense(1,use_bias=False,input_dim=2))\n\n\nnet1.weights\n\nValueError: Weights for model sequential_2 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.\n\n\n\nnet2.weights\n\n[<tf.Variable 'dense_3/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[-1.053657 ],\n        [ 1.3536845]], dtype=float32)>]\n\n\n\nnet1.summary()\n\nValueError: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.\n\n\n\nnet2.summary()\n\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_3 (Dense)             (None, 1)                 2         \n                                                                 \n=================================================================\nTotal params: 2\nTrainable params: 2\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\n풀이3: 스칼라버전, 임의의 초기값을 설정\n(0단계) 데이터정리\n\ny=y.reshape(N,1)\nx=x.reshape(N,1)\nx.shape,y.shape\n\n(TensorShape([200, 1]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential() \n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1,input_dim=1)\n\n\nnet.add(layer)\n\n\n초기값을 설정\n\nnet.weights\n\n[<tf.Variable 'dense_4/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[0.534932]], dtype=float32)>,\n <tf.Variable 'dense_4/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n\n\n\nnet.get_weights()\n\n[array([[0.534932]], dtype=float32), array([0.], dtype=float32)]\n\n\n\nweight, bias순으로 출력\n\n\nnet.set_weights?\n\n\nSignature: net.set_weights(weights)\nDocstring:\nSets the weights of the layer, from NumPy arrays.\nThe weights of a layer represent the state of the layer. This function\nsets the weight values from numpy arrays. The weight values should be\npassed in the order they are created by the layer. Note that the layer's\nweights must be instantiated before calling this function, by calling\nthe layer.\nFor example, a `Dense` layer returns a list of two values: the kernel matrix\nand the bias vector. These can be used to set the weights of another\n`Dense` layer:\n>>> layer_a = tf.keras.layers.Dense(1,\n...   kernel_initializer=tf.constant_initializer(1.))\n>>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n>>> layer_a.get_weights()\n[array([[1.],\n       [1.],\n       [1.]], dtype=float32), array([0.], dtype=float32)]\n>>> layer_b = tf.keras.layers.Dense(1,\n...   kernel_initializer=tf.constant_initializer(2.))\n>>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n>>> layer_b.get_weights()\n[array([[2.],\n       [2.],\n       [2.]], dtype=float32), array([0.], dtype=float32)]\n>>> layer_b.set_weights(layer_a.get_weights())\n>>> layer_b.get_weights()\n[array([[1.],\n       [1.],\n       [1.]], dtype=float32), array([0.], dtype=float32)]\nArgs:\n  weights: a list of NumPy arrays. The number\n    of arrays and their shape must match\n    number of the dimensions of the weights\n    of the layer (i.e. it should match the\n    output of `get_weights`).\nRaises:\n  ValueError: If the provided weights list does not match the\n    layer's specifications.\nFile:      ~/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/base_layer.py\nType:      method\n\n\n\n\n\nlayer_b.set_weights(layer_a.get_weights()) 와 같은방식으로 쓴다는 것이군?\n\n- 한번따라해보자.\n\n_w = net.get_weights()\n_w\n\n[array([[0.534932]], dtype=float32), array([0.], dtype=float32)]\n\n\n\n길이가 2인 리스트이고, 각 원소는 numpy array 임\n\n\nnet.set_weights(\n    [np.array([[10.0]],dtype=np.float32), # weight, β1_hat\n     np.array([-5.0],dtype=np.float32)] # bias, β0_hat \n)\n\n\nnet.weights\n\n[<tf.Variable 'dense_4/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[10.]], dtype=float32)>,\n <tf.Variable 'dense_4/bias:0' shape=(1,) dtype=float32, numpy=array([-5.], dtype=float32)>]\n\n\n\n(3단계) net.compile()\n\nnet.compile(tf.keras.optimizers.SGD(0.1),tf.losses.MSE) \n\n(4단계) net.fit()\n\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N) \n\n<keras.callbacks.History at 0x7f9c642f0d30>\n\n\n결과확인\n\nnet.weights\n\n[<tf.Variable 'dense_4/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[3.933048]], dtype=float32)>,\n <tf.Variable 'dense_4/bias:0' shape=(1,) dtype=float32, numpy=array([2.58366], dtype=float32)>]\n\n\n\n\n풀이4: 벡터버전, 임의의 초기값을 설정\n(0단계) 데이터정리\n\nX.shape, y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential()\n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1,use_bias=False,input_dim=2) \n\n\nnet.add(layer)\n\n\n초기값을 설정하자\n\nnet.set_weights([np.array([[ -5.0],[10.0]], dtype=np.float32)])\n\n\nnet.get_weights()\n\n[array([[-5.],\n        [10.]], dtype=float32)]\n\n\n\n(3단계) net.compile()\n\nnet.compile(tf.keras.optimizers.SGD(0.1), tf.losses.MSE) \n\n(4단계) net.fit()\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) \n\n<keras.callbacks.History at 0x7f9c284f2ce0>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense_5/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.58366 ],\n        [3.933048]], dtype=float32)>]\n\n\n- 사실 실전에서는 초기값을 설정할 필요가 별로 없음.\n\n\n풀이5: 벡터버전 사용자정의 손실함수\n(0단계) 데이터정리\n\nX.shape, y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential()\n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1,use_bias=False) \n\n\nnet.add(layer)\n\n(3단계) net.compile()\n\nloss_fn = lambda y,yhat: (y-yhat).T @ (y-yhat) / N\n\n\nnet.compile(tf.keras.optimizers.SGD(0.1), loss_fn) \n\n(4단계) net.fit()\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) \n\n<keras.callbacks.History at 0x7f9c28350d90>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense_6/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5836723],\n        [3.9330251]], dtype=float32)>]\n\n\n\n\n풀이6: 벡터버전, net.compile의 옵션으로 손실함수 지정\n(0단계) 데이터정리\n\nX.shape, y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential()\n\n(2단계) net.add(layer)\n\nnet.add(tf.keras.layers.Dense(1,use_bias=False))\n\n(3단계) net.compile()\n\nnet.compile(tf.keras.optimizers.SGD(0.1), loss='mse') \n\n(4단계) net.fit()\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) \n\n<keras.callbacks.History at 0x7f9c2831eb90>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense_7/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5836723],\n        [3.9330251]], dtype=float32)>]\n\n\n\n\n풀이7: 벡터버전, net.compile의 옵션으로 손실함수 지정 + 옵티마이저 지정\n(0단계) 데이터정리\n\nX.shape, y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential()\n\n(2단계) net.add(layer)\n\nnet.add(tf.keras.layers.Dense(1,use_bias=False))\n\n(3단계) net.compile()\n\nnet.compile(optimizer='sgd', loss='mse') \n#net.optimizer.lr = tf.Variable(0.1,dtype=tf.float32)\n#net.optimizer.lr = 0.1\n\n(4단계) net.fit()\n\nnet.fit(X,y,epochs=5000,verbose=0,batch_size=N) \n\n<keras.callbacks.History at 0x7f9c28244b20>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense_8/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5842712],\n        [3.9319096]], dtype=float32)>]\n\n\n\n\n\n여러가지 회귀모형의 적합과 학습과정의 모니터링\n\n예제1\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 x_i\\)\n\nnp.random.seed(43052) \nN= 100 \nx= np.random.randn(N) \nepsilon = np.random.randn(N)*0.5 \ny= 2.5+4*x +epsilon\n\n\nX= np.stack([np.ones(N),x],axis=1)\ny= y.reshape(N,1)\n\n\nplt.plot(x,y,'o') # 관측한 자료 \n\n\n\n\n\nbeta_hat = np.array([-3,-2]).reshape(2,1)\n\n\nyhat = X@beta_hat \n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat.reshape(-1),'-') \n\n\n\n\n더 좋은 적합선을 얻기위해서!\n\nslope = (2*X.T@X@beta_hat - 2*X.T@y)/ N \nbeta_hat2 = beta_hat - 0.1*slope  \nyhat2 = X@beta_hat2\n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat.reshape(-1),'-') \nplt.plot(x,yhat2.reshape(-1),'-') \n\n\n\n\n초록색이 좀 더 나아보인다.\n\nbeta_hat = np.array([-3,-2]).reshape(2,1) \nbeta_hats = beta_hat # beta_hats = beta_hat.copy() 가 더 안전한 코드입니다. \nfor i in range(1,30):\n    yhat = X@beta_hat \n    slope = (2*X.T@X@beta_hat - 2*X.T@y) / N \n    beta_hat = beta_hat - 1.0*slope # 0.1은 적당, 0.3은 쪼금빠르지만 그래도 적당, 0.9는 너무 나간것같음, 1.0 은 수렴안함, 1.2 \n    beta_hats = np.concatenate([beta_hats,beta_hat],axis=1) \n\n\nbeta_hats\n\narray([[-3.        ,  7.12238255, -1.2575366 ,  5.73166742, -0.1555309 ,\n         4.86767499,  0.51106397,  4.36611576,  0.87316777,  4.12348617,\n         1.01165173,  4.07771926,  0.97282343,  4.19586617,  0.77814101,\n         4.46653491,  0.4299822 ,  4.89562729, -0.08537358,  5.50446319,\n        -0.79684366,  6.32975688, -1.74933031,  7.42517729, -3.00603683,\n         8.86442507, -4.6523303 , 10.74592463, -6.80132547, 13.19938129],\n       [-2.        ,  8.70824998,  0.16165717,  6.93399596,  1.62435964,\n         5.72089586,  2.63858056,  4.86387722,  3.37280529,  4.22385379,\n         3.94259478,  3.70397678,  4.43004465,  3.23363047,  4.89701606,\n         2.75741782,  5.39439054,  2.22728903,  5.96886945,  1.59655409,\n         6.66836857,  0.81489407,  7.54676324, -0.17628423,  8.66856437,\n        -1.44867655, 10.11401544, -3.09256176, 11.98507323, -5.22340389]])\n\n\n\nb0hats = beta_hats[0].tolist()\nb1hats = beta_hats[1].tolist()\n\n\nnp.linalg.inv(X.T@X) @ X.T @ y\n\narray([[2.5451404 ],\n       [3.94818596]])\n\n\n\nfrom matplotlib import animation \nplt.rcParams[\"animation.html\"] = \"jshtml\" \n\n\nfig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12)\n\n<Figure size 864x360 with 0 Axes>\n\n\n\nax1= fig.add_subplot(1,2,1)\nax2= fig.add_subplot(1,2,2,projection='3d')\n# ax1: 왼쪽그림 \nax1.plot(x,y,'o')\nline, = ax1.plot(x,b0hats[0] + b1hats[0]*x) \n# ax2: 오른쪽그림 \nβ0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing='ij')\nβ0=β0.reshape(-1)\nβ1=β1.reshape(-1)\nloss_fn = lambda b0,b1: np.sum((y-b0-b1*x)**2)\nloss = list(map(loss_fn, β0,β1))\nax2.scatter(β0,β1,loss,alpha=0.02) \nax2.scatter(2.5451404,3.94818596,loss_fn(2.5451404,3.94818596),s=200,marker='*') \n\ndef animate(i):\n    line.set_ydata(b0hats[i] + b1hats[i]*x) \n    ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=\"grey\") \n\nani = animation.FuncAnimation(fig,animate,frames=30) \nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n예제2\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 e^{-x_i}\\)\n\nnp.random.seed(43052) \nN= 100 \nx= np.linspace(-1,1,N)\nepsilon = np.random.randn(N)*0.5 \ny= 2.5+4*np.exp(-x) +epsilon\n\n\nplt.plot(x,y,'o')\n\n\n\n\n\nX= np.stack([np.ones(N),np.exp(-x)],axis=1)\ny= y.reshape(N,1)\n\n\nbeta_hat = np.array([-3,-2]).reshape(2,1)\nbeta_hats = beta_hat.copy() # shallow copy, deep copy <--- 여름 방학 특강 \nfor i in range(1,30): \n    yhat = X@beta_hat \n    slope = (2*X.T@X@beta_hat - 2*X.T@y) /N \n    beta_hat = beta_hat - 0.05*slope \n    beta_hats = np.concatenate([beta_hats,beta_hat],axis=1) \n\n\nbeta_hats\n\narray([[-3.        , -1.74671631, -0.82428979, -0.14453919,  0.35720029,\n         0.72834869,  1.0036803 ,  1.20869624,  1.36209751,  1.47759851,\n         1.56525696,  1.63244908,  1.68458472,  1.72563174,  1.75850062,\n         1.78532638,  1.80767543,  1.82669717,  1.84323521,  1.85790889,\n         1.8711731 ,  1.88336212,  1.89472176,  1.90543297,  1.91562909,\n         1.92540859,  1.93484428,  1.94399023,  1.9528867 ,  1.96156382],\n       [-2.        , -0.25663415,  1.01939241,  1.95275596,  2.63488171,\n         3.13281171,  3.49570765,  3.75961951,  3.95098231,  4.08918044,\n         4.18842797,  4.2591476 ,  4.30898175,  4.34353413,  4.36691339,\n         4.38213187,  4.39139801,  4.39633075,  4.39811673,  4.3976256 ,\n         4.3954946 ,  4.3921905 ,  4.38805511,  4.3833386 ,  4.37822393,\n         4.37284482,  4.36729887,  4.36165718,  4.35597148,  4.35027923]])\n\n\n\nb0hats= beta_hats[0].tolist()\nb1hats= beta_hats[1].tolist()\n\n\nnp.linalg.inv(X.T@X)@X.T@y\n\narray([[2.46307644],\n       [3.99681332]])\n\n\n\nfig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12)\n\n<Figure size 864x360 with 0 Axes>\n\n\n\nax1= fig.add_subplot(1,2,1)\nax2= fig.add_subplot(1,2,2,projection='3d')\n# ax1: 왼쪽그림 \nax1.plot(x,y,'o')\nline, = ax1.plot(x,b0hats[0] + b1hats[0]*np.exp(-x))\n# ax2: 오른쪽그림 \nβ0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing='ij')\nβ0=β0.reshape(-1)\nβ1=β1.reshape(-1)\nloss_fn = lambda b0,b1: np.sum((y-b0-b1*np.exp(-x))**2)\nloss = list(map(loss_fn, β0,β1))\nax2.scatter(β0,β1,loss,alpha=0.02) \nax2.scatter(2.46307644,3.99681332,loss_fn(2.46307644,3.99681332),s=200,marker='*') \n\ndef animate(i):\n    line.set_ydata(b0hats[i] + b1hats[i]*np.exp(-x))\n    ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=\"grey\") \n\nani = animation.FuncAnimation(fig,animate,frames=30) \nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n예제3\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 e^{-x_i} + \\beta_2 \\cos(5x_i)\\)\n\nnp.random.seed(43052) \nN= 100 \nx= np.linspace(-1,1,N)\nepsilon = np.random.randn(N)*0.5 \ny= 2.5+4*np.exp(-x) + 5*np.cos(5*x) + epsilon\n\n\nplt.plot(x,y,'o')\n\n\n\n\n\nX=np.stack([np.ones(N),np.exp(-x),np.cos(5*x)],axis=1) \ny=y.reshape(N,1)\n\n\nbeta_hat = np.array([-3,-2,-1]).reshape(3,1) \nbeta_hats = beta_hat.copy()\nfor i in range(1,30):\n    yhat = X@beta_hat \n    slope = (2*X.T@X@beta_hat -2*X.T@y) /N \n    beta_hat = beta_hat - 0.1 * slope \n    beta_hats= np.concatenate([beta_hats,beta_hat],axis=1)\n\n\nbeta_hats\n\narray([[-3.        , -0.71767532,  0.36255782,  0.89072137,  1.16423101,\n         1.31925078,  1.41819551,  1.48974454,  1.54713983,  1.59655416,\n         1.64091846,  1.68167278,  1.71956758,  1.75503084,  1.78833646,\n         1.81968188,  1.84922398,  1.877096  ,  1.90341567,  1.92828934,\n         1.95181415,  1.97407943,  1.99516755,  2.01515463,  2.0341111 ,\n         2.05210214,  2.06918818,  2.08542523,  2.10086524,  2.11555643],\n       [-2.        ,  1.16947474,  2.64116513,  3.33411605,  3.66880042,\n         3.83768856,  3.92897389,  3.98315095,  4.01888831,  4.04486085,\n         4.06516144,  4.08177665,  4.09571971,  4.10754954,  4.1176088 ,\n         4.12613352,  4.13330391,  4.13926816,  4.14415391,  4.14807403,\n         4.15112966,  4.1534121 ,  4.15500404,  4.15598045,  4.15640936,\n         4.15635249,  4.15586584,  4.15500014,  4.15380139,  4.1523112 ],\n       [-1.        , -0.95492718, -0.66119313, -0.27681968,  0.12788212,\n         0.52254445,  0.89491388,  1.24088224,  1.55993978,  1.85310654,\n         2.12199631,  2.36839745,  2.59408948,  2.8007666 ,  2.99000967,\n         3.16327964,  3.32192026,  3.46716468,  3.60014318,  3.72189116,\n         3.83335689,  3.93540864,  4.02884144,  4.11438316,  4.19270026,\n         4.26440288,  4.33004965,  4.39015202,  4.44517824,  4.49555703]])\n\n\n\nb0hats,b1hats,b2hats = beta_hats\n\n\nnp.linalg.inv(X.T@X) @ X.T @ y\n\narray([[2.46597526],\n       [4.00095138],\n       [5.04161877]])\n\n\n\nfig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12)\n\n<Figure size 864x360 with 0 Axes>\n\n\n\nax1= fig.add_subplot(1,2,1)\nax2= fig.add_subplot(1,2,2,projection='3d')\n# ax1: 왼쪽그림 \nax1.plot(x,y,'o')\nline, = ax1.plot(x,b0hats[0] + b1hats[0]*np.exp(-x) + b2hats[0]*np.cos(5*x))\n# ax2: 오른쪽그림 \n# β0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing='ij')\n# β0=β0.reshape(-1)\n# β1=β1.reshape(-1)\n# loss_fn = lambda b0,b1: np.sum((y-b0-b1*np.exp(-x))**2)\n# loss = list(map(loss_fn, β0,β1))\n# ax2.scatter(β0,β1,loss,alpha=0.02) \n# ax2.scatter(2.46307644,3.99681332,loss_fn(2.46307644,3.99681332),s=200,marker='*') \n\ndef animate(i):\n    line.set_ydata(b0hats[i] + b1hats[i]*np.exp(-x) + b2hats[i]*np.cos(5*x))\n    # ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=\"grey\") \n\nani = animation.FuncAnimation(fig,animate,frames=30) \nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n예제3: 케라스로 해보자!\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 e^{-x_i} + \\beta_2 \\cos(5x_i)\\)\n\nnp.random.seed(43052) \nN= 100 \nx= np.linspace(-1,1,N)\nepsilon = np.random.randn(N)*0.5 \ny= 2.5+4*np.exp(-x) + 5*np.cos(5*x) + epsilon\n\n\nX=np.stack([np.ones(N),np.exp(-x),np.cos(5*x)],axis=1) \ny=y.reshape(N,1)\n\n\nnet = tf.keras.Sequential() # 1: 네트워크 생성\nnet.add(tf.keras.layers.Dense(1,use_bias=False)) # 2: add layer \nnet.compile(tf.optimizers.SGD(0.1), loss='mse') # 3: compile\nnet.fit(X,y,epochs=30, batch_size=N) # 4: fit \n\nEpoch 1/30\n1/1 [==============================] - 0s 75ms/step - loss: 82.1027\nEpoch 2/30\n1/1 [==============================] - 0s 1ms/step - loss: 23.9512\nEpoch 3/30\n1/1 [==============================] - 0s 1ms/step - loss: 10.7256\nEpoch 4/30\n1/1 [==============================] - 0s 1ms/step - loss: 7.0664\nEpoch 5/30\n1/1 [==============================] - 0s 1ms/step - loss: 5.5521\nEpoch 6/30\n1/1 [==============================] - 0s 1ms/step - loss: 4.6075\nEpoch 7/30\n1/1 [==============================] - 0s 2ms/step - loss: 3.8836\nEpoch 8/30\n1/1 [==============================] - 0s 1ms/step - loss: 3.2909\nEpoch 9/30\n1/1 [==============================] - 0s 2ms/step - loss: 2.7971\nEpoch 10/30\n1/1 [==============================] - 0s 1ms/step - loss: 2.3838\nEpoch 11/30\n1/1 [==============================] - 0s 1ms/step - loss: 2.0374\nEpoch 12/30\n1/1 [==============================] - 0s 1ms/step - loss: 1.7471\nEpoch 13/30\n1/1 [==============================] - 0s 1ms/step - loss: 1.5038\nEpoch 14/30\n1/1 [==============================] - 0s 985us/step - loss: 1.2998\nEpoch 15/30\n1/1 [==============================] - 0s 1ms/step - loss: 1.1288\nEpoch 16/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.9854\nEpoch 17/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.8652\nEpoch 18/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.7645\nEpoch 19/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.6800\nEpoch 20/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.6092\nEpoch 21/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.5499\nEpoch 22/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.5001\nEpoch 23/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.4584\nEpoch 24/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.4234\nEpoch 25/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.3941\nEpoch 26/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.3695\nEpoch 27/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.3489\nEpoch 28/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.3316\nEpoch 29/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.3171\nEpoch 30/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.3050\n\n\n<keras.callbacks.History at 0x7f9c06753a60>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense_9/kernel:0' shape=(3, 1) dtype=float32, numpy=\n array([[2.4857023],\n        [3.925291 ],\n        [4.6923084]], dtype=float32)>]\n\n\n\nplt.plot(x,y,'o') \nplt.plot(x,(X@net.weights).reshape(-1),'--')\n\n\n\n\n\n\n\n숙제\n\n예제2: 케라스를 이용하여 아래를 만족하는 적절한 \\(\\beta_0\\)와 \\(\\beta_1\\)을 구하라. 적합결과를 시각화하라. (애니메이션 시각화 X)\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 e^{-x_i}\\)\n\nnp.random.seed(43052) \nN= 100 \nx= np.linspace(-1,1,N)\nepsilon = np.random.randn(N)*0.5 \ny= 2.5+4*np.exp(-x) +epsilon\n\n\nX = np.stack([np.ones(N),np.exp(-x)],axis=1)\ny = y.reshape(N,1)\n\n\nnet = tf.keras.Sequential() # 1: 네트워크 생성\nnet.add(tf.keras.layers.Dense(1,use_bias=False)) # 2: add layer \nnet.compile(tf.optimizers.SGD(0.1), loss='mse') # 3: compile\nnet.fit(X,y,epochs=30, batch_size=N) # 4: fit \n\nEpoch 1/30\n1/1 [==============================] - 0s 76ms/step - loss: 64.1211\nEpoch 2/30\n1/1 [==============================] - 0s 1ms/step - loss: 14.2919\nEpoch 3/30\n1/1 [==============================] - 0s 903us/step - loss: 3.4335\nEpoch 4/30\n1/1 [==============================] - 0s 910us/step - loss: 1.0607\nEpoch 5/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.5360\nEpoch 6/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.4142\nEpoch 7/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.3806\nEpoch 8/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.3667\nEpoch 9/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.3575\nEpoch 10/30\n1/1 [==============================] - 0s 2ms/step - loss: 0.3497\nEpoch 11/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.3427\nEpoch 12/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.3361\nEpoch 13/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.3300\nEpoch 14/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.3242\nEpoch 15/30\n1/1 [==============================] - 0s 991us/step - loss: 0.3189\nEpoch 16/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.3139\nEpoch 17/30\n1/1 [==============================] - 0s 991us/step - loss: 0.3092\nEpoch 18/30\n1/1 [==============================] - 0s 980us/step - loss: 0.3048\nEpoch 19/30\n1/1 [==============================] - 0s 998us/step - loss: 0.3007\nEpoch 20/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.2969\nEpoch 21/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.2933\nEpoch 22/30\n1/1 [==============================] - 0s 946us/step - loss: 0.2900\nEpoch 23/30\n1/1 [==============================] - 0s 968us/step - loss: 0.2869\nEpoch 24/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.2840\nEpoch 25/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.2813\nEpoch 26/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.2787\nEpoch 27/30\n1/1 [==============================] - 0s 979us/step - loss: 0.2763\nEpoch 28/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.2741\nEpoch 29/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.2720\nEpoch 30/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.2701\n\n\n<keras.callbacks.History at 0x7f9c0665e590>"
  },
  {
    "objectID": "posts/2022-05-09-10주차_빅데이터분석특강.html",
    "href": "posts/2022-05-09-10주차_빅데이터분석특강.html",
    "title": "jisim12",
    "section": "",
    "text": "빅데이터분석특강\n\n\ntoc:false\nbranch: master\nbadges: true\ncomments: true\nauthor: 심재인\n\n\n\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow.experimental.numpy as tnp\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\ntf.config.experimental.list_physical_devices()\n\n2022-06-05 17:49:35.436657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+ s + ';}')\n\n\n\n\n\n\n- 기본버전은 아래와 같다 \\[y_i \\approx \\text{sigmoid}(b + w_1 x_{1,i} + \\dots + w_{784}x_{784,i})= \\frac{\\exp(b + w_1 x_{1,i} + \\dots + w_{784}x_{784,i})}{1+\\exp(b + w_1 x_{1,i} + \\dots + w_{784}x_{784,i})}\\]\n- 벡터버전은 아래와 같다. \\[{\\boldsymbol y} \\approx \\text{sigmoid}({\\bf X}{\\bf W} + b) = \\frac{\\exp({\\bf XW} +b)}{1+\\exp({\\bf XW} +b)}\\]\n- 벡터버전에 익숙해지도록 하자. 벡터버전에 사용된 차원 및 연산을 정리하면 아래와 같다.\n\n\\({\\bf X}\\): (n,784) matrix\n\\({\\boldsymbol y}\\): (n,1) matrix\n\\({\\bf W}\\): (784,1) matrix\n\\(b\\): (1,1) matrix\n+, exp 는 브로드캐스팅\n\n\n\n\n- \\(y_i=0 \\text{ or } 1\\) 대신에 \\(\\boldsymbol{y}_i=[y_{i1},y_{i2}]= [1,0] \\text { or } [0,1]\\)와 같이 코딩하면 어떠할까? (즉 원핫인코딩을 한다면?)\n- 활성화 함수를 취하기 전의 버전은 아래와 같이 볼 수 있다.\n\\[[{\\boldsymbol y}_1 ~ {\\boldsymbol y}_2] \\propto  [ {\\bf X}{\\bf W}_1  ~ {\\bf X}{\\bf W}_2] + [b_1 ~ b_2]= {\\bf X} [{\\bf W}_1 {\\bf W}_2] + [b_1 ~ b_2]= {\\bf X}{\\bf W} + {\\boldsymbol b}\\]\n여기에서 매트릭스 및 연산의 차원을 정리하면 아래와 같다.\n\n\\({\\bf X}\\): (n,784) matrix\n\\({\\boldsymbol y}_1,{\\boldsymbol y}_2\\): (n,1) matrix\n\\({\\boldsymbol y}:=[{\\boldsymbol y}_1~ {\\boldsymbol y}_2]\\): (n,2) matrix\n\\({\\bf W}_1\\), \\({\\bf W}_2\\): (784,1) matrix\n\\({\\bf W}:=[{\\bf W}_1~ {\\bf W}_2]\\): (784,2) matrix\n\\(b_1,b_2\\): (1,1) matrix\n$:= [b_1 ~b_2] $: (1,2) matrix\n+ 는 브로드캐스팅\n\n- 즉 로지스틱 모형 (1)의 형태를 겹쳐놓은 형태로 해석할 수 있음. 따라서 \\({\\bf X} {\\bf W}_1 + b_1\\)와 \\({\\bf X} {\\bf W}_2 + b_2\\)의 row값이 클수록 \\({\\boldsymbol y}_1\\)와 \\({\\boldsymbol y}_2\\)의 row값이 1이어야 함\n\n\\({\\boldsymbol y}_1 \\propto {\\bf X} {\\bf W}_1 + b_1\\) \\(\\to\\) \\({\\bf X} {\\bf W}_1 + b_1\\)의 row값이 클수록 \\(\\boldsymbol{y}_1\\)의 row 값이 1이라면 모형계수를 잘 추정한것\n\\({\\boldsymbol y}_2 \\propto {\\bf X} {\\bf W}_2 + b_2\\) \\(\\to\\) \\({\\bf X} {\\bf W}_2 + b_2\\)의 row값이 클수록 \\(\\boldsymbol{y}_2\\)의 row 값을 1이라면 모형계수를 잘 추정한것\n\n- (문제) \\({\\bf X}{\\bf W}_1 +b_1\\)의 값이 500, \\({\\bf X}{\\bf W}_2 +b_2\\)의 값이 200 인 row가 있다고 하자. 대응하는 \\(\\boldsymbol{y}_1, \\boldsymbol{y}_2\\)의 row값은 얼마로 적합되어야 하는가?\n\n\\([0,0]\\)\n\\([0,1]\\)\n\\([1,0]\\) <– 이게 답이다!\n\\([1,1]\\)\n\n\nnote: 둘다 0 혹은 둘다 1로 적합할수는 없으니까 (1), (4)는 제외한다. \\({\\bf X}{\\bf W}_1 +b_1\\)의 값이 \\({\\bf X}{\\bf W}_2 +b_2\\)의 값보다 크므로 (3)번이 합리적임\n\n- 목표: 위와 같은 문제의 답을 유도해주는 활성화함수를 설계하자. 즉 합리적인 \\(\\hat{\\boldsymbol{y}}_1,\\hat{\\boldsymbol{y}}_2\\)를 구해주는 활성화 함수를 설계해보자. 이를 위해서는 아래의 사항들이 충족되어야 한다.\n\n\\(\\hat{\\boldsymbol{y}}_1\\), \\(\\hat{\\boldsymbol{y}}_2\\)의 각 원소는 0보다 크고 1보다 작아야 한다. (확률을 의미해야 하니까)\n\\(\\hat{\\boldsymbol{y}}_1+\\hat{\\boldsymbol{y}}_2={\\bf 1}\\) 이어야 한다. (확률의 총합은 1이니까!)\n\\(\\hat{\\boldsymbol{y}}_1\\)와 \\(\\hat{\\boldsymbol{y}}_2\\)를 각각 따로해석하면 로지스틱처럼 되면 좋겠다.\n\n- 아래와 같은 활성화 함수를 도입하면 어떨까?\n\\[\\hat{\\boldsymbol{y}}=[\\hat{\\boldsymbol y}_1 ~ \\hat{\\boldsymbol y}_2] =  \\big[ \\frac{\\exp({\\bf X}\\hat{\\bf W}_1+\\hat{b}_1)}{\\exp({\\bf X}\\hat{\\bf W}_1+\\hat{b}_1)+\\exp({\\bf X}\\hat{\\bf W}_2+\\hat{b}_2)}  ~~ \\frac{\\exp({\\bf X}\\hat{\\bf W}_2+\\hat{b}_2)}{\\exp({\\bf X}\\hat{\\bf W}_1+\\hat{b}_1)+\\exp({\\bf X}\\hat{\\bf W}_2+\\hat{b}_2)}  \\big]\\]\n- (1),(2)는 만족하는 듯 하다. (3)은 바로 이해되지는 않는다\n\n\\(\\hat{\\boldsymbol{y}}_1\\), \\(\\hat{\\boldsymbol{y}}_2\\)의 각 원소는 0보다 크고 1보다 작아야 한다. –> OK!\n\\(\\hat{\\boldsymbol{y}}_1+\\hat{\\boldsymbol{y}}_2={\\bf 1}\\) 이어야 한다. –> OK!\n\\(\\hat{\\boldsymbol{y}}_1\\)와 \\(\\hat{\\boldsymbol{y}}_2\\)를 각각 따로해석하면 로지스틱처럼 되면 좋겠다. –> ???\n\n- 그런데 조금 따져보면 (3)도 만족된다는 것을 알 수 있다. (sigmoid, softmax Section 참고)\n- 위와 같은 함수를 softmax라고 하자. 즉 아래와 같이 정의하자. \\[\n\\hat{\\boldsymbol y} = \\text{softmax}({\\bf X}\\hat{\\bf W} + {\\boldsymbol b})\n= \\big[ \\frac{\\exp({\\bf X}\\hat{\\bf W}_1+\\hat{b}_1)}{\\exp({\\bf X}\\hat{\\bf W}_1+\\hat{b}_1)+\\exp({\\bf X}\\hat{\\bf W}_2+\\hat{b}_2)}  ~~ \\frac{\\exp({\\bf X}\\hat{\\bf W}_2+\\hat{b}_2)}{\\exp({\\bf X}\\hat{\\bf W}_1+\\hat{b}_1)+\\exp({\\bf X}\\hat{\\bf W}_2+\\hat{b}_2)}  \\big]\n\\]\n\n\n\n\n\n- 아래의 수식을 관찰하자. \\[\\frac{\\exp(\\beta_0+\\beta_1 x_i)}{1+\\exp(\\beta_0+\\beta_1x_i)}=\\frac{\\exp(\\beta_0+\\beta_1 x_i)}{e^0+\\exp(\\beta_0+\\beta_1x_i)}\\]\n- 1을 \\(e^0\\)로 해석하면 모형2의 해석을 아래와 같이 모형1의 해석으로 적용할수 있다. - 모형2: \\({\\bf X}\\hat{\\bf W}_1 +\\hat{b}_1\\) 와 \\({\\bf X}\\hat{\\bf W}_2 +\\hat{b}_2\\) 의 크기를 비교하고 확률 결정 - 모형1: \\({\\bf X}\\hat{\\bf W} +\\hat{b}\\) 와 \\(0\\)의 크기를 비교하고 확률 결정 = \\({\\bf X}\\hat{\\bf W} +\\hat{b}\\)의 row값이 양수이면 1로 예측하고 음수이면 0으로 예측\n- 이항분포를 차원이 2인 다항분포로 해석가능한 것처럼 sigmoid는 차원이 2인 softmax로 해석가능하다. 즉 다항분포가 이항분포의 확장형으로 해석가능한 것처럼 softmax도 sigmoid의 확장형으로 해석가능하다.\n\n\n\n- 언뜻 생각하면 클래스가 2인 경우에도 sigmoid 대신 softmax로 활성화함수를 이용해도 될 듯 하다. 즉 \\(y=0 \\text{ or } 1\\)와 같이 정리하지 않고 \\(y=[0,1] \\text{ or } [1,0]\\) 와 같이 정리해도 무방할 듯 하다.\n- 하지만 sigmoid가 좀 더 좋은 선택이다. 즉 \\(y= 0 \\text{ or } 1\\)로 데이터를 정리하는 것이 더 좋은 선택이다. 왜냐하면 sigmoid는 softmax와 비교하여 파라메터의 수가 적지만 표현력은 동등하기 때문이다.\n- 표현력이 동등한 이유? 아래 수식을 관찰하자. \\[\\big(\\frac{e^{300}}{e^{300}+e^{500}},\\frac{e^{500}}{e^{300}+e^{500}}\\big) =\\big( \\frac{e^{0}}{e^{0}+e^{200}}, \\frac{e^{200}}{e^{0}+e^{200}}\\big)\\]\n\n\\(\\big(\\frac{e^{300}}{e^{300}+e^{500}},\\frac{e^{500}}{e^{300}+e^{500}}\\big)\\)를 표현하기 위해서 300, 500 이라는 2개의 숫자가 필요한것이 아니고 따지고보면 200이라는 하나의 숫자만 필요하다.\n\\((\\hat{\\boldsymbol{y}}_1,\\hat{\\boldsymbol{y}}_2)\\)의 표현에서도 \\({\\bf X}\\hat{\\bf W}_1 +\\hat{b}_1\\) 와 \\({\\bf X}\\hat{\\bf W}_2 +\\hat{b}_2\\) 라는 숫자 각각이 필요한 것이 아니고 \\(({\\bf X}\\hat{\\bf W}_1 +\\hat{b}_1)-({\\bf X}\\hat{\\bf W}_2 +\\hat{b}_2)\\)의 값만 알면 된다.\n\n- 클래스의 수가 2개일 경우는 softmax가 sigmoid에 비하여 장점이 없다. 하지만 softmax는 클래스의 수가 3개 이상일 경우로 쉽게 확장할 수 있다는 점에서 매력적인 활성화 함수이다.\n\n\n\n\n- y의 모양: [0 1 0 0 0 0 0 0 0 0]\n- 활성화함수의 선택: softmax\n- 손실함수의 선택: cross entropy\n\n\n\n- 데이터정리\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\n\nX= x_train.reshape(-1,784)\ny= tf.keras.utils.to_categorical(y_train)\nXX = x_test.reshape(-1,784)\nyy = tf.keras.utils.to_categorical(y_test)\n\n- 시도1: 간단한 신경망\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x1\"\n    \"x2\"\n    \"..\"\n    \"x784\"\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x1\" -> \"node1\"\n    \"x2\" -> \"node1\"\n    \"..\" -> \"node1\"\n    \n    \"x784\" -> \"node1\"\n    \"x1\" -> \"node2\"\n    \"x2\" -> \"node2\"\n    \"..\" -> \"node2\"\n    \"x784\" -> \"node2\"\n    \n    \"x1\" -> \"...\"\n    \"x2\" -> \"...\"\n    \"..\" -> \"...\"\n    \"x784\" -> \"...\"\n\n    \"x1\" -> \"node30\"\n    \"x2\" -> \"node30\"\n    \"..\" -> \"node30\"\n    \"x784\" -> \"node30\"\n\n\n    label = \"Layer 1: relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n\n    \"node1\" -> \"y10\"\n    \"node2\" -> \"y10\"\n    \"...\" -> \"y10\"\n    \"node30\" -> \"y10\"\n    \n    \"node1\" -> \"y1\"\n    \"node2\" -> \"y1\"\n    \"...\" -> \"y1\"\n    \"node30\" -> \"y1\"\n    \n    \"node1\" -> \".\"\n    \"node2\" -> \".\"\n    \"...\" -> \".\"\n    \"node30\" -> \".\"\n    \n    label = \"Layer 2: softmax\"\n}\n''')\n\n\n\n\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(30,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(10,activation = 'softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy'])\nnet.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 2s 964us/step - loss: 2.5465 - accuracy: 0.3079\nEpoch 2/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 1.4875 - accuracy: 0.4092\nEpoch 3/5\n1875/1875 [==============================] - 2s 979us/step - loss: 1.2439 - accuracy: 0.4880\nEpoch 4/5\n1875/1875 [==============================] - 2s 890us/step - loss: 1.0470 - accuracy: 0.5763\nEpoch 5/5\n1875/1875 [==============================] - 2s 890us/step - loss: 0.9293 - accuracy: 0.6183\n\n\n<keras.callbacks.History at 0x7fb9bc0c0c40>\n\n\n\nnet.evaluate(XX,yy)\n\n313/313 [==============================] - 1s 2ms/step - loss: 0.9433 - accuracy: 0.6067\n\n\n[0.9433093070983887, 0.6067000031471252]\n\n\n\nnet.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 30)                23550     \n                                                                 \n dense_1 (Dense)             (None, 10)                310       \n                                                                 \n=================================================================\nTotal params: 23,860\nTrainable params: 23,860\nNon-trainable params: 0\n_________________________________________________________________\n\n\n- 시도2: 더 깊은 신경망\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x1\"\n    \"x2\"\n    \"..\"\n    \"x784\"\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x1\" -> \"node1\"\n    \"x2\" -> \"node1\"\n    \"..\" -> \"node1\"\n    \n    \"x784\" -> \"node1\"\n    \"x1\" -> \"node2\"\n    \"x2\" -> \"node2\"\n    \"..\" -> \"node2\"\n    \"x784\" -> \"node2\"\n    \n    \"x1\" -> \"...\"\n    \"x2\" -> \"...\"\n    \"..\" -> \"...\"\n    \"x784\" -> \"...\"\n\n    \"x1\" -> \"node500\"\n    \"x2\" -> \"node500\"\n    \"..\" -> \"node500\"\n    \"x784\" -> \"node500\"\n\n\n    label = \"Layer 1: relu\"\n}\n\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n\n    \"node1\" -> \"node1(2)\"\n    \"node2\" -> \"node1(2)\"\n    \"...\" -> \"node1(2)\"\n    \"node500\" -> \"node1(2)\"\n\n    \"node1\" -> \"node2(2)\"\n    \"node2\" -> \"node2(2)\"\n    \"...\" -> \"node2(2)\"\n    \"node500\" -> \"node2(2)\"\n    \n    \"node1\" -> \"....\"\n    \"node2\" -> \"....\"\n    \"...\" -> \"....\"\n    \"node500\" -> \"....\"\n    \n    \"node1\" -> \"node500(2)\"\n    \"node2\" -> \"node500(2)\"\n    \"...\" -> \"node500(2)\"\n    \"node500\" -> \"node500(2)\"\n\n    \n    label = \"Layer 2: relu\"\n}\n\nsubgraph cluster_4{\n    style=filled;\n    color=lightgrey;\n\n    \"node1(2)\" -> \"y10\"\n    \"node2(2)\" -> \"y10\"\n    \"....\" -> \"y10\"\n    \"node500(2)\" -> \"y10\"\n    \n    \"node1(2)\" -> \"y1\"\n    \"node2(2)\" -> \"y1\"\n    \"....\" -> \"y1\"\n    \"node500(2)\" -> \"y1\"\n    \n    \"node1(2)\" -> \".\"\n    \"node2(2)\" -> \".\"\n    \"....\" -> \".\"\n    \"node500(2)\" -> \".\"\n    \n    label = \"Layer 3: softmax\"\n}\n''')\n\n\n\n\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(10,activation = 'softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy'])\nnet.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 2s 1ms/step - loss: 2.1316 - accuracy: 0.7357\nEpoch 2/5\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.6888 - accuracy: 0.7796\nEpoch 3/5\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.5565 - accuracy: 0.8109\nEpoch 4/5\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4906 - accuracy: 0.8257\nEpoch 5/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.4652 - accuracy: 0.8347\n\n\n<keras.callbacks.History at 0x7fb9647ee500>\n\n\n\nnet.evaluate(XX,yy)\n\n313/313 [==============================] - 1s 2ms/step - loss: 0.4968 - accuracy: 0.8343\n\n\n[0.4968084394931793, 0.8342999815940857]\n\n\n\nnet.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_2 (Dense)             (None, 500)               392500    \n                                                                 \n dense_3 (Dense)             (None, 500)               250500    \n                                                                 \n dense_4 (Dense)             (None, 10)                5010      \n                                                                 \n=================================================================\nTotal params: 648,010\nTrainable params: 648,010\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\n\n\n\n\n- 의문: 왜 다양한 평가지표가 필요한가? (accuray면 끝나는거 아닌가? 더 이상 뭐가 필요해?)\n- 여러가지 평가지표들: https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values - 이걸 다 암기하는건 불가능함. - 몇 개만 뽑아서 암기하고 왜 쓰는지만 생각해보고 넘어가자!\n\n\n\n- 표1\n\n\n\n\n퇴사(예측)\n안나감(예측)\n\n\n\n\n퇴사(실제)\nTP\nFN\n\n\n안나감(실제)\nFP\nTN\n\n\n\n- 표2 (책에없음)\n\n\n\n\n퇴사(예측)\n안나감(예측)\n\n\n\n\n퇴사(실제)\n$(y,)= $ (O,O)\n$(y,)= $(O,X)\n\n\n안나감(실제)\n$(y,)= $(X,O)\n$(y,)= $(X,X)\n\n\n\n- 표3 (책에없음)\n\n\n\n\n퇴사(예측)\n안나감(예측)\n\n\n\n\n퇴사(실제)\nTP, \\(\\# O/O\\)\nFN, \\(\\#O/X\\)\n\n\n안나감(실제)\nFP, \\(\\#X/O\\)\nTN, \\(\\#X/X\\)\n\n\n\n\n암기법, (1) 두번째 글자를 그대로 쓴다 (2) 첫글자가 T이면 분류를 제대로한것, 첫글자가 F이면 분류를 잘못한것\n\n- 표4 (위키등에 있음)\n\n\n\n\n\n\n\n\n\n\n퇴사(예측)\n안나감(예측)\n\n\n\n\n\n퇴사(실제)\nTP, \\(\\# O/O\\)\nFN, \\(\\# O/X\\)\nSensitivity(민감도)=Recall(재현율)=\\(\\frac{TP}{TP+FN}\\)=\\(\\frac{\\#O/O}{\\# O/O+ \\#O/X}\\)\n\n\n안나감(실제)\nFP, \\(\\# X/O\\)\nTN, \\(\\# X/X\\)\n\n\n\n\nPrecision(프리시즌)=\\(\\frac{TP}{TP+FP}\\)=\\(\\frac{\\# O/O}{\\# O/O+\\# X/O}\\)\n\nAccuracy(애큐러시)=\\(\\frac{TP+TN}{total}\\)=\\(\\frac{\\#O/O+\\# X/X}{total}\\)\n\n\n\n\n\n\n- 최규빈은 입사하여 “퇴사자 예측시스템”의 개발에 들어갔다.\n- 자료의 특성상 대부분의 사람이 퇴사하지 않고 회사에 잘 다닌다. 즉 1000명이 있으면 10명정도 퇴사한다.\n\n\n\n- 정의: Accuracy(애큐러시)=\\(\\frac{TP+TN}{total}\\)=\\(\\frac{\\#O/O+ \\#X/X}{total}\\) - 한국말로는 정확도, 정분류율이라고 한다. - 한국말이 헷갈리므로 그냥 영어를 외우는게 좋다. (어차피 Keras에서 옵션도 영어로 넣음)\n- (상확극 시점1) 왜 애큐러시는 불충분한가? - 회사: 퇴사자예측프로그램 개발해 - 최규빈: 귀찮은데 다 안나간다고 하자! -> 99퍼의 accuracy\n\n모델에 사용한 파라메터 = 0. 그런데 애큐러시 = 99! 이거 엄청 좋은 모형이다?\n\n\n\n\n- 정의: Sensitivity(민감도)=Recall(재현율)=\\(\\frac{TP}{TP+FN}\\)=\\(\\frac{\\# O/O}{\\# O/O+\\# O/X}\\) - 분모: 실제 O인 관측치 수 - 분자: 실제 O를 O라고 예측한 관측치 수 - 뜻: 실제 O를 O라고 예측한 비율\n- (상황극 시점2) recall을 봐야하는 이유 - 인사팀: 실제 퇴사자를 퇴사자로 예측해야 의미가 있음! 우리는 퇴사할것 같은 10명을 찍어달란 의미였어요! (그래야 면담을 하든 할거아냐!) - 최규빈: 가볍고(=파라메터 적고) 잘 맞추는 모형 만들어 달라면서요?\n\n인사팀: (고민중..) 사실 생각해보니까 이 경우는 애큐러시는 의미가 없네. 실제 나간 사람 중 최규빈이 나간다고 한 사람이 몇인지 카운트 하는게 더 의미가 있겠다. 우리는 앞으로 리컬(혹은 민감도)를 보겠다!\n\n\n예시1: 실제로 퇴사한 10명중 최규빈이 나간다고 찍은 사람이 5명이면 리컬이 50%\n\n\n예시2: 최규빈이 아무도 나가지 않는다고 예측해버린다? 실제 10명중에서 최규빈이 나간다고 적중시킨사람은 0명이므로 이 경우 리컬은 0%\n\n\n결론: 우리가 필요한건 recall이니까 앞으로 recall을 가져와! accuracy는 큰 의미없어. (그래도 명색이 모델인데 accuracy가 90은 되면 좋겠다)\n\n\n\n\n- 정의: Precision(프리시즌)=\\(\\frac{TP}{TP+FP}\\)=\\(\\frac{\\# O/O}{\\# O/O+\\# X/O}\\) - 분모: O라고 예측한 관측치 - 분자: O라고 예측한 관측치중 진짜 O인 관측치 - 뜻: O라고 예측한 관측치중 진짜 O인 비율\n- (상황극 시점3) recall 만으로 불충분한 이유\n\n최규빈: 에휴.. 귀찮은데 그냥 좀만 수틀리면 다 나갈것 같다고 해야겠다. -> 한 100명 나간다고 했음 -> 실제로 최규빈이 찍은 100명중에 10명이 다 나감!\n\n\n이 경우 애큐러시는 91%, 리컬은 100% (퇴사자 10명을 일단은 다 맞췄으므로).\n\n\n인사팀: (화가 많이 남) 멀쩡한 사람까지 다 퇴사할 것 같다고 하면 어떡해요? 최규빈 연구원이 나간다고 한 100명중에 실제로 10명만 나갔어요.\n인사팀: 마치 총으로 과녁중앙에 맞춰 달라고 했더니 기관총을 가져와서 한번 긁은것이랑 뭐가 달라요? 맞추는게 문제가 아니고 precision이 너무 낮아요.\n최규빈: accuracy 90% 이상, recall은 높을수록 좋다는게 주문 아니었나요?\n인사팀: (고민중..) 앞으로는 recall과 함께 precision도 같이 제출하세요. precision은 당신이 나간다고 한 사람중에 실제 나간사람의 비율을 의미해요. 이 경우는 \\(\\frac{10}{100}\\)이니까 precision이 10%입니다. (속마음: recall 올리겠다고 무작정 너무 많이 예측하지 말란 말이야!)\n\n\n\n\n- 정의: recall과 precision의 조화평균\n- (상황극 시점4) recall, precision을 모두 고려\n\n최규빈: recall/precision을 같이 내는건 좋은데요, 둘은 trade off의 관계에 있습니다. 물론 둘다 올리는 모형이 있다면 좋지만 그게 쉽지는 않아요. 보통은 precision을 올리려면 recall이 희생되는 면이 있고요, recall을 올리려고 하면 precision이 다소 떨어집니다.\n최규빈: 평가기준이 애매하다는 의미입니다. 모형1,2가 있는데 모형1은 모형2보다 precision이 약간 좋고 대신 recall이 떨어진다면 모형1이 좋은것입니까? 아니면 모형2가 좋은것입니까?\n인사팀: 그렇다면 둘을 평균내서 F1score를 계산해서 제출해주세요.\n\n\n\n\n- 정의:\n\nSpecificity(특이도)=\\(\\frac{TN}{FP+TN}\\)=\\(\\frac{\\# X/X}{\\# X/O+\\# X/X}\\)\nFalse Positive Rate (FPR) = 1-Specificity(특이도) = \\(\\frac{FP}{FP+TN}\\)=\\(\\frac{\\# X/O}{\\# X/O+\\# X/X}\\)\n\n- 의미: FPR = 오해해서 미안해, recall(=TPR)을 올리려고 보니 어쩔 수 없었어 ㅠㅠ - specificity는 안나간 사람을 안나갔다고 찾아낸 비율인데 별로 안중요하다. - FPR은 recall을 올리기 위해서 “실제로는 회사 잘 다니고 있는 사람 중 최규빈이 나갈것 같다고 찍은 사람들” 의 비율이다.\n\n즉 생사람잡은 비율.. 오해해서 미안한 사람의 비율..\n\n\n\n\n- 정의: \\(x\\)축=FPR, \\(y\\)축=TPR 을 그린 커브\n- 의미: - 결국 “오해해서 미안해 vs recall”을 그린 곡선이 ROC커브이다. - 생각해보면 오해하는 사람이 많을수록 당연히 recall은 올라간다. 따라서 우상향하는 곡선이다. - 오해한 사람이 매우 적은데 recall이 우수하면 매우 좋은 모형이다. 그래서 초반부터 ROC값이 급격하게 올라가면 좋은 모형이다.\n\n\n\n- data\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\n\nX= x_train.reshape(-1,784)\ny= tf.keras.utils.to_categorical(y_train)\nXX = x_test.reshape(-1,784)\nyy = tf.keras.utils.to_categorical(y_test)\n\n- 다양한 평가지표를 넣는 방법 (1)\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(10,activation = 'softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy','Recall'])\nnet.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.0415 - accuracy: 0.7486 - recall: 0.7143\nEpoch 2/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.6277 - accuracy: 0.7989 - recall: 0.7492\nEpoch 3/5\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.5331 - accuracy: 0.8233 - recall: 0.7836\nEpoch 4/5\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.4895 - accuracy: 0.8345 - recall: 0.7950\nEpoch 5/5\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.4537 - accuracy: 0.8410 - recall: 0.8020\n\n\n<keras.callbacks.History at 0x7fb96454ceb0>\n\n\n\nnet.evaluate(XX,yy)\n\n313/313 [==============================] - 1s 1ms/step - loss: 0.4773 - accuracy: 0.8266 - recall: 0.7869\n\n\n[0.4773379862308502, 0.8266000151634216, 0.786899983882904]\n\n\n- 다양한 평가지표를 넣는 방법 (2)\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(10,activation = 'softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=[tf.metrics.CategoricalAccuracy(),tf.metrics.Recall()])\nnet.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 2.2118 - categorical_accuracy: 0.7407 - recall: 0.6980\nEpoch 2/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.6113 - categorical_accuracy: 0.8028 - recall: 0.7539\nEpoch 3/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.5309 - categorical_accuracy: 0.8209 - recall: 0.7738\nEpoch 4/5\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.4541 - categorical_accuracy: 0.8401 - recall: 0.7995\nEpoch 5/5\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.4321 - categorical_accuracy: 0.8472 - recall: 0.8112\n\n\n<keras.callbacks.History at 0x7fb9bc8bd180>\n\n\n\nnet.evaluate(XX,yy)\n\n313/313 [==============================] - 1s 2ms/step - loss: 0.4524 - categorical_accuracy: 0.8434 - recall: 0.7866\n\n\n[0.4523784816265106, 0.8434000015258789, 0.7865999937057495]\n\n\n\n\n\n\n- 이미지 데이터를 분류하기 좋은 형태로 자료를 재정리하자.\n\nX = tf.constant(x_train.reshape(-1,28,28,1),dtype=tf.float64)\ny = tf.keras.utils.to_categorical(y_train)\nXX = tf.constant(x_test.reshape(-1,28,28,1),dtype=tf.float64)\nyy = tf.keras.utils.to_categorical(y_test)\n\n\nX.shape,XX.shape,y.shape,yy.shape\n\n(TensorShape([60000, 28, 28, 1]),\n TensorShape([10000, 28, 28, 1]),\n (60000, 10),\n (10000, 10))\n\n\n- 일반적인 이미지 분석 모형을 적용하기 용이한 데이터 형태로 정리했다. -> 그런데 모형에 넣고 돌릴려면 다시 차원을 펼쳐야 하지 않을까?\n- 안펼치고 하고싶다.\n\nflttn = tf.keras.layers.Flatten()\n\n\nset(dir(flttn)) & {'__call__'}\n\n{'__call__'}\n\n\n\nX.shape,flttn(X).shape, X.reshape(-1,784).shape\n\n(TensorShape([60000, 28, 28, 1]),\n TensorShape([60000, 784]),\n TensorShape([60000, 784]))\n\n\n- flttn\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(10,activation = 'softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=[tf.metrics.CategoricalAccuracy(),tf.metrics.Recall()])\nnet.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 2.3353 - categorical_accuracy: 0.7503 - recall_1: 0.7104\nEpoch 2/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.6345 - categorical_accuracy: 0.7938 - recall_1: 0.7381\nEpoch 3/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.5553 - categorical_accuracy: 0.8139 - recall_1: 0.7731\nEpoch 4/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.4954 - categorical_accuracy: 0.8336 - recall_1: 0.7936\nEpoch 5/5\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.4516 - categorical_accuracy: 0.8423 - recall_1: 0.8015\n\n\n<keras.callbacks.History at 0x7fb9bc62d3c0>\n\n\n\nnet.layers\n\n[<keras.layers.core.flatten.Flatten at 0x7fb9bc68feb0>,\n <keras.layers.core.dense.Dense at 0x7fb9bc68e950>,\n <keras.layers.core.dense.Dense at 0x7fb9bc68fc40>,\n <keras.layers.core.dense.Dense at 0x7fb9bc68d6f0>]\n\n\n\nprint(X.shape)\nprint(net.layers[0](X).shape)\nprint(net.layers[1](net.layers[0](X)).shape)\nprint(net.layers[2](net.layers[1](net.layers[0](X))).shape)\n\n(60000, 28, 28, 1)\n(60000, 784)\n(60000, 500)\n(60000, 500)\n\n\n- 좀 더 복잡한 네트워크 -> 하지만 한계가 보인다 -> 좀 더 나은 아키텍처는 없을까\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential() \nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(500,activation='relu'))\nnet.add(tf.keras.layers.Dense(500,activation='relu'))\nnet.add(tf.keras.layers.Dense(500,activation='relu'))\nnet.add(tf.keras.layers.Dense(500,activation='relu'))\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics='accuracy') \nnet.fit(X,y,epochs=10)\n\nEpoch 1/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.1090 - accuracy: 0.7885\nEpoch 2/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.4591 - accuracy: 0.8367\nEpoch 3/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.4148 - accuracy: 0.8536\nEpoch 4/10\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.3907 - accuracy: 0.8612\nEpoch 5/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3775 - accuracy: 0.8665\nEpoch 6/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3597 - accuracy: 0.8739\nEpoch 7/10\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.3463 - accuracy: 0.8769\nEpoch 8/10\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.3381 - accuracy: 0.8796\nEpoch 9/10\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.3282 - accuracy: 0.8832\nEpoch 10/10\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.3181 - accuracy: 0.8862\n\n\n<keras.callbacks.History at 0x7fb9bc40dc60>\n\n\n\nnet.evaluate(XX,yy)\n\n313/313 [==============================] - 1s 2ms/step - loss: 0.3905 - accuracy: 0.8646\n\n\n[0.3904523551464081, 0.8646000027656555]\n\n\n- layer중에 우리는 끽해야 Dense정도 쓰고있었음. \\(\\to\\) flatten과 같은 다른 layer도 많음. \\(\\to\\) 이런것도 써보자"
  },
  {
    "objectID": "posts/Untitled2.html",
    "href": "posts/Untitled2.html",
    "title": "2023/03/17 테스트",
    "section": "",
    "text": "테스트\n\n\n\nimage.png\n\n\n\n1+1\n\n2\n\n\n\n!git add .\n!git commit -m .\n!git push\n!quarto publish gh-pages --no-prompt --no-browser"
  },
  {
    "objectID": "posts/2022-04-18-7주차_빅데이터분석특강.html",
    "href": "posts/2022-04-18-7주차_빅데이터분석특강.html",
    "title": "jisim12",
    "section": "",
    "text": "7주차-4월 18일\n\n빅데이터분석특강\n\n\ntoc:false\nbranch: master\nbadges: true\ncomments: true\nauthor: 심재인\n\n\nimports\n\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport tensorflow as tf \nimport tensorflow.experimental.numpy as tnp \n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+s + '; }')\n\n\n\npiece-wise linear regression\nmodel: \\(y_i=\\begin{cases} x_i +0.3\\epsilon_i & x\\leq 0 \\\\ 3.5x_i +0.3\\epsilon_i & x>0 \\end{cases}\\)\n\nnp.random.seed(43052)\nN=100\nx = np.linspace(-1,1,N)\nlamb = lambda x: x*1+np.random.normal()*0.3 if x<0 else x*3.5+np.random.normal()*0.3 \ny= np.array(list(map(lamb,x)))\ny\n\narray([-0.88497385, -0.65454563, -0.61676249, -0.84702584, -0.84785569,\n       -0.79220455, -1.3777105 , -1.27341781, -1.41643729, -1.26404671,\n       -0.79590224, -0.78824395, -0.86064773, -0.52468679, -1.18247354,\n       -0.29327295, -0.69373049, -0.90561768, -1.07554911, -0.7225404 ,\n       -0.69867774, -0.34811037,  0.11188474, -1.05046296, -0.03840085,\n       -0.38356861, -0.24299798, -0.58403161, -0.20344022, -0.13872303,\n       -0.529586  , -0.27814478, -0.10852781, -0.38294596,  0.02669763,\n       -0.23042603, -0.77720364, -0.34287396, -0.04512022, -0.30180793,\n       -0.26711438, -0.51880349, -0.53939672, -0.32052379, -0.32080763,\n        0.28917092,  0.18175206, -0.48988124, -0.08084459,  0.37706178,\n        0.14478908,  0.07621827, -0.071864  ,  0.05143365,  0.33932009,\n       -0.35071776,  0.87742867,  0.51370399,  0.34863976,  0.55855514,\n        1.14196717,  0.86421076,  0.72957843,  0.57342304,  1.54803332,\n        0.98840018,  1.11129366,  1.42410801,  1.44322465,  1.25926455,\n        1.12940772,  1.46516829,  1.16365096,  1.45560853,  1.9530553 ,\n        2.45940445,  1.52921129,  1.8606463 ,  1.86406718,  1.5866523 ,\n        1.49033473,  2.35242686,  2.12246412,  2.41951931,  2.43615052,\n        1.96024441,  2.65843789,  2.46854394,  2.76381882,  2.78547462,\n        2.56568465,  3.15212157,  3.11482949,  3.17901774,  3.31268904,\n        3.60977818,  3.40949166,  3.30306495,  3.74590922,  3.85610433])\n\n\n\nplt.plot(x,y,'.')\n\n\n\n\n\n풀이1: 단순회귀모형\n\nx= x.reshape(N,1)\ny= y.reshape(N,1)\n\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1)) \nnet.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\nnet.fit(x,y,batch_size=N,epochs=1000,verbose=0) # numpy로 해도 돌아감\n\n2022-04-25 14:04:34.246393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n<keras.callbacks.History at 0x7f315c2d1630>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[2.2616348]], dtype=float32)>,\n <tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([0.6069048], dtype=float32)>]\n\n\n\nyhat = x * 2.2616348 + 0.6069048\nyhat = net.predict(x)\n\n\nplt.plot(x,y,'.')\nplt.plot(x,yhat,'--')\n\n\n\n\n- 실패: 이 모형은 epoch을 10억번 돌려도 실패할 모형임 - 왜? 아키텍처 설계자체가 틀렸음 - 꺽인부분을 표현하기에는 아키텍처의 표현력이 너무 부족하다 -> under fit의 문제\n\n\n풀이2: 비선형 활성화 함수의 도입\n- 여기에서 비선형 활성화 함수는 relu\n- 네트워크를 아래와 같이 수정하자.\n(수정전) hat은 생략\n\n#collapse\ngv('''\n\"x\" -> \"x*w,    bias=True\"[label=\"*w\"] ;\n\"x*w,    bias=True\" -> \"y\"[label=\"indentity\"] ''')\n\nExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\n\n\n<graphviz.sources.Source at 0x7f315c0af970>\n\n\n(수정후) hat은 생략\n\n#collapse\ngv('''\n\"x\" -> \"x*w,    bias=True\"[label=\"*w\"] ;\n\"x*w,    bias=True\" -> \"y\"[label=\"relu\"] ''')\n\nExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\n\n\n<graphviz.sources.Source at 0x7f30f84d5a50>\n\n\n\n마지막에 \\(f(x)=x\\) 라는 함수대신에 relu를 취하는 것으로 구조를 약간 변경\n활성화함수(acitivation function)를 indentity에서 relu로 변경\n\n- relu함수란?\n\n_x = np.linspace(-1,1,100)\ntf.nn.relu(_x)\n\n<tf.Tensor: shape=(100,), dtype=float64, numpy=\narray([0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.01010101, 0.03030303, 0.05050505, 0.07070707, 0.09090909,\n       0.11111111, 0.13131313, 0.15151515, 0.17171717, 0.19191919,\n       0.21212121, 0.23232323, 0.25252525, 0.27272727, 0.29292929,\n       0.31313131, 0.33333333, 0.35353535, 0.37373737, 0.39393939,\n       0.41414141, 0.43434343, 0.45454545, 0.47474747, 0.49494949,\n       0.51515152, 0.53535354, 0.55555556, 0.57575758, 0.5959596 ,\n       0.61616162, 0.63636364, 0.65656566, 0.67676768, 0.6969697 ,\n       0.71717172, 0.73737374, 0.75757576, 0.77777778, 0.7979798 ,\n       0.81818182, 0.83838384, 0.85858586, 0.87878788, 0.8989899 ,\n       0.91919192, 0.93939394, 0.95959596, 0.97979798, 1.        ])>\n\n\n\nplt.plot(_x,_x)\nplt.plot(_x,tf.nn.relu(_x))\n\n\n\n\n\n파란색을 주황색으로 바꿔주는 것이 렐루함수임\n\\(f(x)=\\max(0,x)=\\begin{cases} 0 & x\\leq 0 \\\\ x & x>0 \\end{cases}\\)\n\n- 아키텍처: \\(\\hat{y}_i=relu(\\hat{w}_0+\\hat{w}_1x_i)\\), \\(relu(x)=\\max(0,x)\\)\n- 풀이시작\n1단계\n\nnet2 = tf.keras.Sequential()\n\n2단계\n\ntf.random.set_seed(43053)\nl1 = tf.keras.layers.Dense(1, input_shape=(1,)) \na1 = tf.keras.layers.Activation(tf.nn.relu)\n\n\nnet2.add(l1)\n\n\nnet2.layers\n\n[<keras.layers.core.dense.Dense at 0x7f30e0307820>]\n\n\n\nnet2.add(a1)\n\n\nnet2.layers\n\n[<keras.layers.core.dense.Dense at 0x7f30e0307820>,\n <keras.layers.core.activation.Activation at 0x7f30e0305330>]\n\n\n\nl1.get_weights()\n\n[array([[0.41721308]], dtype=float32), array([0.], dtype=float32)]\n\n\n\nnet2.get_weights()\n\n[array([[0.41721308]], dtype=float32), array([0.], dtype=float32)]\n\n\n(네트워크 상황 확인)\n\nu1= l1(x)\n#u1= x@l1.weights[0] + l1.weights[1]\n\n\nv1= a1(u1)\n#v1= tf.nn.relu(u1)\n\n\nplt.plot(x,x)\nplt.plot(x,u1,'--r')\nplt.plot(x,v1,'--b')\n\n\n\n\n3단계\n\nnet2.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\n\n4단계\n\nnet2.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n<keras.callbacks.History at 0x7f30e037bf70>\n\n\n- result\n\nyhat = tf.nn.relu(x@l1.weights[0] + l1.weights[1]) \nyhat = net2.predict(x)\nyhat = net2(x)\nyhat = a1(l1(x))\nyhat = net2.layers[1](net2.layers[0](x))\n\n\nplt.plot(x,y,'.')\nplt.plot(x,yhat,'--')\n\n\n\n\n- discussion - 이것 역시 수백억번 에폭을 반복해도 이 이상 적합이 힘들다 \\(\\to\\) 모형의 표현력이 떨어진다. - 해결책: 주황색점선이 2개 있다면 어떨까?\n\n\n풀이3: 노드수추가 + 레이어추가\n목표: 2개의 주황색 점선을 만들자.\n1단계\n\nnet3 = tf.keras.Sequential()\n\n2단계\n\ntf.random.set_seed(43053)\nl1 = tf.keras.layers.Dense(2,input_shape=(1,))\na1 = tf.keras.layers.Activation(tf.nn.relu)\n\n\nnet3.add(l1)\nnet3.add(a1)\n\n(네트워크 상황 확인)\n\nl1(x).shape\n# l1(x) : (100,1) -> (100,2)\n\nTensorShape([100, 2])\n\n\n\nplt.plot(x,x)\nplt.plot(x,l1(x),'--')\n\n\n\n\n\nplt.plot(x,x)\nplt.plot(x,a1(l1(x)),'--')\n\n\n\n\n- 이 상태에서는 yhat이 안나온다. 왜? - 차원이 안맞음. a1(l1(x))의 차원은 (N,2)인데 최종적인 yhat의 차원은 (N,1)이어야 함. - 차원이 어찌저찌 맞다고 쳐도 relu를 통과하면 항상 yhat>0 임. 따라서 음수값을 가지는 y는 0으로 밖에 맞출 수 없음.\n- 해결책: a1(l1(x))에 연속으로(Sequential하게!) 또 다른 레이어를 설계! (N,2) -> (N,1) 이 되도록! - yhat= bias + weight1 * a1(l1(x))[0] + weight2 * a1(l1(x))[1]\n- 즉 a1(l1(x)) 를 새로운 입력으로 해석하고 출력을 만들어주는 선형모형을 다시태우면 된다. - 입력차원: 2 - 출력차원: 1\n\nnet3.layers\n\n[<keras.layers.core.dense.Dense at 0x7f30e02223b0>,\n <keras.layers.core.activation.Activation at 0x7f3248129000>]\n\n\n\ntf.random.set_seed(43053)\nl2 = tf.keras.layers.Dense(1, input_shape=(2,))\n\n\nnet3.add(l2)\n\n\nnet3.layers\n\n[<keras.layers.core.dense.Dense at 0x7f30e02223b0>,\n <keras.layers.core.activation.Activation at 0x7f3248129000>,\n <keras.layers.core.dense.Dense at 0x7f315c381000>]\n\n\n\nnet3.summary()\n\nModel: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_2 (Dense)             (None, 2)                 4         \n                                                                 \n activation_1 (Activation)   (None, 2)                 0         \n                                                                 \n dense_3 (Dense)             (None, 1)                 3         \n                                                                 \n=================================================================\nTotal params: 7\nTrainable params: 7\nNon-trainable params: 0\n_________________________________________________________________\n\n\n- 추정해야할 파라메터수가 4,0,3으로 나온다.\n- 수식표현: \\(X \\to X@W^{(1)}+b^{(1)} \\to relu(X@W^{(1)}+b^{(1)}) \\to relu(X@W^{(1)}+b^{(1)})@W^{(2)}+b^{(2)}=yhat\\)\n\n\\(X\\): (N,1)\n\\(W^{(1)}\\): (1,2) ==> 파라메터 2개 추정\n\\(b^{(1)}\\): (2,) ==> 파라메터 2개가 추가 // 여기까지 추정할 파라메터는 4개\n\\(W^{(2)}\\): (2,1) ==> 파라메터 2개 추정\n\\(b^{(2)}\\): (1,) ==> 파라메터 1개가 추가 // 따라서 3개\n\n- 참고: 추정할 파라메터수가 많다 = 복잡한 모형이다. - 초거대AI: 추정할 파라메터수가 엄청 많은..\n\nnet3.weights\n\n[<tf.Variable 'dense_2/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[ 0.34065306, -0.7533803 ]], dtype=float32)>,\n <tf.Variable 'dense_2/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>,\n <tf.Variable 'dense_3/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[ 0.34065306],\n        [-0.7533803 ]], dtype=float32)>,\n <tf.Variable 'dense_3/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n\n\n\nl1.weights\n\n[<tf.Variable 'dense_2/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[ 0.34065306, -0.7533803 ]], dtype=float32)>,\n <tf.Variable 'dense_2/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>]\n\n\n\nl2.weights\n\n[<tf.Variable 'dense_3/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[ 0.34065306],\n        [-0.7533803 ]], dtype=float32)>,\n <tf.Variable 'dense_3/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n\n\n- 좀 더 간단한 수식표현: \\(X \\to (u_1 \\to v_1) \\to (u_2 \\to v_2) = yhat\\) - \\(u_1= X@W^{(1)}+b^{(1)}\\) - \\(v_1= relu(u_1)\\) - \\(u_2= v_1@W^{(2)}+b^{(2)}\\) - \\(v_2= indentity(u_2):=yhat\\)\n\n#collapse\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"X\" \n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"X\" -> \"u1[:,0]\"[label=\"*W1[0,0]\"]\n    \"X\" -> \"u1[:,1]\"[label=\"*W1[0,1]\"]\n    \"u1[:,0]\" -> \"v1[:,0]\"[label=\"relu\"]\n    \"u1[:,1]\" -> \"v1[:,1]\"[label=\"relu\"]\n    label = \"Layer 1\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"v1[:,0]\" -> \"yhat\"[label=\"*W2[0,0]\"]\n    \"v1[:,1]\" -> \"yhat\"[label=\"*W2[1,0]\"]\n    label = \"Layer 2\"\n}\n''')\n\nExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\n\n\n<graphviz.sources.Source at 0x7f32643d78e0>\n\n\n\n#collapse\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"X\" \n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"X\" -> \"node1\"\n    \"X\" -> \"node2\"\n    label = \"Layer 1: relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -> \"yhat\"\n    \"node2\" -> \"yhat\"\n    label = \"Layer 2\"\n}\n''')\n\nExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\n\n\n<graphviz.sources.Source at 0x7f30e02a2230>\n\n\n3단계\n\nnet3.compile(loss='mse',optimizer=tf.optimizers.SGD(0.1))\n\n4단계\n\nnet3.fit(x,y,epochs=1000,verbose=0, batch_size=N)\n\n<keras.callbacks.History at 0x7f30e01a17b0>\n\n\n- 결과확인\n\nnet3.weights\n\n[<tf.Variable 'dense_2/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[ 1.6352793, -0.8550755]], dtype=float32)>,\n <tf.Variable 'dense_2/bias:0' shape=(2,) dtype=float32, numpy=array([-0.0828446,  0.8555219], dtype=float32)>,\n <tf.Variable 'dense_3/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[ 1.6328751],\n        [-1.2001745]], dtype=float32)>,\n <tf.Variable 'dense_3/bias:0' shape=(1,) dtype=float32, numpy=array([1.0253302], dtype=float32)>]\n\n\n\nplt.plot(x,y,'.') \nplt.plot(x,net3(x),'--')\n\n\n\n\n- 분석\n\nplt.plot(x,y,'.') \nplt.plot(x,l1(x),'--')\n\n\n\n\n\nplt.plot(x,y,'.') \nplt.plot(x,a1(l1(x)),'--')\n\n\n\n\n\nplt.plot(x,y,'.') \nplt.plot(x,l2(a1(l1(x))),'--')\n\n\n\n\n- 마지막 2개의 그림을 분석\n\nl2.weights\n\n[<tf.Variable 'dense_3/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[ 1.6328751],\n        [-1.2001745]], dtype=float32)>,\n <tf.Variable 'dense_3/bias:0' shape=(1,) dtype=float32, numpy=array([1.0253302], dtype=float32)>]\n\n\n\nfig, (ax1,ax2,ax3) = plt.subplots(1,3) \nfig.set_figwidth(12) \nax1.plot(x,y,'.')\nax1.plot(x,a1(l1(x))[:,0],'--r')\nax1.plot(x,a1(l1(x))[:,1],'--b')\nax2.plot(x,y,'.')\nax2.plot(x,a1(l1(x))[:,0]*1.6328746,'--r')\nax2.plot(x,a1(l1(x))[:,1]*(-1.2001747)+1.0253307,'--b')\nax3.plot(x,y,'.')\nax3.plot(x,a1(l1(x))[:,0]*1.6328746+a1(l1(x))[:,1]*(-1.2001747)+1.0253307,'--')\n\n\n\n\n\n\n풀이3의 실패\n\ntf.random.set_seed(43054)\n## 1단계\nnet3 = tf.keras.Sequential()\n## 2단계\nnet3.add(tf.keras.layers.Dense(2))\nnet3.add(tf.keras.layers.Activation('relu'))\nnet3.add(tf.keras.layers.Dense(1))\n## 3단계\nnet3.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\n## 4단계\nnet3.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n<keras.callbacks.History at 0x7f30c45a7640>\n\n\n\nplt.plot(x,y,'.')\nplt.plot(x,net3(x),'--')\n\n\n\n\n- 엥? 에폭이 부족한가?\n\nnet3.fit(x,y,epochs=10000,verbose=0,batch_size=N)\nplt.plot(x,y,'.')\nplt.plot(x,net3(x),'--')\n\n\n\n\n- 실패분석\n\nl1,a1,l2 = net3.layers\n\n\nl2.weights\n\n[<tf.Variable 'dense_5/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[0.65121335],\n        [1.8592643 ]], dtype=float32)>,\n <tf.Variable 'dense_5/bias:0' shape=(1,) dtype=float32, numpy=array([-0.60076195], dtype=float32)>]\n\n\n\nfig, (ax1,ax2,ax3,ax4) = plt.subplots(1,4)\nfig.set_figwidth(16)\nax1.plot(x,y,'.')\nax1.plot(x,l1(x)[:,0],'--r')\nax1.plot(x,l1(x)[:,1],'--b')\nax2.plot(x,y,'.')\nax2.plot(x,a1(l1(x))[:,0],'--r')\nax2.plot(x,a1(l1(x))[:,1],'--b')\nax3.plot(x,y,'.')\nax3.plot(x,a1(l1(x))[:,0]*0.65121335,'--r')\nax3.plot(x,a1(l1(x))[:,1]*(1.8592643)+(-0.60076195),'--b')\nax4.plot(x,y,'.')\nax4.plot(x,a1(l1(x))[:,0]*0.65121335+a1(l1(x))[:,1]*(1.8592643)+(-0.60076195),'--')\n\n\n\n\n\n보니까 빨간색선이 하는 역할을 없음\n그런데 생각해보니까 이 상황에서는 빨간색선이 할수 있는 일이 별로 없음\n왜? 지금은 나름 파란색선에 의해서 최적화가 된 상태임 \\(\\to\\) 빨간선이 뭔가 하려고하면 최적화된 상태가 깨질 수 있음 (loss 증가)\n즉 이 상황 자체가 나름 최적회된 상태이다. 이러한 현상을 “global minimum을 찾지 못하고 local minimum에 빠졌다”라고 표현한다.\n\n확인:\n\nnet3.weights\n\n[<tf.Variable 'dense_4/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[-0.03077251,  1.8713338 ]], dtype=float32)>,\n <tf.Variable 'dense_4/bias:0' shape=(2,) dtype=float32, numpy=array([-0.04834982,  0.3259186 ], dtype=float32)>,\n <tf.Variable 'dense_5/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[0.65121335],\n        [1.8592643 ]], dtype=float32)>,\n <tf.Variable 'dense_5/bias:0' shape=(1,) dtype=float32, numpy=array([-0.60076195], dtype=float32)>]\n\n\n\nW1= tf.Variable(tnp.array([[-0.03077251,  1.8713338 ]]))\nb1= tf.Variable(tnp.array([-0.04834982,  0.3259186 ]))\nW2= tf.Variable(tnp.array([[0.65121335],[1.8592643 ]]))\nb2= tf.Variable(tnp.array([-0.60076195]))\n\n\nwith tf.GradientTape() as tape:\n    u = tf.constant(x) @ W1 + b1\n    v = tf.nn.relu(u)\n    yhat = v@W2 + b2\n    loss = tf.losses.mse(y,yhat)\n\n\ntape.gradient(loss,[W1,b1,W2,b2])\n\n[<tf.Tensor: shape=(1, 2), dtype=float64, numpy=array([[ 0.00000000e+00, -4.77330119e-05]])>,\n <tf.Tensor: shape=(2,), dtype=float64, numpy=array([0.0000000e+00, 3.1478608e-06])>,\n <tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n array([[ 0.00000000e+00],\n        [-4.74910706e-05]])>,\n <tf.Tensor: shape=(1,), dtype=float64, numpy=array([-2.43031263e-05])>]\n\n\n예상대로 계수값이 거의 다 0이다. #### 풀이4: 노드수를 더 추가한다면? - 노드수를 더 추가해보면 어떻게 될까? (주황색 점선이 더 여러개 있다면?)\n\n#collapse\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"X\" \n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"X\" -> \"node1\"\n    \"X\" -> \"node2\"\n    \"X\" -> \"...\"\n    \"X\" -> \"node512\"\n    label = \"Layer 1: relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -> \"yhat\"\n    \"node2\" -> \"yhat\"\n    \"...\" -> \"yhat\"\n    \"node512\" -> \"yhat\"\n    label = \"Layer 2\"\n}\n''')\n\nExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\n\n\n<graphviz.sources.Source at 0x7f30c40ff850>\n\n\n\ntf.random.set_seed(43056)\nnet4= tf.keras.Sequential()\nnet4.add(tf.keras.layers.Dense(512,activation='relu')) # 이렇게 해도됩니다. \nnet4.add(tf.keras.layers.Dense(1))         \nnet4.compile(loss='mse',optimizer=tf.optimizers.SGD(0.1)) \nnet4.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n<keras.callbacks.History at 0x7f30bc70d9f0>\n\n\n\nplt.plot(x,y,'.')\nplt.plot(x,net4(x),'--')\n\n\n\n\n\n잘된다..\n한두개의 노드가 역할을 못해도 다른노드들이 잘 보완해주는듯!\n\n- 노드수가 많으면 무조건 좋다? -> 대부분 나쁘지 않음. 그런데 종종 맞추지 말아야할것도 맞춤.. (overfit)\n\nnp.random.seed(43052)\nN=100\n_x = np.linspace(0,1,N).reshape(N,1)\n_y = np.random.normal(loc=0,scale=0.001,size=(N,1))\nplt.plot(_x,_y)\n\n\n\n\n\ntf.random.set_seed(43052)\nnet4 = tf.keras.Sequential()\nnet4.add(tf.keras.layers.Dense(512,activation='relu'))\nnet4.add(tf.keras.layers.Dense(1))\nnet4.compile(loss='mse',optimizer=tf.optimizers.SGD(0.5))\nnet4.fit(_x,_y,epochs=1000,verbose=0,batch_size=N)\n\n<keras.callbacks.History at 0x7f30bc7faec0>\n\n\n\nplt.plot(_x,_y)\nplt.plot(_x,net4(_x),'--')\n\n\n\n\n\n이 예제는 추후 다시 공부할 예정\n\n\n\n\nLogistic regression\n\nmotive\n- 현실에서 이런 경우가 많음 - \\(x\\)가 커질수록 (혹은 작아질수록) 성공확률이 올라간다.\n- 이러한 모형은 아래와 같이 설계할 수 있음 <– 외우세요!! - \\(y_i \\sim Ber(\\pi_i)\\), where \\(\\pi_i=\\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\)\n\n\\(\\hat{y}_i =\\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\frac{1}{1+exp(-\\hat{w}_0-\\hat{w}_1x_i)}\\)\n\\(loss=-\\frac{1}{n}\\sum_{i=1}^{n}\\big(y_i\\log(\\hat{y}_i)+(1-y_i)\\log(1-\\hat{y}_i)\\big)\\)\n\n- 위와 같은 손실함수를 BCEloss라고 부른다. (BCE는 Binary Cross Entropy의 약자)\n\n\n예제\n\nN = 2000\n\n\nx = tnp.linspace(-1,1,N).reshape(N,1)\nw0 = -1 \nw1 = 5 \nu = w0 + x*w1 \n#v = tf.constant(np.exp(u)/(1+np.exp(u))) # v=πi \nv = tf.nn.sigmoid(u) \ny = tf.constant(np.random.binomial(1,v),dtype=tf.float64)\n\n\nplt.plot(x,y,'.',alpha=0.02)\nplt.plot(x,v,'--r')\n\n\n\n\n- 이 아키텍처(yhat을 얻어내는 과정)를 다어어그램으로 나타내면 아래와 같다.\n\n#collapse\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x\" \n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x\" -> \"x*w, bias=True\"[label=\"*w\"]\n    \"x*w, bias=True\" -> \"yhat\"[label=\"sigmoid\"]\n    label = \"Layer 1\"\n}\n''')\n\nExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\n\n\n<graphviz.sources.Source at 0x7f30bc52afb0>\n\n\n- 또는 간단하게 아래와 같이 쓸 수 있다.\n\n#collapse\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    x\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    x -> \"node1=yhat\"\n    label = \"Layer 1: sigmoid\"\n}\n''')\n\nExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\n\n\n<graphviz.sources.Source at 0x7f30bc55a7d0>\n\n\n- 케라스를 이용하여 적합을 해보면\n\n\\(loss=-\\frac{1}{n}\\sum_{i=1}^{n}\\big(y_i\\log(\\hat{y}_i)+(1-y_i)\\log(1-\\hat{y}_i)\\big)\\)\n\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential() \nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nbceloss_fn = lambda y,yhat: -tf.reduce_mean(y*tnp.log(yhat) + (1-y)*tnp.log(1-yhat))\nnet.compile(loss=bceloss_fn, optimizer=tf.optimizers.SGD(0.1))\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n<keras.callbacks.History at 0x7f30bc488490>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense_10/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[4.33877]], dtype=float32)>,\n <tf.Variable 'dense_10/bias:0' shape=(1,) dtype=float32, numpy=array([-0.8301144], dtype=float32)>]\n\n\n\nplt.plot(x,y,'.',alpha=0.1)\nplt.plot(x,v,'--r')\nplt.plot(x,net(x),'--b')\n\n\n\n\n\n\nMSE loss?\n- mse loss를 쓰면 왜 안되는지?\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential() \nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nmseloss_fn = lambda y,yhat: tf.reduce_mean((y-yhat)**2)\nnet.compile(loss=mseloss_fn, optimizer=tf.optimizers.SGD(0.1))\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n<keras.callbacks.History at 0x7f30c419aa10>\n\n\n\nplt.plot(x,y,'.',alpha=0.1)\nplt.plot(x,v,'--r')\nplt.plot(x,net(x),'--b')\n\n\n\n\n\n일단 BCE loss와 비교해보니까 동일 초기값, 동일 epochs에서 적합이 별로임\n\n\n\nMSE loss vs BCE loss\n- MSEloss, BCEloss의 시각화\n\nw0, w1 = np.meshgrid(np.arange(-10,3,0.2), np.arange(-1,10,0.2), indexing='ij')\nw0, w1 = w0.reshape(-1), w1.reshape(-1)\n\ndef loss_fn1(w0,w1):\n    u = w0+w1*x \n    yhat = np.exp(u)/(np.exp(u)+1)\n    return mseloss_fn(y,yhat) \n\ndef loss_fn2(w0,w1):\n    u = w0+w1*x \n    yhat = np.exp(u)/(np.exp(u)+1)\n    return bceloss_fn(y,yhat) \n\nloss1 = list(map(loss_fn1,w0,w1))\nloss2 = list(map(loss_fn2,w0,w1))\n\n\nfig = plt.figure()\nfig.set_figwidth(9)\nfig.set_figheight(9)\nax1=fig.add_subplot(1,2,1,projection='3d')\nax2=fig.add_subplot(1,2,2,projection='3d')\nax1.elev=15\nax2.elev=15\nax1.azim=75\nax2.azim=75\nax1.scatter(w0,w1,loss1,s=0.1)\nax2.scatter(w0,w1,loss2,s=0.1)\n\n<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f30bc226350>\n\n\n\n\n\n\n왼쪽곡면(MSEloss)보다 오른쪽곡면(BCEloss)이 좀더 예쁘게 생김 -> 오른쪽 곡면에서 더 학습이 잘될것 같음\n\n\n\n학습과정 시각화예시1\n- 파라메터학습과정 시각화 // 옵티마이저: SGD, 초기값: (w0,w1) = (-3.0,-1.0)\n\n데이터정리\n\n\nX = tf.concat([tf.ones(N,dtype=tf.float64).reshape(N,1),x],axis=1)\nX\n\n<tf.Tensor: shape=(2000, 2), dtype=float64, numpy=\narray([[ 1.       , -1.       ],\n       [ 1.       , -0.9989995],\n       [ 1.       , -0.997999 ],\n       ...,\n       [ 1.       ,  0.997999 ],\n       [ 1.       ,  0.9989995],\n       [ 1.       ,  1.       ]])>\n\n\n\n1ter돌려봄\n\n\nnet_mse = tf.keras.Sequential()\nnet_mse.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid'))\nnet_mse.compile(optimizer=tf.optimizers.SGD(0.1),loss=mseloss_fn)\nnet_mse.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 89ms/step - loss: 0.1675\n\n\n<keras.callbacks.History at 0x7f30bc293d30>\n\n\n\nnet_bce = tf.keras.Sequential()\nnet_bce.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid'))\nnet_bce.compile(optimizer=tf.optimizers.SGD(0.1),loss=bceloss_fn)\nnet_bce.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 97ms/step - loss: 0.8913\n\n\n<keras.callbacks.History at 0x7f30bc103d60>\n\n\n\nnet_mse.get_weights(), net_bce.get_weights()\n\n([array([[-0.8381598],\n         [ 1.1136571]], dtype=float32)],\n [array([[ 0.6664642],\n         [-0.3024917]], dtype=float32)])\n\n\n\nnet_mse.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)])\nnet_bce.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)])\n\n\nnet_mse.get_weights(), net_bce.get_weights()\n\n([array([[-3.],\n         [-1.]], dtype=float32)],\n [array([[-3.],\n         [-1.]], dtype=float32)])\n\n\n\n학습과정기록: 15에폭마다 기록\n\n\nWhat_mse = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)\nWhat_bce = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)\n\n\nfor k in range(29):\n    net_mse.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    net_bce.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    What_mse = tf.concat([What_mse,net_mse.weights[0]],axis=1)\n    What_bce = tf.concat([What_bce,net_bce.weights[0]],axis=1)\n\n\n시각화\n\n\nfrom matplotlib import animation\nplt.rcParams[\"animation.html\"] = \"jshtml\"\n\n\nfig = plt.figure()\nfig.set_figwidth(6)\nfig.set_figheight(6)\nfig.suptitle(\"SGD, Winit=(-3,-1)\")\nax1=fig.add_subplot(2,2,1,projection='3d')\nax2=fig.add_subplot(2,2,2,projection='3d')\nax1.elev=15;ax2.elev=15;ax1.azim=75;ax2.azim=75\nax3=fig.add_subplot(2,2,3)\nax4=fig.add_subplot(2,2,4)\n\nax1.scatter(w0,w1,loss1,s=0.1);ax1.scatter(-1,5,loss_fn1(-1,5),color='red',marker='*',s=200)\nax2.scatter(w0,w1,loss2,s=0.1);ax2.scatter(-1,5,loss_fn2(-1,5),color='red',marker='*',s=200)\n\nax3.plot(x,y,','); ax3.plot(x,v,'--r'); \nline3, = ax3.plot(x,1/(1+np.exp(-X@What_mse[:,0])),'--b')\nax4.plot(x,y,','); ax4.plot(x,v,'--r')\nline4, = ax4.plot(x,1/(1+np.exp(-X@What_bce[:,0])),'--b')\n\ndef animate(i):\n    _w0_mse,_w1_mse = What_mse[:,i]\n    _w0_bce,_w1_bce = What_bce[:,i]\n    ax1.scatter(_w0_mse, _w1_mse, loss_fn1(_w0_mse, _w1_mse),color='gray')\n    ax2.scatter(_w0_bce, _w1_bce, loss_fn2(_w0_bce, _w1_bce),color='gray')\n    line3.set_ydata(1/(1+np.exp(-X@What_mse[:,i])))\n    line4.set_ydata(1/(1+np.exp(-X@What_bce[:,i])))\n\nani = animation.FuncAnimation(fig, animate, frames=30)\nplt.close()\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n학습과정 시각화예시2\n- 파라메터학습과정 시각화 // 옵티마이저: Adam, 초기값: (w0,w1) = (-3.0,-1.0)\n\n데이터정리\n\n\nX = tf.concat([tf.ones(N,dtype=tf.float64).reshape(N,1),x],axis=1)\nX\n\n<tf.Tensor: shape=(2000, 2), dtype=float64, numpy=\narray([[ 1.       , -1.       ],\n       [ 1.       , -0.9989995],\n       [ 1.       , -0.997999 ],\n       ...,\n       [ 1.       ,  0.997999 ],\n       [ 1.       ,  0.9989995],\n       [ 1.       ,  1.       ]])>\n\n\n\n1ter돌려봄\n\n\nnet_mse = tf.keras.Sequential()\nnet_mse.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid'))\nnet_mse.compile(optimizer=tf.optimizers.Adam(0.1),loss=mseloss_fn)\nnet_mse.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 103ms/step - loss: 0.1665\n\n\n<keras.callbacks.History at 0x7f30bc190970>\n\n\n\nnet_bce = tf.keras.Sequential()\nnet_bce.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid'))\nnet_bce.compile(optimizer=tf.optimizers.Adam(0.1),loss=bceloss_fn)\nnet_bce.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 111ms/step - loss: 0.5997\n\n\n<keras.callbacks.History at 0x7f309451c310>\n\n\n\nnet_mse.get_weights(), net_bce.get_weights()\n\n([array([[-0.9536586],\n         [ 1.453682 ]], dtype=float32)],\n [array([[0.33676884],\n         [0.9878915 ]], dtype=float32)])\n\n\n\nnet_mse.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)])\nnet_bce.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)])\n\n\nnet_mse.get_weights(), net_bce.get_weights()\n\n([array([[-3.],\n         [-1.]], dtype=float32)],\n [array([[-3.],\n         [-1.]], dtype=float32)])\n\n\n\n학습과정기록: 15에폭마다 기록\n\n\nWhat_mse = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)\nWhat_bce = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)\n\n\nfor k in range(29): \n    net_mse.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    net_bce.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    What_mse = tf.concat([What_mse,net_mse.weights[0]],axis=1) \n    What_bce = tf.concat([What_bce,net_bce.weights[0]],axis=1)\n\n\n시각화\n\n\nfrom matplotlib import animation\nplt.rcParams[\"animation.html\"] = \"jshtml\"\n\n\nfig = plt.figure()\nfig.set_figwidth(6)\nfig.set_figheight(6)\nfig.suptitle(\"Adam, Winit=(-3,-1)\")\nax1=fig.add_subplot(2,2,1,projection='3d')\nax2=fig.add_subplot(2,2,2,projection='3d')\nax1.elev=15;ax2.elev=15;ax1.azim=75;ax2.azim=75\nax3=fig.add_subplot(2,2,3)\nax4=fig.add_subplot(2,2,4)\n\nax1.scatter(w0,w1,loss1,s=0.1);ax1.scatter(-1,5,loss_fn1(-1,5),color='red',marker='*',s=200)\nax2.scatter(w0,w1,loss2,s=0.1);ax2.scatter(-1,5,loss_fn2(-1,5),color='red',marker='*',s=200)\n\nax3.plot(x,y,','); ax3.plot(x,v,'--r'); \nline3, = ax3.plot(x,1/(1+np.exp(-X@What_mse[:,0])),'--b')\nax4.plot(x,y,','); ax4.plot(x,v,'--r')\nline4, = ax4.plot(x,1/(1+np.exp(-X@What_bce[:,0])),'--b')\n\ndef animate(i):\n    _w0_mse,_w1_mse = What_mse[:,i]\n    _w0_bce,_w1_bce = What_bce[:,i]\n    ax1.scatter(_w0_mse, _w1_mse, loss_fn1(_w0_mse, _w1_mse),color='gray')\n    ax2.scatter(_w0_bce, _w1_bce, loss_fn2(_w0_bce, _w1_bce),color='gray')\n    line3.set_ydata(1/(1+np.exp(-X@What_mse[:,i])))\n    line4.set_ydata(1/(1+np.exp(-X@What_bce[:,i])))\n\nani = animation.FuncAnimation(fig, animate, frames=30)\nplt.close()\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n학습과정 시각화예시3\n- 파라메터학습과정 시각화 // 옵티마이저: Adam, 초기값: (w0,w1) = (-10.0,-1.0)\n\n데이터정리\n\n\nX = tf.concat([tf.ones(N,dtype=tf.float64).reshape(N,1),x],axis=1)\nX\n\n<tf.Tensor: shape=(2000, 2), dtype=float64, numpy=\narray([[ 1.       , -1.       ],\n       [ 1.       , -0.9989995],\n       [ 1.       , -0.997999 ],\n       ...,\n       [ 1.       ,  0.997999 ],\n       [ 1.       ,  0.9989995],\n       [ 1.       ,  1.       ]])>\n\n\n\n1ter돌려봄\n\n\nnet_mse = tf.keras.Sequential()\nnet_mse.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid'))\nnet_mse.compile(optimizer=tf.optimizers.Adam(0.1),loss=mseloss_fn)\nnet_mse.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 99ms/step - loss: 0.3884\n\n\n<keras.callbacks.History at 0x7f308c765ba0>\n\n\n\nnet_bce = tf.keras.Sequential()\nnet_bce.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid'))\nnet_bce.compile(optimizer=tf.optimizers.Adam(0.1),loss=bceloss_fn)\nnet_bce.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 107ms/step - loss: 0.9309\n\n\n<keras.callbacks.History at 0x7f30945c6530>\n\n\n\nnet_mse.get_weights(), net_bce.get_weights()\n\n([array([[ 0.8862327 ],\n         [-0.39477217]], dtype=float32)],\n [array([[ 0.08077961],\n         [-0.770376  ]], dtype=float32)])\n\n\n\nnet_mse.set_weights([tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32)])\nnet_bce.set_weights([tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32)])\n\n\nnet_mse.get_weights(), net_bce.get_weights()\n\n([array([[-10.],\n         [ -1.]], dtype=float32)],\n [array([[-10.],\n         [ -1.]], dtype=float32)])\n\n\n\n학습과정기록: 15에폭마다 기록\n\n\nWhat_mse = tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32)\nWhat_bce = tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32)\n\n\nfor k in range(29):\n    net_mse.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    net_bce.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    What_mse = tf.concat([What_mse,net_mse.weights[0]],axis=1)\n    What_bce = tf.concat([What_bce,net_bce.weights[0]],axis=1)\n\n\n시각화\n\n\nfrom matplotlib import animation\nplt.rcParams[\"animation.html\"] = \"jshtml\"\n\n\nfig = plt.figure()\nfig.set_figwidth(6)\nfig.set_figheight(6)\nfig.suptitle(\"Adam, Winit=(-10,-1)\")\nax1=fig.add_subplot(2,2,1,projection='3d')\nax2=fig.add_subplot(2,2,2,projection='3d')\nax1.elev=15;ax2.elev=15;ax1.azim=75;ax2.azim=75\nax3=fig.add_subplot(2,2,3)\nax4=fig.add_subplot(2,2,4)\n\nax1.scatter(w0,w1,loss1,s=0.1);ax1.scatter(-1,5,loss_fn1(-1,5),color='red',marker='*',s=200)\nax2.scatter(w0,w1,loss2,s=0.1);ax2.scatter(-1,5,loss_fn2(-1,5),color='red',marker='*',s=200)\n\nax3.plot(x,y,','); ax3.plot(x,v,'--r'); \nline3, = ax3.plot(x,1/(1+np.exp(-X@What_mse[:,0])),'--b')\nax4.plot(x,y,','); ax4.plot(x,v,'--r')\nline4, = ax4.plot(x,1/(1+np.exp(-X@What_bce[:,0])),'--b')\n\ndef animate(i):\n    _w0_mse,_w1_mse = What_mse[:,i]\n    _w0_bce,_w1_bce = What_bce[:,i]\n    ax1.scatter(_w0_mse, _w1_mse, loss_fn1(_w0_mse, _w1_mse),color='gray')\n    ax2.scatter(_w0_bce, _w1_bce, loss_fn2(_w0_bce, _w1_bce),color='gray')\n    line3.set_ydata(1/(1+np.exp(-X@What_mse[:,i])))\n    line4.set_ydata(1/(1+np.exp(-X@What_bce[:,i])))\n\nani = animation.FuncAnimation(fig, animate, frames=30)\nplt.close()\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n아무리 아담이라고 해도 이건 힘듬\n\n- discussion - mse_loss는 경우에 따라서 엄청 수렴속도가 느릴수도 있음. - 근본적인 문제점: mse_loss일 경우 loss function의 곡면이 예쁘지 않음. (전문용어로 convex가 아니라고 말함) - 좋은 옵티마지어를 이용하면 mse_loss일 경우에도 수렴속도를 올릴 수 있음 (학습과정 시각화예시2).그렇지만 이는 근본적인 해결책은 아님. (학습과정 시각화예시3)\n- 요약: 왜 logistic regression에서 mse loss를 쓰면 안되는가? - mse loss를 사용하면 손실함수가 convex하지 않으니까! - 그리고 bce loss를 사용하면 손실함수가 convex하니까!"
  },
  {
    "objectID": "posts/2022-04-28-중간고사예상문제_빅데이터분석특강.html",
    "href": "posts/2022-04-28-중간고사예상문제_빅데이터분석특강.html",
    "title": "jisim12",
    "section": "",
    "text": "빅데이터분석특강\n\n\ntoc:false\nbranch: master\nbadges: true\ncomments: true\nauthor: 심재인"
  },
  {
    "objectID": "posts/2022-04-28-중간고사예상문제_빅데이터분석특강.html#경사하강법과-tf.gradienttape의-사용방법-30점",
    "href": "posts/2022-04-28-중간고사예상문제_빅데이터분석특강.html#경사하강법과-tf.gradienttape의-사용방법-30점",
    "title": "jisim12",
    "section": "1. 경사하강법과 tf.GradientTape()의 사용방법 (30점)",
    "text": "1. 경사하강법과 tf.GradientTape()의 사용방법 (30점)\n(1) 아래는 \\(X_i \\overset{iid}{\\sim} N(3,2^2)\\) 를 생성하는 코드이다. (10점)\n\ntf.random.set_seed(43052)\nx= tnp.random.randn(10000)*2+3\nx\n\n2022-04-25 18:15:05.074138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n<tf.Tensor: shape=(10000,), dtype=float64, numpy=\narray([ 4.12539849,  5.46696729,  5.27243374, ...,  2.89712332,\n        5.01072291, -1.13050477])>\n\n\n함수 \\(L(\\mu,\\sigma)\\)을 최대화하는 \\((\\mu,\\sigma)\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\(\\mu\\)의 초기값은 2로 \\(\\sigma\\)의 초기값은 3으로 설정할 것)\n\\[L(\\mu,\\sigma)=\\prod_{i=1}^{n}f(x_i), \\quad f(x_i)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x_i-\\mu}{\\sigma})^2}\\]\nhint: \\(L(\\mu,\\sigma)\\)를 최대화하는 \\((\\mu,\\sigma)\\)는 \\(\\log L(\\mu,\\sigma)\\)를 역시 최대화한다는 사실을 이용할 것.\nhint: \\(\\mu\\)의 참값은 3, \\(\\sigma\\)의 참값은 2이다. (따라서 \\(\\mu\\)와 \\(\\sigma\\)는 각각 2와 3근처로 추정되어야 한다.)\n- 풀이\n\nN = 10000\n\n\ny_true=(x-3)**2/2**2\n\n\nepsilon = tnp.random.randn(N)*0.5\ny=(x-3)**2/2**2+epsilon\n\n\nx.shape, y.shape\n\n(TensorShape([10000]), TensorShape([10000]))\n\n\n\nbeta = tf.Variable(2.0)\nalpha = tf.Variable(3.0)\n\n\nfor epoc in range(1000):   \n    with tf.GradientTape() as tape: \n        yhat = (x-beta)**2/(alpha**2)\n        loss = tf.reduce_sum((y-yhat)**2)/N \n    slope0,slope1 = tape.gradient(loss,[beta,alpha]) \n    beta.assign_sub(alpha * slope0) \n    alpha.assign_sub(alpha * slope1) \n\n\nbeta, alpha\n\n(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=161.78185>,\n <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=159.62784>)\n\n\n\nyhat=(x-beta)**2/(alpha**2)\n\n\nplt.plot(x,y,'.')\nplt.plot(x,yhat,'r.')\n\n\n\n\n(2)\n(3)"
  },
  {
    "objectID": "posts/2022-04-28-중간고사예상문제_빅데이터분석특강.html#회귀분석의-이론적해와-tf.keras.optimizer-이용방법-20점",
    "href": "posts/2022-04-28-중간고사예상문제_빅데이터분석특강.html#회귀분석의-이론적해와-tf.keras.optimizer-이용방법-20점",
    "title": "jisim12",
    "section": "2. 회귀분석의 이론적해와 tf.keras.optimizer 이용방법 (20점)",
    "text": "2. 회귀분석의 이론적해와 tf.keras.optimizer 이용방법 (20점)\n아래와 같은 선형모형을 고려하자.\n\\[y_i = \\beta_0 + \\beta_1 x_i +\\epsilon_i.\\]\n이때 오차항은 정규분포로 가정한다. 즉 \\(\\epsilon_i \\overset{iid}{\\sim} N(0,\\sigma^2)\\)라고 가정한다.\n관측데이터가 아래와 같을때 아래의 물음에 답하라.\n\nx= tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4])\ny= tnp.array([55.4183651 , 58.19427589, 61.23082496, 62.31255873, 63.1070028 , \n              63.69569103, 67.24704918, 71.43650092, 73.10130336, 77.84988286])\n# X= tnp.array([[1.0, 20.1], [1.0, 22.2], [1.0, 22.7], [1.0, 23.3], [1.0, 24.4],\n#               [1.0, 25.1], [1.0, 26.2], [1.0, 27.3], [1.0, 28.4], [1.0, 30.4]])\n\n(1) MSE loss를 최소화 하는 \\(\\beta_0,\\beta_1\\)의 해석해를 구하라.\n-풀이\n\nN = 10\n\n\ny = y.reshape(N,1)\n\n\nX = tf.stack([tf.ones(N,dtype='float64'),x],axis=1)\n\n\ny=y.reshape(N,1)\nx=x.reshape(N,1)\ny.shape, X.shape\n\n(TensorShape([10, 1]), TensorShape([10, 2]))\n\n\n\ntf.linalg.inv(X.T @ X) @ X.T @ y\n\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[9.94457323],\n       [2.21570461]])>\n\n\n\ny_hat=9.94457323+2.21570461*x\n\n\nplt.plot(x,y,'.')\nplt.plot(x,y_hat,'r--')\n\n\n\n\n(2) 경사하강법과 MSE loss의 도함수를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\n주의 tf.GradeintTape()를 이용하지 말고 MSE loss의 해석적 도함수를 사용할 것.\n-풀이\n\nN = 10\n\n\ny=y.reshape(N,1)\nX=tf.stack([tf.ones(N,dtype=tf.float64),x],axis=1)\ny.shape,X.shape\n\nInvalidArgumentError: Shapes of all inputs must match: values[0].shape = [10] != values[1].shape = [10,1] [Op:Pack] name: stack\n\n\n\ny=y.reshape(N,1)\nX.shape, y.shape\n\n(TensorShape([10, 2]), TensorShape([10, 1]))\n\n\n\nbeta_hat = tnp.array([9,2]).reshape(2,1)\nbeta_hat\n\n<tf.Tensor: shape=(2, 1), dtype=int64, numpy=\narray([[9],\n       [2]])>\n\n\n\nalpha = 0.001\n\n\nfor epoc in range(1000):\n    slope = (-2*X.T @ y + 2*X.T@ X @ beta_hat)/N\n    beta_hat = beta_hat - alpha * slope\n\n\nbeta_hat\n\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[9.03545357],\n       [2.25155218]])>\n\n\n\ny_hat=9.03545357+2.25155218*x\n\n\nplt.plot(x,y,'.')\nplt.plot(x,y_hat,'r--')\n\n\n\n\n(3) tf.keras.optimizers의 apply_gradients()를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\n-풀이\n\nopt.apply_gradients([(slope,beta_hat)])\nbeta\n\nNameError: name 'opt' is not defined\n\n\n\nX.shape,y.shape\n\n(TensorShape([10, 2]), TensorShape([10, 1]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([9,2],dtype='float64').reshape(2,1))\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[9.],\n       [2.]])>\n\n\n\nalpha=0.001\nopt = tf.keras.optimizers.SGD(alpha)\n\nNameError: name 'tf' is not defined\n\n\n\nfor epoc in range(1000): \n    with tf.GradientTape() as tape: \n        yhat = X@beta_hat\n        loss = (y-yhat).T @ (y-yhat) / N \n    slope = tape.gradient(loss,beta_hat)  \n    opt.apply_gradients( [(slope,beta_hat)] )\n\n\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[9.03545358],\n       [2.25155218]])>\n\n\n\ny_hat=9.03545358+2.25155218*x\n\n\nplt.plot(x,y,'.')\nplt.plot(x,y_hat,'r--')\n\n\n\n\n(4) tf.keras.optimizers의 minimize()를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\n- 풀이\n\nmse_fn = tf.losses.MeanSquaredError()\nmse_fn(y,yhat)\n\n<tf.Tensor: shape=(), dtype=float64, numpy=0.9609637260437012>\n\n\n\nmseloss_fn=tf.losses.MeanSquaredError()\n\n\ny=y.reshape(N,1)\nX.shape,y.shape\n\n(TensorShape([10, 2]), TensorShape([10, 1]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([9,2],dtype='float64').reshape(2,1))\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[9.],\n       [2.]])>\n\n\n\nalpha=0.0015\nopt = tf.keras.optimizers.SGD(alpha) \n\n\nmseloss_fn(y.reshape(-1),yhat.reshape(-1))\n\n<tf.Tensor: shape=(), dtype=float64, numpy=0.960963785648346>\n\n\n\ndef loss_fn():\n    yhat= X@beta_hat\n    loss = mseloss_fn(y.reshape(-1),yhat.reshape(-1))\n    return loss\n\n\nfor epoc in range(1000):\n    opt.minimize(loss_fn,beta_hat)\n\n\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[9.04793931],\n       [2.25105986]])>\n\n\n\ny_hat=9.03545358+2.25155218*x\n\n\nplt.plot(x,y,'.')\nplt.plot(x,y_hat,'r--')\n\n\n\n\nhint1 alpha=0.0015로 설정할 것\nhint2 epoc은 10000번정도 반복실행하며 적당한 횟수를 찾을 것\nhint3 (1)의 최적값에 반드시 정확히 수렴시킬 필요는 없음 (너무 많은 에폭이 소모됨)\nhint4 초기값으로 [5,10] 정도 이용할 것"
  },
  {
    "objectID": "posts/2022-04-28-중간고사예상문제_빅데이터분석특강.html#keras를-이용한-풀이-30점",
    "href": "posts/2022-04-28-중간고사예상문제_빅데이터분석특강.html#keras를-이용한-풀이-30점",
    "title": "jisim12",
    "section": "3. keras를 이용한 풀이 (30점)",
    "text": "3. keras를 이용한 풀이 (30점)\n(1) 아래와 같은 모형을 고려하자.\n\\[y_i= \\beta_0 + \\sum_{k=1}^{5} \\beta_k \\cos(k t_i)+\\epsilon_i, \\quad i=0,1,\\dots, 999\\]\n여기에서 \\(t_i=\\frac{2\\pi i}{1000}\\) 이다. 그리고 \\(\\epsilon_i \\sim i.i.d~ N(0,\\sigma^2)\\), 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자.\n\nnp.random.seed(43052)\nt= np.array(range(1000))* np.pi/1000\ny = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.1\nplt.plot(t,y,'.',alpha=0.1)\n\n\n\n\ntf.keras를 이용하여 \\(\\beta_0,\\dots,\\beta_5\\)를 추정하라. (\\(\\beta_0,\\dots,\\beta_5\\)의 참값은 각각 -2,3,1,0,0,0.5 이다)\n\n6주차 keras 예제3번 활용\n\n\nX = np.stack([np.ones(1000),np.cos(1*t),np.cos(2*t),np.cos(3*t),np.cos(4*t),np.cos(5*t)],axis=1)\ny = y.reshape(1000,1)\n\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1,use_bias=False))\nnet.compile(tf.optimizers.SGD(0.1), loss='mse')\nnet.fit(X,y,epochs=30, batch_size=N)\n\nEpoch 1/30\n100/100 [==============================] - 0s 711us/step - loss: 0.4868\nEpoch 2/30\n100/100 [==============================] - 0s 852us/step - loss: 0.0094\nEpoch 3/30\n100/100 [==============================] - 0s 789us/step - loss: 0.0095\nEpoch 4/30\n100/100 [==============================] - 0s 747us/step - loss: 0.0094\nEpoch 5/30\n100/100 [==============================] - 0s 744us/step - loss: 0.0095\nEpoch 6/30\n100/100 [==============================] - 0s 713us/step - loss: 0.0094\nEpoch 7/30\n100/100 [==============================] - 0s 721us/step - loss: 0.0092\nEpoch 8/30\n100/100 [==============================] - 0s 788us/step - loss: 0.0093\nEpoch 9/30\n100/100 [==============================] - 0s 759us/step - loss: 0.0093\nEpoch 10/30\n100/100 [==============================] - 0s 729us/step - loss: 0.0095\nEpoch 11/30\n100/100 [==============================] - 0s 795us/step - loss: 0.0093\nEpoch 12/30\n100/100 [==============================] - 0s 742us/step - loss: 0.0093\nEpoch 13/30\n100/100 [==============================] - 0s 725us/step - loss: 0.0093\nEpoch 14/30\n100/100 [==============================] - 0s 761us/step - loss: 0.0095\nEpoch 15/30\n100/100 [==============================] - 0s 690us/step - loss: 0.0094\nEpoch 16/30\n100/100 [==============================] - 0s 699us/step - loss: 0.0093\nEpoch 17/30\n100/100 [==============================] - 0s 598us/step - loss: 0.0095\nEpoch 18/30\n100/100 [==============================] - 0s 612us/step - loss: 0.0094\nEpoch 19/30\n100/100 [==============================] - 0s 696us/step - loss: 0.0095\nEpoch 20/30\n100/100 [==============================] - 0s 661us/step - loss: 0.0095\nEpoch 21/30\n100/100 [==============================] - 0s 687us/step - loss: 0.0092\nEpoch 22/30\n100/100 [==============================] - 0s 671us/step - loss: 0.0094\nEpoch 23/30\n100/100 [==============================] - 0s 691us/step - loss: 0.0095\nEpoch 24/30\n100/100 [==============================] - 0s 706us/step - loss: 0.0094\nEpoch 25/30\n100/100 [==============================] - 0s 695us/step - loss: 0.0094\nEpoch 26/30\n100/100 [==============================] - 0s 706us/step - loss: 0.0095\nEpoch 27/30\n100/100 [==============================] - 0s 683us/step - loss: 0.0095\nEpoch 28/30\n100/100 [==============================] - 0s 743us/step - loss: 0.0094\nEpoch 29/30\n100/100 [==============================] - 0s 800us/step - loss: 0.0094\nEpoch 30/30\n100/100 [==============================] - 0s 797us/step - loss: 0.0095\n\n\n<keras.callbacks.History at 0x7f5b7c2e5e10>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense/kernel:0' shape=(6, 1) dtype=float32, numpy=\n array([[-1.9899189e+00],\n        [ 3.0180848e+00],\n        [ 1.0091044e+00],\n        [-1.8584070e-03],\n        [-4.8465207e-03],\n        [ 4.9381223e-01]], dtype=float32)>]\n\n\n(2) 아래와 같은 모형을 고려하자.\n\\[y_i \\sim Ber(\\pi_i), ~ \\text{where} ~ \\pi_i=\\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\]\n위의 모형에서 관측한 데이터는 아래와 같다.\n\ntf.random.set_seed(43052)\nx = tnp.linspace(-1,1,2000)\ny = tf.constant(np.random.binomial(1, tf.nn.sigmoid(-1+5*x)),dtype=tf.float64)\nplt.plot(x,y,'.',alpha=0.05)\n\n\n\n\ntf.keras를 이용하여 \\(w_0,w_1\\)을 추정하라. (참고: \\(w_0, w_1\\)에 대한 참값은 -1과 5이다.)\n\n7주차 Logistic regression 예제 참고\n\n\nx.shape, y.shape\n\n(TensorShape([2000]), TensorShape([2000]))\n\n\n\nx=x.reshape(2000,1)\nx.shape, y.shape\n\n(TensorShape([2000, 1]), TensorShape([2000]))\n\n\n\nnet = tf.keras.Sequential() \nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nbceloss_fn = lambda y,yhat: -tf.reduce_mean(y*tnp.log(yhat) + (1-y)*tnp.log(1-yhat))\nnet.compile(loss=bceloss_fn, optimizer=tf.optimizers.SGD(0.1))\nnet.fit(x,y,epochs=10000,verbose=0,batch_size=2000)\n\n<keras.callbacks.History at 0x7f5d075eed10>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense_1/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[5.09306]], dtype=float32)>,\n <tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32, numpy=array([-1.0963831], dtype=float32)>]\n\n\n\nplt.plot(x,y,'.',alpha=0.1)\nplt.plot(x,net(x),'--b')"
  },
  {
    "objectID": "posts/2022-04-28-중간고사예상문제_빅데이터분석특강.html#piecewise-linear-regression-15점",
    "href": "posts/2022-04-28-중간고사예상문제_빅데이터분석특강.html#piecewise-linear-regression-15점",
    "title": "jisim12",
    "section": "4. Piecewise-linear regression (15점)",
    "text": "4. Piecewise-linear regression (15점)"
  },
  {
    "objectID": "posts/2022-04-28-중간고사예상문제_빅데이터분석특강.html#다음을-잘-읽고-참과-거짓을-판단하라.-5점",
    "href": "posts/2022-04-28-중간고사예상문제_빅데이터분석특강.html#다음을-잘-읽고-참과-거짓을-판단하라.-5점",
    "title": "jisim12",
    "section": "5. 다음을 잘 읽고 참과 거짓을 판단하라. (5점)",
    "text": "5. 다음을 잘 읽고 참과 거짓을 판단하라. (5점)\n(1) 적절한 학습률이 선택된다면, 경사하강법은 손실함수가 convex일때 언제 전역최소해를 찾을 수 있다.\n(2)\n(3)\n(4)\n(5)\n\nsome notes\n- 용어를 모르겠는 분은 질문하시기 바랍니다.\n- 풀다가 에러나는 코드 질문하면 에러 수정해드립니다."
  },
  {
    "objectID": "posts/2022-04-29-심재인_빅데이터분석특강_중간고사.html",
    "href": "posts/2022-04-29-심재인_빅데이터분석특강_중간고사.html",
    "title": "jisim12",
    "section": "",
    "text": "import numpy as np\nimport tensorflow as tf \nimport tensorflow.experimental.numpy as tnp \n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport matplotlib.pyplot as plt \n\n\n\n(1) 아래는 \\(X_i \\overset{iid}{\\sim} N(3,2^2)\\) 를 생성하는 코드이다.\n\ntf.random.set_seed(43052)\nx= tnp.random.randn(10000)*2+3\nx\n\n2022-04-25 20:47:56.696349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n<tf.Tensor: shape=(10000,), dtype=float64, numpy=\narray([ 4.12539849,  5.46696729,  5.27243374, ...,  2.89712332,\n        5.01072291, -1.13050477])>\n\n\n함수 \\(L(\\mu,\\sigma)\\)을 최대화하는 \\((\\mu,\\sigma)\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\(\\mu\\)의 초기값은 2로 \\(\\sigma\\)의 초기값은 3으로 설정할 것)\n\\[L(\\mu,\\sigma)=\\prod_{i=1}^{n}f(x_i), \\quad f(x_i)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x_i-\\mu}{\\sigma})^2}\\]\n\nN = 10000\n\n\ny_true=(x-3)**2/2**2\n\n\nepsilon = tnp.random.randn(N)*0.5\ny=(x-3)**2/2**2+epsilon\n\n\nx.shape, y.shape\n\n(TensorShape([10000]), TensorShape([10000]))\n\n\n\nbeta = tf.Variable(2.0)\nalpha = tf.Variable(3.0)\n\n\nfor epoc in range(1000):   \n    with tf.GradientTape() as tape: \n        yhat = (x-beta)**2/(alpha**2)\n        loss = tf.reduce_sum((y-yhat)**2)/N \n    slope0,slope1 = tape.gradient(loss,[beta,alpha]) \n    beta.assign_sub(alpha * slope0) \n    alpha.assign_sub(alpha * slope1)\n\n\nbeta, alpha\n\n(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=161.78185>,\n <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=159.62784>)\n\n\n\nyhat=(x-beta)**2/(alpha**2)\n\n\nplt.plot(x,y,'.')\nplt.plot(x,yhat,'r.')\n\n\n\n\n(2) 아래는 \\(X_i \\overset{iid}{\\sim} Ber(0.8)\\)을 생성하는 코드이다.\n\ntf.random.set_seed(43052)\nx= tf.constant(np.random.binomial(1,0.8,(10000,)))\nx\n\n<tf.Tensor: shape=(10000,), dtype=int64, numpy=array([0, 1, 1, ..., 1, 0, 1])>\n\n\n함수 \\(L(p)\\)을 최대화하는 \\(p\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\(p\\)의 초기값은 0.3으로 설정할 것)\n\\[L(\\mu,\\sigma)=\\prod_{i=1}^{n}f(x_i), \\quad f(x_i)=p^{x_i}(1-p)^{1-x_i}\\]\n\nN = 10000\n\n\ny_true=(x-0.8)**2\n\n\nepsilon = tnp.random.randn(N)*0.5\ny=(x-0.8)**2+epsilon\n\n\nx.shape, y.shape\n\n(TensorShape([10000]), TensorShape([10000]))\n\n\n\nbeta = tf.Variable(2.0)\nalpha = tf.Variable(3.0)\n\n\nfor epoc in range(1000):   \n    with tf.GradientTape() as tape: \n        yhat = (x-beta)**2/(alpha**2)\n        loss = tf.reduce_sum((y-yhat)**2)/N \n    slope0,slope1 = tape.gradient(loss,[beta,alpha]) \n    beta.assign_sub(alpha * slope0) \n    alpha.assign_sub(alpha * slope1)\n\n\nbeta, alpha\n\n(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.3362731>,\n <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.6516612>)\n\n\n\nyhat=(x-beta)**2/(alpha**2)\n\n\nplt.plot(x,y,'.')\nplt.plot(x,yhat,'r.')\n\n\n\n\n(3) 아래의 모형에 따라서 \\(\\{Y_i\\}_{i=1}^{10000}\\)를 생성하는 코드를 작성하라. - \\(Y_i \\overset{iid}{\\sim} N(\\mu_i,1)\\) - \\(\\mu_i = \\beta_0 + \\beta_1 x_i = 0.5 + 2 x_i\\) , where \\(x_i = \\frac{i}{10000}\\)\n함수 \\(L(\\beta_0,\\beta_1)\\)을 최대화하는 \\((\\beta_0,\\beta_1)\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\(\\beta_0,\\beta_1\\)의 초기값은 모두 1로 설정할 것)\n\\[L(\\beta_0,\\beta_1)=\\prod_{i=1}^{n}f(y_i), \\quad f(y_i)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(y_i-\\mu_i)^2}, \\quad \\mu_i=\\beta_0+\\beta_1 x_i\\]\n\n\n\n아래와 같은 선형모형을 고려하자.\n\\[y_i = \\beta_0 + \\beta_1 x_i +\\epsilon_i.\\]\n이때 오차항은 정규분포로 가정한다. 즉 \\(\\epsilon_i \\overset{iid}{\\sim} N(0,\\sigma^2)\\)라고 가정한다.\n관측데이터가 아래와 같을때 아래의 물음에 답하라.\n\nx= tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4])\ny= tnp.array([55.4183651 , 58.19427589, 61.23082496, 62.31255873, 63.1070028 , \n              63.69569103, 67.24704918, 71.43650092, 73.10130336, 77.84988286])\n# X= tnp.array([[1.0, 20.1], [1.0, 22.2], [1.0, 22.7], [1.0, 23.3], [1.0, 24.4],\n#               [1.0, 25.1], [1.0, 26.2], [1.0, 27.3], [1.0, 28.4], [1.0, 30.4]])\n\n(1) MSE loss를 최소화 하는 \\(\\beta_0,\\beta_1\\)의 해석해를 구하라.\n\nN = 10\n\n\ny = y.reshape(N,1)\n\n\nX = tf.stack([tf.ones(N,dtype='float64'),x],axis=1)\n\n\ny=y.reshape(N,1)\nx=x.reshape(N,1)\ny.shape, X.shape\n\n(TensorShape([10, 1]), TensorShape([10, 2]))\n\n\n\ntf.linalg.inv(X.T @ X) @ X.T @ y\n\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[9.94457323],\n       [2.21570461]])>\n\n\n\ny_hat=9.94457323+2.21570461*x\n\n\nplt.plot(x,y,'.')\nplt.plot(x,y_hat,'r--')\n\n\n\n\n(2) 경사하강법과 MSE loss의 도함수를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\n주의 tf.GradeintTape()를 이용하지 말고 MSE loss의 해석적 도함수를 사용할 것.\n\nN = 10\n\n\ny=y.reshape(N,1)\nX=tf.stack([tf.ones(N,dtype=tf.float64),x],axis=1)\ny.shape,X.shape\n\n(TensorShape([10, 1]), TensorShape([10, 2]))\n\n\n\ny=y.reshape(N,1)\nX.shape, y.shape\n\n(TensorShape([10, 2]), TensorShape([10, 1]))\n\n\n\nbeta_hat = tnp.array([9,2]).reshape(2,1)\nbeta_hat\n\n<tf.Tensor: shape=(2, 1), dtype=int64, numpy=\narray([[9],\n       [2]])>\n\n\n\nalpha = 0.001\n\n\nfor epoc in range(1000):\n    slope = (-2*X.T @ y + 2*X.T@ X @ beta_hat)/N\n    beta_hat = beta_hat - alpha * slope\n\n\nbeta_hat\n\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[9.03545357],\n       [2.25155218]])>\n\n\n\ny_hat=9.03545357+2.25155218*x\n\n\nplt.plot(x,y,'.')\nplt.plot(x,y_hat,'r--')\n\n\n\n\n(3) tf.keras.optimizers의 apply_gradients()를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\n\nopt.apply_gradients([(slope,beta_hat)])\nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.3362731>\n\n\n\nX.shape,y.shape\n\n(TensorShape([10, 2]), TensorShape([10, 1]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([9,2],dtype='float64').reshape(2,1))\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[9.],\n       [2.]])>\n\n\n\nalpha=0.05\nopt = tf.keras.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000): \n    with tf.GradientTape() as tape: \n        yhat = X@beta_hat\n        loss = (y-yhat).T @ (y-yhat) / N \n    slope = tape.gradient(loss,beta_hat)  \n    opt.apply_gradients( [(slope,beta_hat)] )\n\n\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[nan],\n       [nan]])>\n\n\n\ny_hat=9.03545358+2.25155218*x\n\n\nplt.plot(x,y,'.')\nplt.plot(x,y_hat,'r--')\n\n\n\n\n(4) tf.keras.optimizers의 minimize()를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\n\nmse_fn = tf.losses.MeanSquaredError()\nmse_fn(y,yhat)\n\n<tf.Tensor: shape=(), dtype=float64, numpy=nan>\n\n\n\nmseloss_fn=tf.losses.MeanSquaredError()\n\n\ny=y.reshape(N,1)\nX.shape,y.shape\n\n(TensorShape([10, 2]), TensorShape([10, 1]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([9,2],dtype='float64').reshape(2,1))\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[9.],\n       [2.]])>\n\n\n\nalpha=0.0015\nopt = tf.keras.optimizers.SGD(alpha) \n\n\nmseloss_fn(y.reshape(-1),yhat.reshape(-1))\n\n<tf.Tensor: shape=(), dtype=float64, numpy=nan>\n\n\n\ndef loss_fn():\n    yhat= X@beta_hat\n    loss = mseloss_fn(y.reshape(-1),yhat.reshape(-1))\n    return loss\n\n\nfor epoc in range(1000):\n    opt.minimize(loss_fn,beta_hat)\n\n\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[9.04793931],\n       [2.25105986]])>\n\n\n\ny_hat=9.03545358+2.25155218*x\n\n\nplt.plot(x,y,'.')\nplt.plot(x,y_hat,'r--')\n\n\n\n\n\n\n\n(1) 아래와 같은 모형을 고려하자.\n\\[y_i= \\beta_0 + \\sum_{k=1}^{5} \\beta_k \\cos(k t_i)+\\epsilon_i, \\quad i=0,1,\\dots, 999\\]\n여기에서 \\(t_i=\\frac{2\\pi i}{1000}\\) 이다. 그리고 \\(\\epsilon_i \\sim i.i.d~ N(0,\\sigma^2)\\), 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자.\n\nnp.random.seed(43052)\nt= np.array(range(1000))* np.pi/1000\ny = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2\nplt.plot(t,y,'.',alpha=0.2)\n\n\n\n\ntf.keras를 이용하여 \\(\\beta_0,\\dots,\\beta_5\\)를 추정하라. (\\(\\beta_0,\\dots,\\beta_5\\)의 참값은 각각 -2,3,1,0,0,0.5 이다)\n\nX = np.stack([np.ones(1000),np.cos(1*t),np.cos(2*t),np.cos(3*t),np.cos(4*t),np.cos(5*t)],axis=1)\ny = y.reshape(1000,1)\n\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1,use_bias=False))\nnet.compile(tf.optimizers.SGD(0.1), loss='mse')\nnet.fit(X,y,epochs=30, batch_size=N)\n\nEpoch 1/30\n100/100 [==============================] - 0s 790us/step - loss: 0.2753\nEpoch 2/30\n100/100 [==============================] - 0s 738us/step - loss: 0.0374\nEpoch 3/30\n100/100 [==============================] - 0s 709us/step - loss: 0.0378\nEpoch 4/30\n100/100 [==============================] - 0s 718us/step - loss: 0.0375\nEpoch 5/30\n100/100 [==============================] - 0s 710us/step - loss: 0.0378\nEpoch 6/30\n100/100 [==============================] - 0s 753us/step - loss: 0.0377\nEpoch 7/30\n100/100 [==============================] - 0s 806us/step - loss: 0.0369\nEpoch 8/30\n100/100 [==============================] - 0s 830us/step - loss: 0.0373\nEpoch 9/30\n100/100 [==============================] - 0s 831us/step - loss: 0.0372\nEpoch 10/30\n100/100 [==============================] - 0s 868us/step - loss: 0.0379\nEpoch 11/30\n100/100 [==============================] - 0s 831us/step - loss: 0.0372\nEpoch 12/30\n100/100 [==============================] - 0s 827us/step - loss: 0.0372\nEpoch 13/30\n100/100 [==============================] - 0s 846us/step - loss: 0.0374\nEpoch 14/30\n100/100 [==============================] - 0s 820us/step - loss: 0.0379\nEpoch 15/30\n100/100 [==============================] - 0s 926us/step - loss: 0.0375\nEpoch 16/30\n100/100 [==============================] - 0s 949us/step - loss: 0.0373\nEpoch 17/30\n100/100 [==============================] - 0s 833us/step - loss: 0.0379\nEpoch 18/30\n100/100 [==============================] - 0s 706us/step - loss: 0.0377\nEpoch 19/30\n100/100 [==============================] - 0s 700us/step - loss: 0.0380\nEpoch 20/30\n100/100 [==============================] - 0s 744us/step - loss: 0.0381\nEpoch 21/30\n100/100 [==============================] - 0s 689us/step - loss: 0.0370\nEpoch 22/30\n100/100 [==============================] - 0s 733us/step - loss: 0.0377\nEpoch 23/30\n100/100 [==============================] - 0s 733us/step - loss: 0.0379\nEpoch 24/30\n100/100 [==============================] - 0s 719us/step - loss: 0.0377\nEpoch 25/30\n100/100 [==============================] - 0s 681us/step - loss: 0.0377\nEpoch 26/30\n100/100 [==============================] - 0s 700us/step - loss: 0.0380\nEpoch 27/30\n100/100 [==============================] - 0s 718us/step - loss: 0.0381\nEpoch 28/30\n100/100 [==============================] - 0s 689us/step - loss: 0.0375\nEpoch 29/30\n100/100 [==============================] - 0s 763us/step - loss: 0.0376\nEpoch 30/30\n100/100 [==============================] - 0s 768us/step - loss: 0.0379\n\n\n<keras.callbacks.History at 0x7fe354554850>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense/kernel:0' shape=(6, 1) dtype=float32, numpy=\n array([[-1.9798379 ],\n        [ 3.0361695 ],\n        [ 1.0182086 ],\n        [-0.00371684],\n        [-0.00969306],\n        [ 0.4876245 ]], dtype=float32)>]\n\n\n(2) 아래와 같은 모형을 고려하자.\n\\[y_i \\sim Ber(\\pi_i), ~ \\text{where} ~ \\pi_i=\\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\]\n위의 모형에서 관측한 데이터는 아래와 같다.\n\ntf.random.set_seed(43052)\nx = tnp.linspace(-1,1,2000) \ny = tf.constant(np.random.binomial(1, tf.nn.sigmoid(-1+5*x)),dtype=tf.float64) \nplt.plot(x,y,'.',alpha=0.05)\n\n\n\n\ntf.keras를 이용하여 \\(w_0,w_1\\)을 추정하라. (참고: \\(w_0, w_1\\)에 대한 참값은 -1과 5이다.)\n\nx.shape, y.shape\n\n(TensorShape([2000]), TensorShape([2000]))\n\n\n\nx=x.reshape(2000,1)\nx.shape, y.shape\n\n(TensorShape([2000, 1]), TensorShape([2000]))\n\n\n\nnet = tf.keras.Sequential() \nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nbceloss_fn = lambda y,yhat: -tf.reduce_mean(y*tnp.log(yhat) + (1-y)*tnp.log(1-yhat))\nnet.compile(loss=bceloss_fn, optimizer=tf.optimizers.SGD(0.1))\nnet.fit(x,y,epochs=10000,verbose=0,batch_size=2000)\n\n<keras.callbacks.History at 0x7fe354356980>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense_1/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[5.09306]], dtype=float32)>,\n <tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32, numpy=array([-1.0963831], dtype=float32)>]\n\n\n\nplt.plot(x,y,'.',alpha=0.1)\nplt.plot(x,net(x),'--b')\n\n\n\n\n\n\n\n아래의 모형을 고려하자.\nmodel: \\(y_i=\\begin{cases} x_i +0.3\\epsilon_i & x\\leq 0 \\\\ 3.5x_i +0.3\\epsilon_i & x>0 \\end{cases}\\)\n아래는 위의 모형에서 생성한 샘플이다.\n\n## data \nnp.random.seed(43052)\nN=100\nx= np.linspace(-1,1,N).reshape(N,1)\ny= np.array(list(map(lambda x: x*1+np.random.normal()*0.3 if x<0 else x*3.5+np.random.normal()*0.3,x))).reshape(N,1)\n\n(1) 다음은 \\((x_i,y_i)\\)를 아래와 같은 아키텍처로 적합시키는 코드이다.\n\n$ = _0+_1x $\n\n\ntf.random.set_seed(43054) \nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1)) \nnet.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\nnet.fit(x,y,batch_size=N,epochs=1000,verbose=0) # numpy로 해도 돌아감\n\n<keras.callbacks.History at 0x7fe3542e8e80>\n\n\n케라스에 의해 추정된 \\(\\hat{\\beta}_0,\\hat{\\beta}_1\\)을 구하라.\n\nnet.weights\n\n[<tf.Variable 'dense_2/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[2.2616348]], dtype=float32)>,\n <tf.Variable 'dense_2/bias:0' shape=(1,) dtype=float32, numpy=array([0.6069048], dtype=float32)>]\n\n\n\nplt.plot(x,y,'.',alpha=0.5) \nplt.plot(x,net(x),'--')\n\n\n\n\n(2) 다음은 \\((x_i,y_i)\\)를 아래와 같은 아키텍처로 적합시키는 코드이다.\n\n\\(\\boldsymbol{u}= x\\boldsymbol{W}^{(1)}+\\boldsymbol{b}^{(1)}\\)\n\\(\\boldsymbol{v}= \\text{relu}(u)\\)\n\\(y= \\boldsymbol{v}\\boldsymbol{W}^{(2)}+b^{(2)}\\)\n\n\ntf.random.set_seed(43056) \n## 1단계\nnet = tf.keras.Sequential() \nnet.add(tf.keras.layers.Dense(2))\nnet.add(tf.keras.layers.Activation('relu')) \nnet.add(tf.keras.layers.Dense(1))\nnet.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n<keras.callbacks.History at 0x7fe3541d1090>\n\n\n\\({\\boldsymbol u}\\)를 이용하여 \\({\\boldsymbol v}\\)를 만드는 코드와 \\({\\boldsymbol v}\\)를 이용하여 \\(y\\)를 만드는 코드를 작성하라.\n\nnet.weights\n\n[<tf.Variable 'dense_3/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[1.9178674, 0.7250776]], dtype=float32)>,\n <tf.Variable 'dense_3/bias:0' shape=(2,) dtype=float32, numpy=array([ 0.33402315, -0.72684675], dtype=float32)>,\n <tf.Variable 'dense_4/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[ 1.8141525 ],\n        [-0.67835057]], dtype=float32)>,\n <tf.Variable 'dense_4/bias:0' shape=(1,) dtype=float32, numpy=array([-0.60076195], dtype=float32)>]\n\n\n\nW1= tf.Variable(tnp.array([[-0.03077251,  1.8713338 ]]))\nb1= tf.Variable(tnp.array([-0.04834982,  0.3259186 ]))\nW2= tf.Variable(tnp.array([[0.65121335],[1.8592643 ]]))\nb2= tf.Variable(tnp.array([-0.60076195])) \n\n\nwith tf.GradientTape() as tape: \n    u = tf.constant(x) @ W1 + b1 \n    v = tf.nn.relu(u) \n    yhat = v@W2 + b2 \n    loss = tf.losses.mse(y,yhat) \n\n\ntape.gradient(loss,[W1,b1,W2,b2])\n\n[<tf.Tensor: shape=(1, 2), dtype=float64, numpy=array([[ 0.00000000e+00, -4.77330119e-05]])>,\n <tf.Tensor: shape=(2,), dtype=float64, numpy=array([0.0000000e+00, 3.1478608e-06])>,\n <tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n array([[ 0.00000000e+00],\n        [-4.74910706e-05]])>,\n <tf.Tensor: shape=(1,), dtype=float64, numpy=array([-2.43031263e-05])>]\n\n\n\nplt.plot(x,y,'.') \nplt.plot(x,net(x),'--')\n\n\n\n\n(3) 아래는 (1)-(2)번 모형에 대한 discussion이다. 올바른 것을 모두 골라라.\n(곤이) (2) 모형은 활성화함수로 relu를 사용하였다. (O)\n(철용) (1) 모형에서 추정해야할 파라메터의 수는 2개이다.\n(아귀) (2) 모형이 (1) 모형보다 복잡한 모형이다. (X)\n(짝귀) (1) 의 모형은 오버피팅의 위험이 있다. (O)\n\n\n\n(1) 적절한 학습률이 선택된다면, 경사하강법은 손실함수가 convex일때 언제 전역최소해를 찾을 수 있다. (참)\n(2) tf.GradeintTape()는 경사하강법을 이용하여 최적점을 찾아주는 tool이다. (참)\n(3) 학습률이 크다는 것은 파라메터는 1회 업데이트 하는 양이 크다는 것을 의미한다. (참)\n(4) 학습률이 크면 학습파라메터의 수렴속도가 빨라지지만 때때로 과적합에 빠질 수도 있다. (참)\n(5) 단순회귀분석에서 MSE loss를 최소화 하는 해는 경사하강법을 이용하지 않아도 해석적으로 구할 수 있다. (거짓)"
  },
  {
    "objectID": "posts/2022-05-23-12주차_빅데이터분석특강.html",
    "href": "posts/2022-05-23-12주차_빅데이터분석특강.html",
    "title": "jisim12",
    "section": "",
    "text": "빅데이터분석특강\n\n\ntoc:false\nbranch: master\nbadges: true\ncomments: true\nauthor: 심재인\n\n\n\n\nimport tensorflow as tf\nimport tensorflow.experimental.numpy as tnp\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n%load_ext tensorboard\n\n\n\n\n\n\n- 데이터생성 (그냥 흑백대비 데이터)\n\n_X1 = tnp.ones([50,25])*10 \n_X1\n\n\n_X2 = tnp.zeros([50,25])*10 \n_X2\n\n\ntf.concat([_X1,_X2],axis=1)\n\n\n_noise = tnp.random.randn(50*50).reshape(50,50)\n_noise\n\n\nXXX = tf.concat([_X1,_X2],axis=1) + _noise\n\n\nXXX=XXX.reshape(1,50,50,1)\n\n\nplt.imshow(XXX.reshape(50,50),cmap='gray')\n\n- conv layer 생성\n\nconv = tf.keras.layers.Conv2D(2,(2,2))\n\n\nconv.weights # 처음에는 가중치가 없음\n\n\nconv(XXX) # 가중치를 만들기 위해서 XXX를 conv에 한번 통과시킴\nconv.weights # 이제 가중치가 생김\n\n- 가중치의 값을 확인해보자.\n\nconv.weights[0] # kernel에 해당하는것 \n\n\nconv.weights[1] # bias에 해당하는것\n\n- 필터값을 원하는 것으로 변경해보자.\n\nw0 = [[0.25,0.25],[0.25,0.25]] # 잡티를 제거하는 효과를 준다.\nw1 = [[-1.0,1.0],[-1.0,1.0]] # 경계를 찾기 좋아보이는 필터이다. (엣지검출)\n\n\nw=np.concatenate([np.array(w0).reshape(2,2,1,1),np.array(w1).reshape(2,2,1,1)],axis=-1)\nw\n\n\nb= np.array([0.0,0.0])\nb\n\n\nconv.set_weights([w,b])\nconv.get_weights()\n\n\n첫번째는 평균을 구하는 필터,\n두번째는 엣지를 검출하는 필터\n\n- 필터를 넣은 결과를 확인\n\nXXX0=conv(XXX)[...,0] # 채널0\nXXX0\n\n\nXXX1=conv(XXX)[...,1] # 채널1\nXXX1\n\n- 각 채널을 시각화\n\nfig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2)\n\n\nax1.imshow(XXX.reshape(50,50),cmap='gray')\n\n\nax3.imshow(XXX0.reshape(49,49),cmap='gray')\n\n\nax4.imshow(XXX1.reshape(49,49),cmap='gray')\n\n\nfig\n\n\n2사분면: 원래이미지\n3사분면: 원래이미지 -> 평균을 의미하는 conv적용\n4사분면: 원래이미지 -> 엣지를 검출하는 conv적용\n\n- conv(XXX)의 각 채널에 한번더 conv를 통과시켜보자\n\nconv(XXX0.reshape(1,49,49,1))[...,0] ### XXX0 -> 평균필터 <=> XXX -> 평균필터 -> 평균필터\nconv(XXX0.reshape(1,49,49,1))[...,1] ### XXX0 -> 엣지필터 <=> XXX -> 평균필터 -> 엣지필터\nconv(XXX1.reshape(1,49,49,1))[...,0] ### XXX1 -> 평균필터 <=> XXX -> 엣지필터 -> 평균필터\nconv(XXX1.reshape(1,49,49,1))[...,1] ### XXX1 -> 엣지필터 <=> XXX -> 엣지필터 -> 엣지필터\n\n\nfig,ax =plt.subplots(3,4)\n\n\nax[0][0].imshow(XXX.reshape(50,50),cmap='gray') # 원래이미지\n\n\nax[1][0].imshow(XXX0.reshape(49,49),cmap='gray') # 원래이미지 -> 평균필터 \nax[1][2].imshow(XXX1.reshape(49,49),cmap='gray') # 원래이미지 -> 엣지필터\n\n\nax[2][0].imshow(conv(XXX0.reshape(1,49,49,1))[...,0].reshape(48,48),cmap='gray') # 원래이미지 -> 평균필터 \nax[2][1].imshow(conv(XXX0.reshape(1,49,49,1))[...,1].reshape(48,48),cmap='gray') # 원래이미지 -> 엣지필터\nax[2][2].imshow(conv(XXX1.reshape(1,49,49,1))[...,0].reshape(48,48),cmap='gray') # 원래이미지 -> 평균필터 \nax[2][3].imshow(conv(XXX1.reshape(1,49,49,1))[...,1].reshape(48,48),cmap='gray') # 원래이미지 -> 엣지필터\n\n\nfig.set_figheight(8)\nfig.set_figwidth(16)\nfig.tight_layout()\nfig\n\n- 요약 - conv의 weight에 따라서 엣지를 검출하는 필터가 만들어지기도 하고 스무딩의 역할을 하는 필터가 만들어지기도 한다. 그리고 우리는 의미를 알 수 없지만 어떠한 역할을 하는 필터가 만들어질 것이다. - 이것들을 조합하다보면 우연히 이미지를 분류하기에 유리한 특징을 뽑아내는 weight가 맞춰질 수도 있겠다. - 채널수를 많이 만들고 다양한 웨이트조합을 실험하다보면 보다 복잡한 이미지의 특징을 추출할 수도 있을 것이다? - 컨볼루션 레이어의 역할 = 이미지의 특징을 추출하는 역할\n- 참고: 스트라이드, 패딩 - 스트라이드: 윈도우가 1칸씩 이동하는 것이 아니라 2~3칸씩 이동함 - 패딩: 이미지의 가장자리에 정당한 값을 넣어서 (예를들어 0) 컨볼루션을 수행. 따라서 컨볼루션 연산 이후에도 이미지의 크기가 줄어들지 않도록 방지한다.\n\n\n\n- 기본적역할: 이미지의 크기를 줄이는 것 - 이미지의의 크기를 줄여야하는 이유? 어차피 최종적으로 10차원으로 줄어야하므로 - 이미지의 크기를 줄이면서도 동시에 아주 크리티컬한 특징은 손실없이 유지하고 싶다~\n- 점점 작은 이미지가 되면서 중요한 특징들은 살아남지만 그렇지 않으면 죽는다. (캐리커쳐 느낌)\n- 평균이 아니라 max를 쓴 이유는? 그냥 평균보다 나을것이라고 생각했음.. - 그런데 사실은 꼭 그렇지만은 않아서 최근에는 꼭 맥스풀링을 고집하진 않는 추세 (평균풀링도 많이씀)\n\n\n\n- 아래와 같이 아키텍처의 다이어그램형태로 표현하고 굳이 노드별로 이미지를 그리진 않음\n\n- 물론 아래와 같이 그리는 경우도 있음\n\n\n\n\n- 격자형태로 배열된 자료를 처리하는데 특화된 신경망이다. - 시계열 (1차원격자), 이미지 (2차원격자)\n- 실제응용에서 엄청난 성공을 거두었다.\n- 이름의 유래는 컨볼루션이라는 수학적 연산을 사용했기 때문 - 컨볼루션은 조금 특별한 선형변환이다.\n- 신경과학의 원리가 심층학습에 영향을 미친 사례이다.\n\n\n\n- 희소성 + 매개변수의 공유 - 다소 철학적인 모티브임 - 희소성: 이미지를 분석하여 특징을 뽑아낼때 부분부분의 특징만 뽑으면 된다는 의미 - 매개변수의 공유: 한 채널에는 하나의 역할을 하는 커널을 설계하면 된다는 의미 (스무딩이든 엣징이든). 즉 어떤지역은 스무딩, 어떤지역은 엣징을 할 필요가 없이 한채널에서는 엣징만, 다른채널에서는 스무딩만 수행한뒤 여러채널을 조합해서 이해하면 된다.\n- 매개변수 공유효과로 인해서 파라메터가 확 줄어든다.\n(예시) (1,6,6,1) -> (1,5,5,2) - MLP방식이면 (36,50) 의 차원을 가진 매트릭스가 필요함 => 1800개의 매개변수 필요 - CNN은 8개의 매개변수 필요\n\n\n\n- 기본유닛 - conv - activation - pooling - conv - conv - activation - pooling\n\n\n\n\n\n\n- 아래의 예제를 복습하자.\n\nnp.random.seed(43052)\nx = np.linspace(0,1,100).reshape(100,1)\ny = np.random.normal(loc=0,scale=0.01,size=(100,1))\nplt.plot(x,y)\n\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(2048,activation='relu'))\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\nnet.fit(x,y,epochs=5000,verbose=0,batch_size=100)\n\n\nplt.plot(x,y)\nplt.plot(x,net(x),'--')\n\n- train/test로 나누어서 생각해보자.\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(2048,activation='relu'))\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\nnet.fit(x[:80],y[:80],epochs=5000,verbose=0,batch_size=80)\n\n\nplt.plot(x,y)\nplt.plot(x[:80],net(x[:80]),'--')\n\n\nplt.plot(x,y)\nplt.plot(x[:80],net(x[:80]),'--')\nplt.plot(x[80:],net(x[80:]),'--')\n\n\ntrain에서 추세를 따라가는게 좋은게 아니다 \\(\\to\\) 그냥 직선으로 핏하는거 이외에는 다 오버핏이다.\n\n- 매 에폭마다 적당히 80%의 노드들을 빼고 학습하자 \\(\\to\\) 너무 잘 학습되는 문제는 생기지 않을 것이다 (과적합이 방지될것이다?)\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(2048,activation='relu'))\nnet.add(tf.keras.layers.Dropout(0.8))\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\nnet.fit(x[:80],y[:80],epochs=5000,verbose=0,batch_size=80)\n\n\nplt.plot(x,y)\nplt.plot(x[:80],net(x[:80]),'--')\nplt.plot(x[80:],net(x[80:]),'--')\n\n- 드랍아웃에 대한 summary - 직관: 특정노드를 랜덤으로 off시키면 학습이 방해되어 오히려 과적합이 방지되는 효과가 있다 (그렇지만 진짜 중요한 특징이라면 랜덤으로 off 되더라도 어느정도는 학습될 듯) - note: 드랍아웃을 쓰면 오버핏이 줄어드는건 맞지만 완전히 없어지는건 아니다. - note: 오버핏을 줄이는 유일한 방법이 드랍아웃만 있는것도 아니며, 드랍아웃이 오버핏을 줄이는 가장 효과적인 방법도 아니다 (최근에는 dropout보다 batch nomalization을 사용하는 추세임)\n\n\n\n- data\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\n\nX= x_train.reshape(-1,28,28,1)/255 ## 입력이 0~255 -> 0~1로 표준화 시키는 효과 + float으로 자료형이 바뀜 \ny = tf.keras.utils.to_categorical(y_train)\nXX = x_test.reshape(-1,28,28,1)/255\nyy = tf.keras.utils.to_categorical(y_test)\n\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(50,activation='relu'))\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer='adam',loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n\n2022-05-31 16:53:48.849962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n\n#collapse_output\ncb1 = tf.keras.callbacks.TensorBoard()\nnet.fit(X,y,epochs=20,batch_size=200,validation_split=0.2,callbacks=cb1,verbose=1)\n\nEpoch 1/20\n240/240 [==============================] - 1s 2ms/step - loss: 0.6819 - accuracy: 0.7696 - val_loss: 0.4942 - val_accuracy: 0.8319\nEpoch 2/20\n240/240 [==============================] - 0s 2ms/step - loss: 0.4641 - accuracy: 0.8415 - val_loss: 0.4446 - val_accuracy: 0.8464\nEpoch 3/20\n240/240 [==============================] - 0s 2ms/step - loss: 0.4207 - accuracy: 0.8533 - val_loss: 0.4199 - val_accuracy: 0.8533\nEpoch 4/20\n240/240 [==============================] - 0s 2ms/step - loss: 0.3964 - accuracy: 0.8633 - val_loss: 0.4084 - val_accuracy: 0.8551\nEpoch 5/20\n240/240 [==============================] - 0s 2ms/step - loss: 0.3810 - accuracy: 0.8677 - val_loss: 0.4046 - val_accuracy: 0.8597\nEpoch 6/20\n240/240 [==============================] - 1s 2ms/step - loss: 0.3677 - accuracy: 0.8722 - val_loss: 0.4016 - val_accuracy: 0.8635\nEpoch 7/20\n240/240 [==============================] - 0s 1ms/step - loss: 0.3520 - accuracy: 0.8783 - val_loss: 0.3747 - val_accuracy: 0.8702\nEpoch 8/20\n240/240 [==============================] - 0s 1ms/step - loss: 0.3466 - accuracy: 0.8777 - val_loss: 0.3715 - val_accuracy: 0.8725\nEpoch 9/20\n240/240 [==============================] - 0s 1ms/step - loss: 0.3375 - accuracy: 0.8811 - val_loss: 0.3739 - val_accuracy: 0.8707\nEpoch 10/20\n240/240 [==============================] - 0s 1ms/step - loss: 0.3279 - accuracy: 0.8852 - val_loss: 0.3615 - val_accuracy: 0.8754\nEpoch 11/20\n240/240 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8871 - val_loss: 0.3545 - val_accuracy: 0.8781\nEpoch 12/20\n240/240 [==============================] - 0s 1ms/step - loss: 0.3148 - accuracy: 0.8883 - val_loss: 0.3697 - val_accuracy: 0.8703\nEpoch 13/20\n240/240 [==============================] - 0s 2ms/step - loss: 0.3086 - accuracy: 0.8910 - val_loss: 0.3542 - val_accuracy: 0.8749\nEpoch 14/20\n240/240 [==============================] - 0s 1ms/step - loss: 0.3020 - accuracy: 0.8938 - val_loss: 0.3620 - val_accuracy: 0.8748\nEpoch 15/20\n240/240 [==============================] - 1s 2ms/step - loss: 0.2949 - accuracy: 0.8936 - val_loss: 0.3610 - val_accuracy: 0.8717\nEpoch 16/20\n240/240 [==============================] - 0s 2ms/step - loss: 0.2928 - accuracy: 0.8963 - val_loss: 0.3607 - val_accuracy: 0.8755\nEpoch 17/20\n240/240 [==============================] - 0s 1ms/step - loss: 0.2897 - accuracy: 0.8963 - val_loss: 0.3542 - val_accuracy: 0.8773\nEpoch 18/20\n240/240 [==============================] - 0s 2ms/step - loss: 0.2819 - accuracy: 0.8981 - val_loss: 0.3632 - val_accuracy: 0.8721\nEpoch 19/20\n240/240 [==============================] - 0s 2ms/step - loss: 0.2791 - accuracy: 0.9000 - val_loss: 0.3403 - val_accuracy: 0.8829\nEpoch 20/20\n240/240 [==============================] - 0s 2ms/step - loss: 0.2729 - accuracy: 0.9029 - val_loss: 0.3528 - val_accuracy: 0.8782\n\n\n<keras.callbacks.History at 0x7fc05827aec0>\n\n\n- 텐서보드 여는 방법1\n\n%load_ext tensorboard\n# 주피터노트북 (혹은 주피터랩)에서 텐서보드를 임베딩하여 넣을 수 있도록 도와주는 매직펑션\n\nThe tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n\n\n\n#!rm -rf logs\n#!kill 313799\n\n\n#\n%tensorboard --logdir logs --host 0.0.0.0\n# %tensorboard --logdir logs <-- 실습에서는 이렇게 하면됩니다.\n\n\n      \n      \n      \n    \n\n\n(참고사항) 파이썬 3.10의 경우 아래의 수정이 필요\n?/python3.10/site-packages/tensorboard/_vendor/html5lib/_trie/_base.py 을 열고\nfrom collections import Mapping ### 수정전\nfrom collections.abc import Mapping ### 수정후 \n와 같이 수정한다.\n\n왜냐하면 파이썬 3.10부터 from collections import Mapping 가 동작하지 않고 from collections.abc import Mapping 가 동작하도록 문법이 바뀜\n\n- 텐서보드를 실행하는 방법2\n\n#\n# !tensorboard --logdir logs --host 0.0.0.0\n# !tensorboard --logdir logs <-- 실습에서는 이렇게 하면됩니다.\n\n\n\n\n- 텐서보드를 살펴보니 특정에폭 이후에는 오히려 과적합이 진행되는 듯 하다 (학습할수록 손해인듯 하다) \\(\\to\\) 그 특정에폭까지만 학습해보자\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(5000,activation='relu')) ## 과적합좀 시키려고\nnet.add(tf.keras.layers.Dense(5000,activation='relu')) ## 레이어를 2장만듬 + 레이어하나당 노드수도 증가\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer='adam',loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1)\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1)\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1)\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1)\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1)\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1)\n\n- 몇 번 좀 참았다가 멈추면 좋겠다.\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(5000,activation='relu')) ## 과적합좀 시키려고 \nnet.add(tf.keras.layers.Dense(5000,activation='relu')) ## 레이어를 2장만듬 + 레이어하나당 노드수도 증가 \nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer='adam',loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=5) # 좀더 참다가 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1)\n\n- 텐서보드로 그려보자?\n\n#\n# %tensorboard --logdir logs --host 0.0.0.0 \n# 아무것도 안나온다 -> 왜? cb1을 써야 텐서보드가 나옴\n\n- 조기종료와 텐서보드를 같이 쓰려면?\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(50,activation='relu')) \nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer='adam',loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n\n\ncb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=7) # 좀더 참다가 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=[cb1,cb2])\n\n\n# \n# 조기종료가 구현된 그림이 출력\n# %tensorboard --logdir logs --host 0.0.0.0\n\n\n\n\n- 하이퍼파라메터 설정\n\nfrom tensorboard.plugins.hparams import api as hp\n\n\na=net.evaluate(XX,yy)\n\n\n!rm -rf logs\nfor u in [50,5000]: \n    for d in [0.0,0.5]: \n        for o in ['adam','sgd']:\n            logdir = 'logs/hpguebin_{}_{}_{}'.format(u,d,o)\n            with tf.summary.create_file_writer(logdir).as_default():\n                net = tf.keras.Sequential()\n                net.add(tf.keras.layers.Flatten())\n                net.add(tf.keras.layers.Dense(u,activation='relu'))\n                net.add(tf.keras.layers.Dropout(d))\n                net.add(tf.keras.layers.Dense(10,activation='softmax'))\n                net.compile(optimizer=o,loss=tf.losses.categorical_crossentropy,metrics=['accuracy','Recall'])\n                cb3 = hp.KerasCallback(logdir, {'유닛수':u, '드랍아웃비율':d, '옵티마이저':o})\n                net.fit(X,y,epochs=3,callbacks=cb3)\n                _rslt=net.evaluate(XX,yy)\n                _mymetric=_rslt[1]*0.8 + _rslt[2]*0.2  \n                tf.summary.scalar('애큐러시와리컬의가중평균(테스트셋)', _mymetric, step=1)\n\n\n#\n#%tensorboard --logdir logs --host 0.0.0.0\n\n\n\n\n\n- 아래의 네트워크에서 옵티마이저를 adam, sgd를 선택하여 각각 적합시켜보고 testset의 loss를 성능비교를 하라. epoch은 5정도로 설정하라.\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(50,activation='relu'))\nnet.add(tf.keras.layers.Dense(50,activation='relu'))\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer=???,loss=tf.losses.categorical_crossentropy,metrics=['accuracy','Recall'])\n\n!rm -rf logs\ntf.random.set_seed(202150754)\nnet1 = tf.keras.Sequential()\nnet1.add(tf.keras.layers.Flatten())\nnet1.add(tf.keras.layers.Dense(50,activation='relu'))\nnet1.add(tf.keras.layers.Dense(50,activation='relu'))\nnet1.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet1.compile(optimizer='adam',loss=tf.losses.categorical_crossentropy,metrics=['accuracy','Recall'])\n\n\ncb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=7)\nnet1.fit(X,y,epochs=5,batch_size=200,validation_split=0.2,callbacks=[cb1,cb2])\n\n\n%tensorboard --logdir logs --host 0.0.0.0\n\n- sgd 적합\n\n!rm -rf logs\ntf.random.set_seed(202150754)\nnet2 = tf.keras.Sequential()\nnet2.add(tf.keras.layers.Flatten())\nnet2.add(tf.keras.layers.Dense(50,activation='relu'))\nnet2.add(tf.keras.layers.Dense(50,activation='relu'))\nnet2.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet2.compile(optimizer='sgd',loss=tf.losses.categorical_crossentropy,metrics=['accuracy','Recall'])\n\n\nnet2.fit(X,y,epochs=5,batch_size=200,validation_split=0.2,callbacks=[cb1,cb2]) \n\n\n%tensorboard --logdir logs --host 0.0.0.0\n\n\nnet1.evaluate(XX,yy)[0]\n\n\nnet2.evaluate(XX,yy)[0]\n\nadam을 선택했을떄 loss는 0.41707104444503784 이고,\nsgd을 선택했을때 loss는 0.6277241706848145 이다.\nloss 값을 보면 sgd보다 adam 값이 더 낮다."
  },
  {
    "objectID": "posts/Untitled.html",
    "href": "posts/Untitled.html",
    "title": "jisim12",
    "section": "",
    "text": "import tensorflow as tf\nimport tensorflow.experimental.numpy as tnp\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\n_X1 = tnp.ones([50,25])*10 \n_X1\n\n2022-05-23 23:36:31.857883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n<tf.Tensor: shape=(50, 25), dtype=float64, numpy=\narray([[10., 10., 10., ..., 10., 10., 10.],\n       [10., 10., 10., ..., 10., 10., 10.],\n       [10., 10., 10., ..., 10., 10., 10.],\n       ...,\n       [10., 10., 10., ..., 10., 10., 10.],\n       [10., 10., 10., ..., 10., 10., 10.],\n       [10., 10., 10., ..., 10., 10., 10.]])>\n\n\n\n_X2 = tnp.zeros([50,25])*10 \n_X2\n\n<tf.Tensor: shape=(50, 25), dtype=float64, numpy=\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])>\n\n\n\ntf.concat([_X1,_X2],axis=1)\n\n<tf.Tensor: shape=(50, 50), dtype=float64, numpy=\narray([[10., 10., 10., ...,  0.,  0.,  0.],\n       [10., 10., 10., ...,  0.,  0.,  0.],\n       [10., 10., 10., ...,  0.,  0.,  0.],\n       ...,\n       [10., 10., 10., ...,  0.,  0.,  0.],\n       [10., 10., 10., ...,  0.,  0.,  0.],\n       [10., 10., 10., ...,  0.,  0.,  0.]])>\n\n\n\n_noise = tnp.random.randn(50*50).reshape(50,50)\n_noise\n\n<tf.Tensor: shape=(50, 50), dtype=float64, numpy=\narray([[-0.35893472,  1.34715133, -1.09867888, ...,  0.86637446,\n         0.76534142,  0.02492512],\n       [-2.0924809 ,  0.40532776, -0.22088057, ..., -0.40185935,\n         0.45020357,  1.668251  ],\n       [ 1.5649497 ,  0.09772148, -0.44497024, ...,  0.95732265,\n         1.36476409,  0.81405914],\n       ...,\n       [-0.15583511, -0.59527225, -0.46381306, ...,  0.16362863,\n         1.08391654,  0.49281776],\n       [ 1.07869912,  1.07151975, -0.0196227 , ...,  1.25430225,\n         0.43976791,  0.40134943],\n       [ 0.15613959,  0.10136015,  2.32804407, ..., -1.03957414,\n        -2.6255478 , -0.08386856]])>\n\n\n\nXXX = tf.concat([_X1,_X2],axis=1) + _noise\n\n\nXXX=XXX.reshape(1,50,50,1)\n\n\nplt.imshow(XXX.reshape(50,50),cmap='gray')\n\nNameError: name 'plt' is not defined\n\n\n\nconv = tf.keras.layers.Conv2D(2,(2,2))"
  },
  {
    "objectID": "posts/2022-05-02-9주차(1)_빅데이터분석특강.html",
    "href": "posts/2022-05-02-9주차(1)_빅데이터분석특강.html",
    "title": "jisim12",
    "section": "",
    "text": "9주차-5월 02일 (1)\n\n빅데이터분석특강\n\n\ntoc:false\nbranch: master\nbadges: true\ncomments: true\nauthor: 심재인\n\n\nimports\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.experimental.numpy as tnp\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport matplotlib.pyplot as plt\n\n\n\n우도함수와 최대우도추정량\n예제\n\\(X_i \\overset{iid}{\\sim} Ber(p)\\)에서 얻은 샘플이 아래와 같다고 하자.\n\nx=[0,1,0,1]\nx\n\n[0, 1, 0, 1]\n\n\n\\(p\\)는 얼마라고 볼 수 있는가? –> 0.5\n왜?? \\(p\\)가 0.5라고 주장할 수 있는 이론적 근거, 혹은 논리체계가 무엇인가?\n- suppose: \\(p=0.1\\) 이라고 하자.\n그렇다면 \\((x_1,x_2,x_3,x_4)=(0,1,0,1)\\)와 같은 샘플이 얻어질 확률이 아래와 같다.\n\n0.9 * 0.1 * 0.9 * 0.1\n\n0.008100000000000001\n\n\n- suppose: \\(p=0.2\\) 이라고 하자.\n그렇다면 \\((x_1,x_2,x_3,x_4)=(0,1,0,1)\\)와 같은 샘플이 얻어질 확률이 아래와 같다.\n\n0.8 * 0.2 * 0.8 * 0.2\n\n0.025600000000000008\n\n\n- 질문1: \\(p=0.1\\)인것 같냐? 아니면 \\(p=0.2\\)인것 같냐? -> \\(p=0.2\\) - 왜?? \\(p=0.2\\)일 확률이 더 크다!\n(여기서 잠깐 중요한것) 확률이라는 말을 함부로 쓸 수 없다.\n- 0.0256은 “\\(p=0.2\\)일 경우 샘플 (0,1,0,1)이 얻어질 확률”이지 “\\(p=0.2\\)일 확률”은 아니다.\n“\\(p=0.2\\)인 확률” 이라는 개념이 성립하려면 아래코드에서 sum([(1-p)*p*(1-p)*p for p in _plist])이 1보다는 작아야 한다. (그런데 1보다 크다)\n\n_plist = np.linspace(0.499,0.501,1000)\nsum([(1-p)*p*(1-p)*p for p in _plist])\n\n62.49983299986714\n\n\n- 확률이라는 말을 쓸 수 없지만 확률의 느낌은 있음 -> 가능도라는 말을 쓰자. - 0.0256 \\(=\\) \\(p\\)가 0.2일 경우 샘플 (0,1,0,1)이 얻어질 확률 \\(=\\) \\(p\\)가 0.2일 가능도\n- 다시 질문1로 돌아가자! - 질문1: \\(p=0.1\\)인 것 같냐? 아니면 \\(p=0.2\\)인 것 같냐? -> 답 \\(p=0.2\\) -> 왜? \\(p=0.2\\)인 가능도가 더 크니까! - 질문2: \\(p=0.2\\)인 것 같냐? 아니면 \\(p=0.3\\)인 것 같냐? -> 답 \\(p=0.3\\) -> 왜? \\(p=0.3\\)인 가능도가 더 크니까! - 질문3: …\n- 궁극의 질문: \\(p\\)가 뭐일 것 같아? - \\(p\\)가 입력으로 들어가면 가능도가 계산되는 함수를 만들자. - 그 함수를 최대화하는 \\(p\\)를 찾자. - 그 \\(p\\)가 궁극의 질문에 대한 대답이 된다.\n- 잠깐 용어정리 - 가능도함수 \\(=\\) 우도함수 \\(=\\) likelihood function \\(:=\\) \\(L(p)\\) - \\(p\\)의 maximum likelihood estimator \\(=\\) p의 MLE \\(:=\\) \\(\\hat{p}^{mle}\\) \\(=\\) \\(\\text{argmax}_p L(p)\\) \\(=\\) \\(\\hat{p}\\)\n(예제의 풀이)\n- 이 예제의 경우 가능도함수를 정의하자. - \\(L(p)\\): \\(p\\)의 가능도함수 = \\(p\\)가 모수일때 샘플 (0,1,0,1)이 얻어질 확률 = \\(p\\)가 모수일때 \\(x_1\\)이 0일 확률 \\(\\times \\dots \\times\\) \\(p\\)가 모수일때 \\(x_4\\)가 1일 확률 - \\(L(p)=\\prod_{i=1}^{4} f(x_i;p)= \\prod_{i=1}^{4}p^{x_i}(1-p)^{1-x_i}\\)\n\nnote: 참고로 이 과정을 일반화 하면 \\(X_1,\\dots,X_n \\overset{iid}{\\sim} Ber(p)\\) 일때 \\(p\\)의 likelihood function은 \\(\\prod_{i=1}^{n}p^{x_i}(1-p)^{1-x_i}\\) 라고 볼 수 있다.\n\n\nnote: 더 일반화: \\(x_1,\\dots,x_n\\)이 pdf가 \\(f(x)\\)인 분포에서 뽑힌 서로 독립인 샘플일때 likelihood function은 \\(\\prod_{i=1}^{n}f(x_i)\\)라고 볼 수 있다.\n\n- 이 예제의 경우 \\(p\\)의 최대우도추정량을 구하면\n\\[\\hat{p}^{mle} = \\text{argmax}_p L(p) = \\text{argmax}_p  \\big\\{ p^2(1-p)^2 \\big\\}= \\frac{1}{2}\\]\n\n\n중간고사 1번\n(1) \\(N(\\mu,\\sigma)\\)에서 얻은 샘플이 아래와 같다고 할때 \\(\\mu,\\sigma\\)의 MLE를 구하여라.\n<tf.Tensor: shape=(10000,), dtype=float64, numpy=\narray([ 4.12539849,  5.46696729,  5.27243374, ...,  2.89712332,\n        5.01072291, -1.13050477])>\n(2) \\(Ber(p)\\)에서 얻은 샘플이 아래와 같다고 할 때 \\(p\\)의 MLE를 구하여라.\n<tf.Tensor: shape=(10000,), dtype=int64, numpy=array([1, 1, 1, ..., 0, 0, 1])>\n(3) \\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\), \\(\\epsilon_i \\overset{iid}{\\sim} N(0,1)\\) 일때 \\((\\beta_0,\\beta_1)\\)의 MLE를 구하여라. (회귀모형)\n(풀이) 가능도함수\n\\[L(\\beta_0,\\beta_1)=\\prod_{i=1}^{n}f(y_i), \\quad f(y_i)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(y_i-\\mu_i)^2}, \\quad \\mu_i=\\beta_0+\\beta_1 x_i\\]\n를 최대화하는 \\(\\beta_0,\\beta_1\\)을 구하면된다. 그런데 이것은 아래를 최소화하는 \\(\\beta_0,\\beta_1\\)을 구하는 것과 같다.\n\\[-\\log L(\\beta_0,\\beta_1) = \\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2\\]\n위의 식은 SSE와 같다. 결국 오차항이 정규분포를 따르는 회귀모형의 MLE는 MSE를 최소화하는 \\(\\beta_0,\\beta_1\\)을 구하면 된다.\n중간고사 1-(3)의 다른 풀이\nstep1: 생성\n\nx= tf.constant(np.arange(1,10001)/10000)\ny= tnp.random.randn(10000) + (0.5 + 2*x)\n\nstep2: minimize MSEloss (원래는 maximize log-likelihood)\n\nmaximize likelihood였던 문제를 minimize MSEloss로 바꾸어도 되는근거? 주어진 함수(=가능도함수)를 최대화하는 \\(\\beta_0,\\beta_1\\)은 MSE를 최소화하는 \\(\\beta_0,\\beta_1\\)과 동일하므로\n\n\nbeta0= tf.Variable(1.0)\nbeta1= tf.Variable(1.0)\nfor i in range(2000):\n    with tf.GradientTape() as tape:\n        #minus_log_likelihood = tf.reduce_sum((y-beta0-beta1*x)**2)\n        loss =  tf.reduce_sum((y-beta0-beta1*x)**2)\n    slope1, slope2 = tape.gradient(loss,[beta0,beta1])\n    beta0.assign_sub(slope1* 0.1/10000) # N=10000\n    beta1.assign_sub(slope2* 0.1/10000)\n\n\nbeta0,beta1\n\n(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.47993016>,\n <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.020918>)\n\n\n- 문제를 풀면서 생각해보니 손실함수는 -로그가능도함수로 선택하면 될 것 같다? - 손실함수를 선택하는 기준이 -로그가능도함수만 존재하는 것은 아니나 대부분 그러하긴함\n(4) 출제하지 못한 중간고사 문제\n아래의 모형을 생각하자. - \\(Y_i \\overset{iid}{\\sim} Ber(\\pi_i)\\) - \\(\\pi_i = \\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}=\\frac{\\exp(-1+5x_i)}{1+\\exp(-1+5x_i)}\\)\n아래는 위의 모형에서 얻은 샘플이다.\n\nx = tnp.linspace(-1,1,2000)\npi = tnp.exp(-1+5*x) / (1+tnp.exp(-1+5*x))\ny = np.random.binomial(1,pi)\ny = tf.constant(y)\n\n함수 \\(L(w_0,w_1)\\)을 최대화하는 \\((w_0,w_1)\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\((w_0,w_1)\\)의 초기값은 모두 0.1로 설정할 것)\n\\[L(w_0,w_1)=\\prod_{i=1}^{n}f(y_i), \\quad f(x_i)={\\pi_i}^{y_i}(1-\\pi_i)^{1-y_i},\\quad \\pi_i=\\text{sigmoid}(w_0+w_1x_i)\\]\n(풀이1)\n\nw0hat = tf.Variable(1.0)\nw1hat = tf.Variable(1.0)\n\n\nfor i in range(1000):\n    with tf.GradientTape() as tape:\n        pihat = tnp.exp(w0hat+w1hat *x) / (1+tnp.exp(w0hat+w1hat *x))\n        pdf = pihat**y * (1-pihat)**(1-y)\n        logL = tf.reduce_mean(tnp.log(pdf))\n    slope1,slope2 = tape.gradient(logL,[w0hat,w1hat])\n    w0hat.assign_add(slope1*0.1)\n    w1hat.assign_add(slope2*0.1)\n\n\nw0hat,w1hat\n\n(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.88931483>,\n <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.231925>)\n\n\n(해석) - 로지스틱에서 가능도함수와 BCEloss의 관계\n\\(L(w_0,w_1)\\)를 최대화하는 \\(w_0,w_1\\)은 아래를 최소화하는 \\(w_0,w_1\\)와 같다.\n\\[-\\log L(w_0,w_1) = - \\sum_{i=1}^{n}\\big(y_i \\log(\\pi_i) + (1-y_i)\\log(1-\\pi_i)\\big)\\]\n이것은 최적의 \\(w_0,w_1\\)을 \\(\\hat{w}_0,\\hat{w}_1\\)이라고 하면 \\(\\hat{\\pi}_i=\\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\hat{y}_i\\)이 되고 따라서 위의 식은 \\(n\\times\\)BCEloss의 형태임을 쉽게 알 수 있다.\n결국 로지스틱 모형에서 \\((w_0,w_1)\\)의 MLE를 구하기 위해서는 BCEloss를 최소화하는 \\((w_0,w_1)\\)을 구하면 된다!\n(풀이2)\n\nw0hat = tf.Variable(1.0)\nw1hat = tf.Variable(1.0)\n\n\nfor i in range(1000):\n    with tf.GradientTape() as tape:\n        yhat = tnp.exp(w0hat+w1hat *x) / (1+tnp.exp(w0hat+w1hat *x))\n        loss = tf.losses.binary_crossentropy(y,yhat)\n    slope1,slope2 = tape.gradient(loss,[w0hat,w1hat])\n    w0hat.assign_sub(slope1*0.1)\n    w1hat.assign_sub(slope2*0.1)\n\n\nw0hat,w1hat\n\n(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.88931495>,\n <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.2319255>)\n\n\n\n\n손실함수의 설계 (선택)\n- 회귀분석이든 로지스틱이든 손실함수는 minus_log_likelihood 로 선택한다. - 그런데 (오차항이 정규분포인) 회귀분석 일때는 minus_log_likelihood 가 MSEloss가 되고 - 로지스틱일때는 minus_log_likelihood 가 BCEloss가 된다\n- minus_log_likelihood가 손실함수를 선택하는 유일한 기준은 아니다. <— 참고만하세요, 이 수업에서는 안중요합니다. - 오차항이 대칭이고 서로독립이며 등분산 가정을 만족하는 어떠한 분포에서의 회귀모형이 있다고 하자. 이 회귀모형에서 \\(\\hat{\\beta}\\)은 여전히 MSEloss를 최소화하는 \\(\\beta\\)를 구함으로써 얻을 수 있다. - 이 경우 MSEloss를 쓰는 이론적근거? \\(\\hat{\\beta}\\)이 BLUE가 되기 때문임 (가우스-마코프정리)"
  },
  {
    "objectID": "posts/2022-03-07-1주차_빅데이터분석특강.html",
    "href": "posts/2022-03-07-1주차_빅데이터분석특강.html",
    "title": "jisim12",
    "section": "",
    "text": "1주차-3월 07일\n\n빅데이터분석특강\n\n\ntoc:false\nbranch: master\nbadges: true\ncomments: true\nauthor: 심재인\n\n\n로드맵\n- 오늘수업할내용: 단순선형회귀\n- 단순선형회귀를 배우는 이유?\n\n우리가 배우고싶은것: 심층신경망(DNN) \\(\\to\\) 합성곱신경망(CNN) \\(\\to\\) 적대적생성신경망(GAN)\n심층신경망을 바로 이해하기 어려움\n다음의 과정으로 이해해야함: (선형대수학 \\(\\to\\)) 회귀분석 \\(\\to\\) 로지스틱회귀분석 \\(\\to\\) 심층신경망\n\n\n\n선형회귀\n- 상황극 - 나는 동네에 커피점을 하나 차렸음. - 장사를 하다보니까 날이 더울수록 아이스아메리카노의 판매량이 증가한다는 사실을 깨달았다. - 일기예보는 미리 나와있으니까 그 정보를 잘 이용하면 ‘온도 -> 아이스아메리카노 판매량 예측’ 이 가능할것 같다. (내가 앞으로 얼마나 벌지 예측가능)\n- 가짜자료 생성\n\nimport matplotlib.pyplot as plt \nimport tensorflow as tf\n\n온도 \\({\\bf x}\\)가 아래와 같다고 하자.\n\nx=tf.constant([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) # 기온 \nx\n\n<tf.Tensor: shape=(10,), dtype=float32, numpy=\narray([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4],\n      dtype=float32)>\n\n\n아이스아메리카노의 판매량 \\({\\bf y}\\)이 아래와 같다고 하자. (판매량은 정수로 나오겠지만 편의상 소수점도 가능하다고 생각하자) \\[{\\bf y} \\approx 10.2 +2.2 {\\bf x}\\] - 여기에서 10.2, 2.2 의 숫자는 제가 임의로 정한것임 - 식의의미: 온도가 0일때 10.2잔정도 팔림 + 온도가 1도 증가하면 2.2잔정도 더 팔림 - 물결의의미: 현실반영. 세상은 꼭 수식대로 정확하게 이루어지지 않음.\n\ntf.random.set_seed(43052)\nepsilon=tf.random.normal([10])\ny=10.2 + 2.2*x + epsilon\ny\n\n<tf.Tensor: shape=(10,), dtype=float32, numpy=\narray([55.418365, 58.194283, 61.230827, 62.312557, 63.107002, 63.69569 ,\n       67.247055, 71.4365  , 73.1013  , 77.84988 ], dtype=float32)>\n\n\n- 우리는 아래와 같은 자료를 모았다고 생각하자.\n\ntf.transpose(tf.concat([[x],[y]],0))\n\n<tf.Tensor: shape=(10, 2), dtype=float32, numpy=\narray([[20.1     , 55.418365],\n       [22.2     , 58.194283],\n       [22.7     , 61.230827],\n       [23.3     , 62.312557],\n       [24.4     , 63.107002],\n       [25.1     , 63.69569 ],\n       [26.2     , 67.247055],\n       [27.3     , 71.4365  ],\n       [28.4     , 73.1013  ],\n       [30.4     , 77.84988 ]], dtype=float32)>\n\n\n- 그려보자.\n\nplt.plot(x,y,'.') # 파란점, 관측한 데이터 \nplt.plot(x,10.2 + 2.2*x, '--')  # 주황색점선, 세상의 법칙 \n\n\n\n\n- 우리의 목표: 파란색점 \\(\\to\\) 주황색점선을 추론 // 데이터를 바탕으로 세상의 법칙을 추론\n- 아이디어: 데이터를 보니까 \\(x\\)와 \\(y\\)가 선형의 관계에 있는듯 보인다. 즉 모든 \\(i=1,2,\\dots, 10\\)에 대하여 아래를 만족하는 적당한 a,b (혹은 \\(\\beta_0,\\beta_1\\)) 가 존재할것 같다. - \\(y_{i} \\approx ax_{i}+b\\) - \\(y_{i} \\approx \\beta_1 x_{i}+\\beta_0\\)\n- 어림짐작으로 \\(a,b\\)를 알아내보자.\n데이터를 살펴보자.\n\ntf.transpose(tf.concat([[x],[y]],0))\n\n<tf.Tensor: shape=(10, 2), dtype=float32, numpy=\narray([[20.1     , 55.418365],\n       [22.2     , 58.194283],\n       [22.7     , 61.230827],\n       [23.3     , 62.312557],\n       [24.4     , 63.107002],\n       [25.1     , 63.69569 ],\n       [26.2     , 67.247055],\n       [27.3     , 71.4365  ],\n       [28.4     , 73.1013  ],\n       [30.4     , 77.84988 ]], dtype=float32)>\n\n\n적당히 왼쪽*2+15 = 오른쪽의 관계가 성립하는것 같다.\n따라서 \\(a=2, b=15\\) 혹은 \\(\\beta_0=15, \\beta_1=2\\) 로 추론할 수 있겠다.\n- 누군가가 \\((\\beta_0,\\beta_1)=(14,2)\\) 이라고 주장할 수 있다. (어차피 지금은 감각으로 추론하는 과정이니까)\n- 새로운 주장으로 인해서 \\((\\beta_0,\\beta_1)=(15,2)\\) 로 볼 수도 있고 \\((\\beta_0,\\beta_1)=(14,2)\\) 로 볼 수도 있다. 이중에서 어떠한 추정치가 좋은지 판단할 수 있을까? - 후보1: \\((\\beta_0,\\beta_1)=(15,2)\\) - 후보2: \\((\\beta_0,\\beta_1)=(14,2)\\)\n- 가능한 \\(y_i \\approx \\beta_0 + \\beta_1 x_i\\) 이 되도록 만드는 \\((\\beta_0,\\beta_1)\\) 이 좋을 것이다. \\(\\to\\) 후보 1,2를 비교해보자.\n(관찰에 의한 비교)\n후보1에 대해서 \\(i=1,2\\)를 넣고 관찰하여 보자.\n\n20.1 * 2 + 15 , 55.418365 # i=1\n\n(55.2, 55.418365)\n\n\n\n22.2 * 2 + 15 , 58.194283 # i=2\n\n(59.4, 58.194283)\n\n\n후보2에 대하여 \\(i=1,2\\)를 넣고 관찰하여 보자.\n\n20.1 * 2 + 14 , 55.418365 # i=1\n\n(54.2, 55.418365)\n\n\n\n22.2 * 2 + 14 , 58.194283 # i=2\n\n(58.4, 58.194283)\n\n\n\\(i=1\\)인 경우에는 후보1이 더 잘맞는것 같은데 \\(i=2\\)인 경우는 후보2가 더 잘맞는것 같다.\n(좀 더 체계적인 비교)\n\\(i=1,2,3, \\dots, 10\\) 에서 후보1과 후보2중 어떤것이 더 좋은지 비교하는 체계적인 방법을 생각해보자.\n후보 1,2에 대하여 \\(\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2\\)를 계산하여 비교해보자.\n\nsum1=0 \nfor i in range(10):\n    sum1=sum1+(y[i]-15-2*x[i])**2\n\n\nsum2=0 \nfor i in range(10):\n    sum2=sum2+(y[i]-14-2*x[i])**2\n\n\nsum1,sum2\n\n(<tf.Tensor: shape=(), dtype=float32, numpy=14.734169>,\n <tf.Tensor: shape=(), dtype=float32, numpy=31.521086>)\n\n\n후보1이 더 \\(\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2\\)의 값이 작다.\n후보1이 종합적으로 후보2에 비하여 좋다. 이 과정을 무한번 반복하면 최적의 추정치를 찾을 수 있다.\n- 수학을 이용해서 좀 더 체계적으로 찾아보자. 결국 아래식을 가장 작게 만드는 \\(\\beta_0,\\beta_1\\)을 찾으면 된다.\n\\(\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2\\)\n그런데 결국 \\(\\beta_0, \\beta_1\\)에 대한 이차식인데 이 식을 최소화하는 \\(\\beta_0,\\beta_1\\)을 구하기 위해서는 아래를 연립하여 풀면된다\n\\(\\begin{cases} \\frac{\\partial}{\\partial \\beta_0}\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2=0 \\\\ \\frac{\\partial}{\\partial \\beta_1}\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2=0 \\end{cases}\\)\n- 풀어보자.\n\\(\\begin{cases} \\sum_{i=1}^{10} -2(y_i -\\beta_0 -\\beta_1 x_i)=0 \\\\ \\sum_{i=1}^{10} -2x_i(y_i -\\beta_0 -\\beta_1 x_i)=0 \\end{cases}\\)\n정리하면\n\\[\\hat{\\beta}_0= \\bar{y}-\\hat{\\beta}_1 \\bar{x}\\]\n\\[\\hat{\\beta}_1= \\frac{S_{xy}}{S_{xx}}=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}\\]\n- 따라서 최적의 추정치 \\((\\hat{\\beta}_0,\\hat{\\beta}_1)\\)를 이용한 추세선을 아래와 같이 계산할 수 있음.\n\nSxx= sum((x-sum(x)/10)**2)\nSxx\n\n<tf.Tensor: shape=(), dtype=float32, numpy=87.84898>\n\n\n\nSxy=  sum((x-sum(x)/10)*(y-sum(y)/10))\nSxy\n\n<tf.Tensor: shape=(), dtype=float32, numpy=194.64737>\n\n\n\nbeta1_estimated = Sxy/Sxx \nbeta1_estimated\n\n<tf.Tensor: shape=(), dtype=float32, numpy=2.2157042>\n\n\n\nbeta0_estimated = sum(y)/10 - beta1_estimated * sum(x)/10 \nbeta0_estimated\n\n<tf.Tensor: shape=(), dtype=float32, numpy=9.94458>\n\n\n\nplt.plot(x,y,'.')\nplt.plot(x,beta0_estimated + beta1_estimated * x, '--') # 주황색선: 세상의 법칙을 추정한선 \nplt.plot(x,10.2 + 2.2* x, '--') # 초록색선: ture, 세상의법칙\n\n\n\n\n\nNote: 샘플수가 커질수록 주황색선은 점점 초록색선으로 가까워진다.\n\n- 꽤 훌륭한 도구임. 그런데 약간의 단점이 존재한다.\n\n공식이 좀 복잡함..\n\\(x\\)가 여러개일 경우 확장이 어려움\n\n- 단점을 극복하기 위해서 우리가 지금까지 했던논의를 매트릭스로 바꾸어서 다시 써보자.\n- 모형의 매트릭스화\n우리의 모형은 아래와 같다.\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i, \\quad i=1,2,\\dots,10\\)\n풀어서 쓰면\n\\(\\begin{cases} y_1 = \\beta_0 +\\beta_1 x_1 + \\epsilon_1 \\\\ y_2 = \\beta_0 +\\beta_1 x_2 + \\epsilon_2 \\\\ \\dots \\\\ y_{10} = \\beta_0 +\\beta_1 x_{10} + \\epsilon_{10} \\end{cases}\\)\n아래와 같이 쓸 수 있다.\n$\n\\[\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\dots \\\\\ny_{10}\n\\end{bmatrix}\\]\n=\n\\[\\begin{bmatrix}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\dots & \\dots \\\\\n1 & x_{10}\n\\end{bmatrix}\\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\]\n\n\\[\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\dots \\\\\n\\epsilon_{10}\n\\end{bmatrix}\\]\n$\n\n벡터와 매트릭스 형태로 정리하면\n\\({\\bf y} = {\\bf X} {\\boldsymbol \\beta} + \\boldsymbol{\\epsilon}\\) - 손실함수의 매트릭스화: 우리가 최소화 하려던 손실함수는 아래와 같다.\n\\(loss=\\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2\\)\n이것을 벡터표현으로 하면 아래와 같다.\n\\(loss=\\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})\\)\n풀어보면\n\\(loss=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})={\\bf y}^\\top {\\bf y} - {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}\\)\n- 미분하는 과정의 매트릭스화\nloss를 최소화하는 \\({\\boldsymbol \\beta}\\)를 구해야하므로 loss를 \\({\\boldsymbol \\beta}\\)로 미분한식을 0이라고 놓고 풀면 된다.\n\\(\\frac{\\partial}{\\partial \\boldsymbol{\\beta}} loss = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf y} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}\\)\n$= 0 - {}^- {}^ + 2{}^ $\n따라서 \\(\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}loss=0\\)을 풀면 아래와 같다.\n$= ({}){-1}{}^ $\n- 공식도 매트릭스로 표현하면: $= ({}){-1}{}^ $ <– 외우세요\n- 적용을 해보자.\n(X를 만드는 방법1)\n\nX=tf.transpose(tf.concat([[[1.0]*10],[x]],0)) # \nX\n\n<tf.Tensor: shape=(10, 2), dtype=float32, numpy=\narray([[ 1. , 20.1],\n       [ 1. , 22.2],\n       [ 1. , 22.7],\n       [ 1. , 23.3],\n       [ 1. , 24.4],\n       [ 1. , 25.1],\n       [ 1. , 26.2],\n       [ 1. , 27.3],\n       [ 1. , 28.4],\n       [ 1. , 30.4]], dtype=float32)>\n\n\n(X를 만드는 방법2)\n\nfrom tensorflow.python.ops.numpy_ops import np_config\nnp_config.enable_numpy_behavior()\n\n\nX=tf.concat([[[1.0]*10],[x]],0).T\nX\n\n<tf.Tensor: shape=(10, 2), dtype=float32, numpy=\narray([[ 1. , 20.1],\n       [ 1. , 22.2],\n       [ 1. , 22.7],\n       [ 1. , 23.3],\n       [ 1. , 24.4],\n       [ 1. , 25.1],\n       [ 1. , 26.2],\n       [ 1. , 27.3],\n       [ 1. , 28.4],\n       [ 1. , 30.4]], dtype=float32)>\n\n\n\ntf.linalg.inv(X.T @ X) @ X.T @ y\n\nInternalError: Attempting to perform BLAS operation using StreamExecutor without BLAS support [Op:MatMul]\n\n\n- 잘 구해진다.\n- 그런데..\n\nbeta0_estimated,beta1_estimated\n\n(<tf.Tensor: shape=(), dtype=float32, numpy=9.94458>,\n <tf.Tensor: shape=(), dtype=float32, numpy=2.2157042>)\n\n\n값이 좀 다르다..?\n- 같은 값입니다! 신경쓰지 마세요! 텐서플로우가 좀 대충계산합니다.\n\nimport tensorflow.experimental.numpy as tnp\n\n\nx=tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) \ny=10.2 + 2.2*x + epsilon \n\n\nbeta1_estimated = sum((x-sum(x)/10)*(y-sum(y)/10)) / sum((x-sum(x)/10)**2)\nbeta0_estimated = sum(y)/10 - beta1_estimated * sum(x)/10 \n\n\nbeta0_estimated, beta1_estimated\n\n(<tf.Tensor: shape=(), dtype=float64, numpy=9.944573243234018>,\n <tf.Tensor: shape=(), dtype=float64, numpy=2.215704607783491>)\n\n\n\nX=tnp.concatenate([[tnp.array([1.0]*10)],[x]],0).T\ntf.linalg.inv(X.T @ X) @ X.T @ y\n\nInternalError: Attempting to perform BLAS operation using StreamExecutor without BLAS support [Op:MatMul]\n\n\n\n\n앞으로 할것\n- 선형대수학의 미분이론..\n- 실습 (tensorflow에서 매트릭스를 자유롭게 다루기)"
  },
  {
    "objectID": "posts/2022-05-16-11주차_빅데이터분석특강.html",
    "href": "posts/2022-05-16-11주차_빅데이터분석특강.html",
    "title": "jisim12",
    "section": "",
    "text": "11주차-5월 16일\n\n빅데이터분석특강\n\n\ntoc:false\nbranch: master\nbadges: true\ncomments: true\nauthor: 심재인\n\n\nimports\n\nimport tensorflow as tf \nimport tensorflow.experimental.numpy as tnp \nimport numpy as np \nimport matplotlib.pyplot as plt\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\n\nX = tf.constant(x_train.reshape(-1,28,28,1),dtype=tf.float64)\ny = tf.keras.utils.to_categorical(y_train)\nXX = tf.constant(x_test.reshape(-1,28,28,1),dtype=tf.float64)\nyy = tf.keras.utils.to_categorical(y_test)\n\n- 첫시도\n\nnet1 = tf.keras.Sequential()\nnet1.add(tf.keras.layers.Flatten())\nnet1.add(tf.keras.layers.Dense(500,activation='relu'))\nnet1.add(tf.keras.layers.Dense(500,activation='relu'))\nnet1.add(tf.keras.layers.Dense(500,activation='relu'))\nnet1.add(tf.keras.layers.Dense(500,activation='relu'))\nnet1.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet1.compile(optimizer='adam', loss=tf.losses.categorical_crossentropy,metrics='accuracy')\nnet1.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 3s 2ms/step - loss: 1.0565 - accuracy: 0.7923\nEpoch 2/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.4495 - accuracy: 0.8389\nEpoch 3/5\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.4135 - accuracy: 0.8525\nEpoch 4/5\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.4001 - accuracy: 0.8579\nEpoch 5/5\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.3753 - accuracy: 0.8673\n\n\n<keras.callbacks.History at 0x7f55d055a8c0>\n\n\n\nnet1.evaluate(XX,yy)\n\n313/313 [==============================] - 1s 2ms/step - loss: 0.4420 - accuracy: 0.8570\n\n\n[0.4419509172439575, 0.8569999933242798]\n\n\n\nnet1.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten (Flatten)           (None, 784)               0         \n                                                                 \n dense (Dense)               (None, 500)               392500    \n                                                                 \n dense_1 (Dense)             (None, 500)               250500    \n                                                                 \n dense_2 (Dense)             (None, 500)               250500    \n                                                                 \n dense_3 (Dense)             (None, 500)               250500    \n                                                                 \n dense_4 (Dense)             (None, 10)                5010      \n                                                                 \n=================================================================\nTotal params: 1,149,010\nTrainable params: 1,149,010\nNon-trainable params: 0\n_________________________________________________________________\n\n\n- 두번째 시도\n\nnet2 = tf.keras.Sequential()\nnet2.add(tf.keras.layers.Conv2D(30,(2,2),activation='relu'))\nnet2.add(tf.keras.layers.MaxPool2D()) \nnet2.add(tf.keras.layers.Conv2D(30,(2,2),activation='relu'))\nnet2.add(tf.keras.layers.MaxPool2D()) \nnet2.add(tf.keras.layers.Flatten())\n#net2.add(tf.keras.layers.Dense(500,activation='relu'))\nnet2.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet2.compile(optimizer='adam', loss=tf.losses.categorical_crossentropy,metrics='accuracy')\nnet2.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.7357 - accuracy: 0.8065\nEpoch 2/5\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.3728 - accuracy: 0.8676\nEpoch 3/5\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.3346 - accuracy: 0.8798\nEpoch 4/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.3110 - accuracy: 0.8875\nEpoch 5/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.2929 - accuracy: 0.8925\n\n\n<keras.callbacks.History at 0x7f55d055be50>\n\n\n\nnet2.evaluate(XX,yy)\n\n313/313 [==============================] - 1s 1ms/step - loss: 0.3280 - accuracy: 0.8807\n\n\n[0.32795271277427673, 0.8806999921798706]\n\n\n\nnet2.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 27, 27, 30)        150       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 13, 13, 30)       0         \n )                                                               \n                                                                 \n conv2d_1 (Conv2D)           (None, 12, 12, 30)        3630      \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 6, 6, 30)         0         \n 2D)                                                             \n                                                                 \n flatten_1 (Flatten)         (None, 1080)              0         \n                                                                 \n dense_5 (Dense)             (None, 10)                10810     \n                                                                 \n=================================================================\nTotal params: 14,590\nTrainable params: 14,590\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n14590/ 1149010\n\n0.012697887746842934\n\n\n\nc1, m1, c2, m2, flttn, dns = net2.layers\n\n\n\nMaxPool2D\n\n테스트1\n- 레이어생성\n\nm=tf.keras.layers.MaxPool2D()\n\n- 입력데이터\n\nXXX = tnp.arange(1*4*4*1).reshape(1,4,4,1)\nXXX.reshape(1,4,4)\n\n<tf.Tensor: shape=(1, 4, 4), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11],\n        [12, 13, 14, 15]]])>\n\n\n- 입력데이터가 레이어를 통과한 모습\n\nm(XXX).reshape(1,2,2)\n\n<tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[ 5,  7],\n        [13, 15]]])>\n\n\n- MaxPool2D layer의 역할: (2,2)윈도우를 만들고 (2,2)윈도우에서 max를 뽑아 값을 기록, 윈도우를 움직이면서 반복\n\n\n테스트2\n\nXXX = tnp.arange(1*6*6*1).reshape(1,6,6,1)\nXXX.reshape(1,6,6)\n\n<tf.Tensor: shape=(1, 6, 6), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11],\n        [12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23],\n        [24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35]]])>\n\n\n\nm(XXX).reshape(1,3,3)\n\n<tf.Tensor: shape=(1, 3, 3), dtype=int64, numpy=\narray([[[ 7,  9, 11],\n        [19, 21, 23],\n        [31, 33, 35]]])>\n\n\n\n\n테스트3\n\nm=tf.keras.layers.MaxPool2D(pool_size=(3, 3))\n\n\nXXX = tnp.arange(1*6*6*1).reshape(1,6,6,1)\nXXX.reshape(1,6,6)\n\n<tf.Tensor: shape=(1, 6, 6), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11],\n        [12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23],\n        [24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35]]])>\n\n\n\nm(XXX).reshape(1,2,2)\n\n<tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[14, 17],\n        [32, 35]]])>\n\n\n\n\n테스트4\n\nm=tf.keras.layers.MaxPool2D(pool_size=(2, 2))\n\n\nXXX = tnp.arange(1*5*5*1).reshape(1,5,5,1)\nXXX.reshape(1,5,5)\n\n<tf.Tensor: shape=(1, 5, 5), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3,  4],\n        [ 5,  6,  7,  8,  9],\n        [10, 11, 12, 13, 14],\n        [15, 16, 17, 18, 19],\n        [20, 21, 22, 23, 24]]])>\n\n\n\nm(XXX).reshape(1,2,2)\n\n<tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[ 6,  8],\n        [16, 18]]])>\n\n\n\nm=tf.keras.layers.MaxPool2D(pool_size=(2, 2),padding=\"same\")\n\n\nXXX = tnp.arange(1*5*5*1).reshape(1,5,5,1)\nXXX.reshape(1,5,5)\n\n<tf.Tensor: shape=(1, 5, 5), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3,  4],\n        [ 5,  6,  7,  8,  9],\n        [10, 11, 12, 13, 14],\n        [15, 16, 17, 18, 19],\n        [20, 21, 22, 23, 24]]])>\n\n\n\nm(XXX).reshape(1,3,3)\n\n<tf.Tensor: shape=(1, 3, 3), dtype=int64, numpy=\narray([[[ 6,  8,  9],\n        [16, 18, 19],\n        [21, 23, 24]]])>\n\n\n\n\n테스트5\n\nXXX = tnp.arange(2*4*4*1).reshape(2,4,4,1)\nXXX.reshape(2,4,4)\n\n<tf.Tensor: shape=(2, 4, 4), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11],\n        [12, 13, 14, 15]],\n\n       [[16, 17, 18, 19],\n        [20, 21, 22, 23],\n        [24, 25, 26, 27],\n        [28, 29, 30, 31]]])>\n\n\n\nm(XXX).reshape(2,2,2)\n\n<tf.Tensor: shape=(2, 2, 2), dtype=int64, numpy=\narray([[[ 5,  7],\n        [13, 15]],\n\n       [[21, 23],\n        [29, 31]]])>\n\n\n\n\n테스트6\n\nXXX = tnp.arange(1*4*4*3).reshape(1,4,4,3)\n\n\nXXX[...,0]\n\n<tf.Tensor: shape=(1, 4, 4), dtype=int64, numpy=\narray([[[ 0,  3,  6,  9],\n        [12, 15, 18, 21],\n        [24, 27, 30, 33],\n        [36, 39, 42, 45]]])>\n\n\n\nm(XXX)[...,0]\n\n<tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[15, 21],\n        [39, 45]]])>\n\n\n\n\n\nConv2D\n\n테스트1\n- 레이어생성\n\ncnv = tf.keras.layers.Conv2D(1,(2,2))\n\n- XXX생성\n\nXXX = tnp.arange(1*4*4*1,dtype=tf.float64).reshape(1,4,4,1)\nXXX.reshape(1,4,4)\n\n<tf.Tensor: shape=(1, 4, 4), dtype=float64, numpy=\narray([[[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]]])>\n\n\n\ncnv(XXX).reshape(1,3,3)\n\n<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[-2.8128474, -3.2847993, -3.756751 ],\n        [-4.7006545, -5.172607 , -5.6445584],\n        [-6.588462 , -7.060414 , -7.532366 ]]], dtype=float32)>\n\n\n\nXXX에서 cnv(XXX)로 가는 맵핑을 찾는건 쉽지 않아보인다.\n심지어 랜덤으로 결정되는 부분도 있어보임\n\n- 코드정리 + 시드통일\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(1,(2,2))\nXXX = tnp.arange(1*4*4*1,dtype=tf.float64).reshape(1,4,4,1)\n\n- conv의 입출력\n\nprint(XXX.reshape(1,4,4))\nprint(cnv(XXX).reshape(1,3,3))\n\ntf.Tensor(\n[[[ 0.  1.  2.  3.]\n  [ 4.  5.  6.  7.]\n  [ 8.  9. 10. 11.]\n  [12. 13. 14. 15.]]], shape=(1, 4, 4), dtype=float64)\ntf.Tensor(\n[[[ -4.125754   -5.312817   -6.4998803]\n  [ -8.874006  -10.0610695 -11.248133 ]\n  [-13.622259  -14.809322  -15.996386 ]]], shape=(1, 3, 3), dtype=float32)\n\n\n- conv연산 추론\n\ntf.reshape(cnv.weights[0],(2,2))\n\n<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[-0.13014299, -0.23927206],\n       [-0.20175874, -0.6158894 ]], dtype=float32)>\n\n\n\n0 * -0.13014299 + 1 * -0.23927206 + 4 * -0.20175874 + 5 * -0.6158894 + 0\n\n-4.1257540200000005\n\n\n- 내가 정의한 weights를 대입하여 conv 연산 확인\n\ncnv.get_weights()[0].shape\n\n(2, 2, 1, 1)\n\n\n\nw = np.array([1/4,1/4,1/4,1/4],dtype=np.float32).reshape(2, 2, 1, 1)\nb = np.array([3],dtype=np.float32)\n\n\ncnv.set_weights([w,b])\n\n\nXXX.reshape(1,4,4)\n\n<tf.Tensor: shape=(1, 4, 4), dtype=float64, numpy=\narray([[[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]]])>\n\n\n\ncnv(XXX).reshape(1,3,3)\n\n<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[ 5.5,  6.5,  7.5],\n        [ 9.5, 10.5, 11.5],\n        [13.5, 14.5, 15.5]]], dtype=float32)>\n\n\n\nnp.mean([0,1,4,5])+3, np.mean([1,2,5,6])+3, np.mean([2,3,6,7])+3\n\n(5.5, 6.5, 7.5)\n\n\n\n\ntf.keras.layers.Conv2D(1,kernel_size=(2,2)) 요약\n- 요약\n\nsize=(2,2)인 윈도우를 만듬.\nXXX에 윈도우를 통과시켜서 (2,2)크기의 sub XXX 를 얻음. sub XXX의 각 원소에 conv2d.weights[0]의 각 원소를 element-wise하게 곱한다.\n(2)의 결과를 모두 더한다. 그리고 그 결과에 다시 conv2d.weights[1]을 수행\n윈도우를 이동시키면서 반복!\n\n\n\n테스트2\n- 레이어와 XXX생성\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(1,(3,3))\nXXX = tnp.arange(1*5*5*1,dtype=tf.float64).reshape(1,5,5,1)\n\n\nXXX.reshape(1,5,5) ## 입력: XXX\n\n<tf.Tensor: shape=(1, 5, 5), dtype=float64, numpy=\narray([[[ 0.,  1.,  2.,  3.,  4.],\n        [ 5.,  6.,  7.,  8.,  9.],\n        [10., 11., 12., 13., 14.],\n        [15., 16., 17., 18., 19.],\n        [20., 21., 22., 23., 24.]]])>\n\n\n\ntf.reshape(cnv.weights[0],(3,3)) ## 커널의 가중치\n\n<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[-0.08676198, -0.1595147 , -0.13450584],\n       [-0.4105929 , -0.38366908,  0.07744962],\n       [-0.09255642,  0.4915564 ,  0.20828158]], dtype=float32)>\n\n\n\ncnv(XXX).reshape(1,3,3) ## 출력: conv(XXX)\n\n<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[ 2.7395768 ,  2.2492635 ,  1.7589504 ],\n        [ 0.28801066, -0.20230258, -0.6926158 ],\n        [-2.1635566 , -2.6538715 , -3.1441827 ]]], dtype=float32)>\n\n\n\ntf.reduce_sum(XXX.reshape(1,5,5)[0,:3,:3] * tf.reshape(cnv.weights[0],(3,3)))\n\n<tf.Tensor: shape=(), dtype=float64, numpy=2.739577144384384>\n\n\n\n\n테스트3\n\n![](https://github.com/guebin/2021BDA/blob/master/_notebooks/2021-11-04-conv.png?raw=true)\n\n/bin/bash: -c: line 0: syntax error near unexpected token `https://github.com/guebin/2021BDA/blob/master/_notebooks/2021-11-04-conv.png?raw=true'\n/bin/bash: -c: line 0: `[](https://github.com/guebin/2021BDA/blob/master/_notebooks/2021-11-04-conv.png?raw=true)'\n\n\n\nXXX = tf.constant([[3,3,2,1,0],[0,0,1,3,1],[3,1,2,2,3],[2,0,0,2,2],[2,0,0,0,1]],dtype=tf.float64).reshape(1,5,5,1)\nXXX.reshape(1,5,5)\n\n<tf.Tensor: shape=(1, 5, 5), dtype=float64, numpy=\narray([[[3., 3., 2., 1., 0.],\n        [0., 0., 1., 3., 1.],\n        [3., 1., 2., 2., 3.],\n        [2., 0., 0., 2., 2.],\n        [2., 0., 0., 0., 1.]]])>\n\n\n\ncnv = tf.keras.layers.Conv2D(1,(3,3))\n\n\ncnv.weights\n\n[]\n\n\n\ncnv(XXX).reshape(1,3,3)\n\n<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[-0.6065799 , -0.69143724, -1.1179221 ],\n        [ 2.2352955 ,  1.5314975 ,  1.8658445 ],\n        [ 0.7364182 ,  1.4419123 ,  1.2381717 ]]], dtype=float32)>\n\n\n\ncnv.weights[0]\n\n<tf.Variable 'conv2d_6/kernel:0' shape=(3, 3, 1, 1) dtype=float32, numpy=\narray([[[[-0.3444655 ]],\n\n        [[ 0.4521824 ]],\n\n        [[ 0.236296  ]]],\n\n\n       [[[ 0.54707503]],\n\n        [[ 0.19746327]],\n\n        [[ 0.20471048]]],\n\n\n       [[[-0.1245549 ]],\n\n        [[-0.25237298]],\n\n        [[-0.4904977 ]]]], dtype=float32)>\n\n\n\n_w = tf.constant([[0,1,2],[2,2,0],[0,1,2]],dtype=tf.float64).reshape(3,3,1,1)\n_b = tf.constant([0],dtype=tf.float64)\n\n\ncnv.set_weights([_w,_b])\n\n\ncnv(XXX).reshape(1,3,3)\n\n<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[12., 12., 17.],\n        [10., 17., 19.],\n        [ 9.,  6., 14.]]], dtype=float32)>\n\n\n\n\n테스트4\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(1,(2,2))\nXXX = tnp.arange(2*5*5*1,dtype=tf.float64).reshape(2,5,5,1)\n\n\nprint(XXX.reshape(2,5,5))\ncnv(XXX) # weights를 초기화 시키기 위해서 레이어를 1회 통과 \ncnv.set_weights([w,b])\nprint(cnv(XXX).reshape(2,4,4))\n\ntf.Tensor(\n[[[ 0.  1.  2.  3.  4.]\n  [ 5.  6.  7.  8.  9.]\n  [10. 11. 12. 13. 14.]\n  [15. 16. 17. 18. 19.]\n  [20. 21. 22. 23. 24.]]\n\n [[25. 26. 27. 28. 29.]\n  [30. 31. 32. 33. 34.]\n  [35. 36. 37. 38. 39.]\n  [40. 41. 42. 43. 44.]\n  [45. 46. 47. 48. 49.]]], shape=(2, 5, 5), dtype=float64)\ntf.Tensor(\n[[[ 6.  7.  8.  9.]\n  [11. 12. 13. 14.]\n  [16. 17. 18. 19.]\n  [21. 22. 23. 24.]]\n\n [[31. 32. 33. 34.]\n  [36. 37. 38. 39.]\n  [41. 42. 43. 44.]\n  [46. 47. 48. 49.]]], shape=(2, 4, 4), dtype=float32)\n\n\n\nnp.mean([0,1,5,6])+3,np.mean([25,26,30,31])+3,\n\n(6.0, 31.0)\n\n\n\n\n테스트5\n-\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(4,(2,2),activation='relu')\nXXX = tnp.arange(1*2*2*1,dtype=tf.float64).reshape(1,2,2,1)\n\n\nprint(XXX.reshape(1,2,2))\n\ntf.Tensor(\n[[[0. 1.]\n  [2. 3.]]], shape=(1, 2, 2), dtype=float64)\n\n\n\ncnv(XXX)\n\n<tf.Tensor: shape=(1, 1, 1, 4), dtype=float32, numpy=array([[[[1.048703, 0.      , 0.      , 0.      ]]]], dtype=float32)>\n\n\n\ncnv.weights[0] # (2,2) 커널의 크기 // 1은 XXX의 채널수 // 4는 conv(XXX)의 채널수\n\n<tf.Variable 'conv2d_8/kernel:0' shape=(2, 2, 1, 4) dtype=float32, numpy=\narray([[[[-0.08230966, -0.15132892, -0.12760344, -0.38952267]],\n\n        [[-0.36398047,  0.07347518, -0.08780673,  0.46633136]]],\n\n\n       [[[ 0.19759327, -0.46042526, -0.15406173, -0.34838456]],\n\n        [[ 0.33916563, -0.08248386,  0.11705655, -0.49948823]]]],\n      dtype=float32)>\n\n\n\ncnv.weights[0][...,0].reshape(2,2) ## conv(XXX)의 첫번째채널 출력을 얻기 위해 곱해지는 w\n\n<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[-0.08230966, -0.36398047],\n       [ 0.19759327,  0.33916563]], dtype=float32)>\n\n\n\ntf.reduce_sum(XXX.reshape(1,2,2) * cnv.weights[0][...,0].reshape(2,2)) ### conv(XXX)의 첫번째 채널 출력결과\n\n<tf.Tensor: shape=(), dtype=float64, numpy=1.0487029552459717>\n\n\n- 계산결과를 확인하기 쉽게 하기 위한 약간의 트릭\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(4,(2,2))\nXXX = tnp.array([1]*1*2*2*1,dtype=tf.float64).reshape(1,2,2,1)\n\n\nprint(XXX.reshape(1,2,2))\n\ntf.Tensor(\n[[[1. 1.]\n  [1. 1.]]], shape=(1, 2, 2), dtype=float64)\n\n\n\n이렇게 XXX를 설정하면 cnv(XXX)의 결과는 단지 cnv의 weight들의 sum이 된다.\n\n\ncnv(XXX)\n\n<tf.Tensor: shape=(1, 1, 1, 4), dtype=float32, numpy=\narray([[[[ 0.09046876, -0.6207628 , -0.25241536, -0.7710641 ]]]],\n      dtype=float32)>\n\n\n\ncnv.weights[0] # (2,2) 커널의 크기 // 1은 XXX의 채널수 // 4는 conv(XXX)의 채널수\n\n<tf.Variable 'conv2d_9/kernel:0' shape=(2, 2, 1, 4) dtype=float32, numpy=\narray([[[[-0.08230966, -0.15132892, -0.12760344, -0.38952267]],\n\n        [[-0.36398047,  0.07347518, -0.08780673,  0.46633136]]],\n\n\n       [[[ 0.19759327, -0.46042526, -0.15406173, -0.34838456]],\n\n        [[ 0.33916563, -0.08248386,  0.11705655, -0.49948823]]]],\n      dtype=float32)>\n\n\n\ncnv.weights[0][...,0].reshape(2,2) ## conv(XXX)의 첫번째채널 출력을 얻기 위해 곱해지는 w\n\n<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[-0.08230966, -0.36398047],\n       [ 0.19759327,  0.33916563]], dtype=float32)>\n\n\n\ntf.reduce_sum(cnv.weights[0][...,0])\n#tf.reduce_sum(XXX.reshape(1,2,2) * cnv.weights[0][...,0].reshape(2,2)) ### conv(XXX)의 첫번째 채널 출력결과\n\n<tf.Tensor: shape=(), dtype=float32, numpy=0.090468764>\n\n\n\n\n테스트6\n- 결과확인을 쉽게하기 위해서 XXX를 1로 통일\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(4,(2,2))\nXXX = tnp.array([1]*1*2*2*3,dtype=tf.float64).reshape(1,2,2,3)\n\n\ncnv(XXX)\n\n<tf.Tensor: shape=(1, 1, 1, 4), dtype=float32, numpy=\narray([[[[ 0.3297621, -0.4498347, -1.0487393, -1.580095 ]]]],\n      dtype=float32)>\n\n\n\ncnv.weights[0] ## (2,2)는 커널의 사이즈 // 3은 XXX의채널 // 4는 cnv(XXX)의 채널\n\n<tf.Variable 'conv2d_10/kernel:0' shape=(2, 2, 3, 4) dtype=float32, numpy=\narray([[[[-0.06956434, -0.12789628, -0.10784459, -0.32920673],\n         [-0.30761963,  0.06209785, -0.07421023,  0.3941219 ],\n         [ 0.16699678, -0.38913035, -0.13020593, -0.29443866]],\n\n        [[ 0.28664726, -0.0697116 ,  0.09893084, -0.4221446 ],\n         [-0.23161241, -0.16410837, -0.36420006,  0.12424195],\n         [-0.14245945,  0.36286396, -0.10751781,  0.1733647 ]]],\n\n\n       [[[ 0.02764335,  0.15547717, -0.42024496, -0.31893867],\n         [ 0.22414821,  0.3619454 , -0.00282967, -0.3503708 ],\n         [ 0.4610079 , -0.17417148,  0.00401336, -0.29777044]],\n\n        [[-0.1620284 , -0.42066965, -0.01578814, -0.4240524 ],\n         [ 0.37925082,  0.24236053,  0.3949356 , -0.20996472],\n         [-0.30264795, -0.28889188, -0.3237777 ,  0.37506342]]]],\n      dtype=float32)>\n\n\n\ncnv.weights[0][...,0] ## cnv(XXX)의 첫번째 채널결과를 얻기 위해서 사용하는 w\n\n<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=\narray([[[-0.06956434, -0.30761963,  0.16699678],\n        [ 0.28664726, -0.23161241, -0.14245945]],\n\n       [[ 0.02764335,  0.22414821,  0.4610079 ],\n        [-0.1620284 ,  0.37925082, -0.30264795]]], dtype=float32)>\n\n\n\ntf.reduce_sum(cnv.weights[0][...,0]) ### cnv(XXX)의 첫번째 채널의 결과\n\n<tf.Tensor: shape=(), dtype=float32, numpy=0.32976213>\n\n\n\nprint(tf.reduce_sum(cnv.weights[0][...,0]))\nprint(tf.reduce_sum(cnv.weights[0][...,1]))\nprint(tf.reduce_sum(cnv.weights[0][...,2]))\nprint(tf.reduce_sum(cnv.weights[0][...,3])) ### cnv(XXX)의 결과\n\ntf.Tensor(0.32976213, shape=(), dtype=float32)\ntf.Tensor(-0.44983464, shape=(), dtype=float32)\ntf.Tensor(-1.0487392, shape=(), dtype=float32)\ntf.Tensor(-1.5800952, shape=(), dtype=float32)\n\n\n\nw_red = cnv.weights[0][...,0][...,0] \nw_green = cnv.weights[0][...,0][...,1] \nw_blue = cnv.weights[0][...,0][...,2]\n\n\ntf.reduce_sum(XXX[...,0] * w_red + XXX[...,1] * w_green + XXX[...,2] * w_blue) ## cnv(XXX)의 첫채널 출력결과\n\n<tf.Tensor: shape=(), dtype=float64, numpy=0.32976213097572327>\n\n\n\n\n\nhw\n아래와 같은 흑백이미지가 있다고 하자.\n0 0 0 1 1 1 \n0 0 0 1 1 1 \n0 0 0 1 1 1 \n0 0 0 1 1 1 \n0 0 0 1 1 1\n0 0 0 1 1 1 \n위의 이미지에 아래와 같은 weight를 가진 필터를 적용하여 convolution한 결과를 계산하라. (bias는 0으로 가정한다)\n-1 1 \n-1 1 \n\ncnv = tf.keras.layers.Conv2D(1,(2,2))\nXXX = tf.constant([[0,0,0,1,1,1],[0,0,0,1,1,1],[0,0,0,1,1,1],[0,0,0,1,1,1],[0,0,0,1,1,1],[0,0,0,1,1,1]],dtype=tf.float64).reshape(1,6,6,1)\nXXX.reshape(1,6,6)\n\n<tf.Tensor: shape=(1, 6, 6), dtype=float64, numpy=\narray([[[0., 0., 0., 1., 1., 1.],\n        [0., 0., 0., 1., 1., 1.],\n        [0., 0., 0., 1., 1., 1.],\n        [0., 0., 0., 1., 1., 1.],\n        [0., 0., 0., 1., 1., 1.],\n        [0., 0., 0., 1., 1., 1.]]])>\n\n\n\ncnv(XXX).reshape(1,5,5)\n\n<tf.Tensor: shape=(1, 5, 5), dtype=float32, numpy=\narray([[[0.       , 0.       , 1.4988861, 1.336632 , 1.336632 ],\n        [0.       , 0.       , 1.4988861, 1.336632 , 1.336632 ],\n        [0.       , 0.       , 1.4988861, 1.336632 , 1.336632 ],\n        [0.       , 0.       , 1.4988861, 1.336632 , 1.336632 ],\n        [0.       , 0.       , 1.4988861, 1.336632 , 1.336632 ]]],\n      dtype=float32)>\n\n\n\ncnv.weights[0]\n\n<tf.Variable 'conv2d_11/kernel:0' shape=(2, 2, 1, 1) dtype=float32, numpy=\narray([[[[-0.51669824]],\n\n        [[ 0.6782736 ]]],\n\n\n       [[[ 0.3544441 ]],\n\n        [[ 0.8206125 ]]]], dtype=float32)>\n\n\n\n_w = tf.constant([[-1,1],[-1,1]],dtype=tf.float64).reshape(2,2,1,1)\n_b = tf.constant([0],dtype=tf.float64)\n\n\ncnv.set_weights([_w,_b])\n\n\ncnv.weights\n\n[<tf.Variable 'conv2d_11/kernel:0' shape=(2, 2, 1, 1) dtype=float32, numpy=\n array([[[[-1.]],\n \n         [[ 1.]]],\n \n \n        [[[-1.]],\n \n         [[ 1.]]]], dtype=float32)>,\n <tf.Variable 'conv2d_11/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n\n\n\ncnv(XXX).reshape(1,5,5)\n\n<tf.Tensor: shape=(1, 5, 5), dtype=float32, numpy=\narray([[[0., 0., 2., 0., 0.],\n        [0., 0., 2., 0., 0.],\n        [0., 0., 2., 0., 0.],\n        [0., 0., 2., 0., 0.],\n        [0., 0., 2., 0., 0.]]], dtype=float32)>"
  },
  {
    "objectID": "posts/2022-06-13-기말고사_빅데이터분석특강.html",
    "href": "posts/2022-06-13-기말고사_빅데이터분석특강.html",
    "title": "jisim12",
    "section": "",
    "text": "빅데이터분석특강\n\n\n기말고사 - toc:false - branch: master - badges: true - comments: true - author: 심재인\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport tensorflow as tf \nimport tensorflow.experimental.numpy as tnp\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\n%load_ext tensorboard\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+ s + ';}')\n\n\n\n\n(1) tf.keras.datasets.fashion_mnist.load_data()을 이용하여 fashion_mnist 자료를 불러온 뒤 아래의 네트워크를 이용하여 적합하라.\n\n평가지표로 accuracy를 이용할 것\nepoch은 10으로 설정할 것\noptimizer는 adam을 이용할 것\n\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x1\"\n    \"x2\"\n    \"..\"\n    \"x784\"\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x1\" -> \"node1\"\n    \"x2\" -> \"node1\"\n    \"..\" -> \"node1\"\n    \"x784\" -> \"node1\"\n    \n    \"x1\" -> \"node2\"\n    \"x2\" -> \"node2\"\n    \"..\" -> \"node2\"\n    \"x784\" -> \"node2\"\n    \n    \"x1\" -> \"...\"\n    \"x2\" -> \"...\"\n    \"..\" -> \"...\"\n    \"x784\" -> \"...\"\n\n    \"x1\" -> \"node20\"\n    \"x2\" -> \"node20\"\n    \"..\" -> \"node20\"\n    \"x784\" -> \"node20\"\n\n\n    label = \"Layer 1: relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -> \"node1 \"\n    \"node2\" -> \"node1 \"\n    \"...\" -> \"node1 \"\n    \"node20\" -> \"node1 \"\n    \n    \"node1\" -> \"node2 \"\n    \"node2\" -> \"node2 \"\n    \"...\" -> \"node2 \"\n    \"node20\" -> \"node2 \"\n    \n    \"node1\" -> \"... \"\n    \"node2\" -> \"... \"\n    \"...\" -> \"... \"\n    \"node20\" -> \"... \"\n\n    \"node1\" -> \"node30 \"\n    \"node2\" -> \"node30 \"\n    \"...\" -> \"node30 \"\n    \"node20\" -> \"node30 \"\n\n\n    label = \"Layer 2: relu\"\n}\nsubgraph cluster_4{\n    style=filled;\n    color=lightgrey;\n\n    \"node1 \" -> \"y10\"\n    \"node2 \" -> \"y10\"\n    \"... \" -> \"y10\"\n    \"node30 \" -> \"y10\"\n    \n    \"node1 \" -> \"y1\"\n    \"node2 \" -> \"y1\"\n    \"... \" -> \"y1\"\n    \"node30 \" -> \"y1\"\n    \n    \"node1 \" -> \".\"\n    \"node2 \" -> \".\"\n    \"... \" -> \".\"\n    \"node30 \" -> \".\"\n    \n    label = \"Layer 3: softmax\"\n}\n''')\n\n\n\n\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\n\nX = tf.constant(x_train.reshape(-1,28,28,1),dtype=tf.float64)\ny = tf.keras.utils.to_categorical(y_train)\nXX = tf.constant(x_test.reshape(-1,28,28,1),dtype=tf.float64)\nyy = tf.keras.utils.to_categorical(y_test)\n\n\ntf.random.set_seed(4305)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(20,activation='relu'))\nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy'])\nnet.fit(X,y,epochs=10)\n\nEpoch 1/10\n1875/1875 [==============================] - 3s 2ms/step - loss: 1.5829 - accuracy: 0.5105\nEpoch 2/10\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.9581 - accuracy: 0.6048\nEpoch 3/10\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.9115 - accuracy: 0.6188\nEpoch 4/10\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.8911 - accuracy: 0.6262\nEpoch 5/10\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.8937 - accuracy: 0.6198\nEpoch 6/10\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.8745 - accuracy: 0.6296\nEpoch 7/10\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.8683 - accuracy: 0.6305\nEpoch 8/10\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.8586 - accuracy: 0.6331\nEpoch 9/10\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.8512 - accuracy: 0.6346\nEpoch 10/10\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.8361 - accuracy: 0.6405\n\n\n<keras.callbacks.History at 0x7f6074641780>\n\n\n(2) (1)에서 적합된 네트워크를 이용하여 test data의 accuracy를 구하라.\n\nnet.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 1ms/step - loss: 0.9906 - accuracy: 0.6023\n\n\n[0.9906449317932129, 0.6022999882698059]\n\n\n(3) train set에서 20%의 자료를 validation 으로 분리하여 50에폭동안 학습하라. 텐서보드를 이용하여 train accuracy와 validation accuracy를 시각화 하고 결과를 해석하라. 오버피팅이라고 볼 수 있는가?\n\ntf.random.set_seed(4305)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(20,activation='relu'))\nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy'])\n\n\n#collapse_output\ncb1 = tf.keras.callbacks.TensorBoard()\nnet.fit(X,y,epochs=50,batch_size=200,validation_split=0.2,callbacks=cb1,verbose=1)\n\nEpoch 1/50\n240/240 [==============================] - 1s 2ms/step - loss: 3.7604 - accuracy: 0.2533 - val_loss: 1.8268 - val_accuracy: 0.3212\nEpoch 2/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.7592 - accuracy: 0.3275 - val_loss: 1.6927 - val_accuracy: 0.3509\nEpoch 3/50\n240/240 [==============================] - 1s 2ms/step - loss: 1.6008 - accuracy: 0.3767 - val_loss: 1.5118 - val_accuracy: 0.4139\nEpoch 4/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.4380 - accuracy: 0.4215 - val_loss: 1.3867 - val_accuracy: 0.4374\nEpoch 5/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.3066 - accuracy: 0.4505 - val_loss: 1.2980 - val_accuracy: 0.4444\nEpoch 6/50\n240/240 [==============================] - 1s 2ms/step - loss: 1.2581 - accuracy: 0.4582 - val_loss: 1.2748 - val_accuracy: 0.4487\nEpoch 7/50\n240/240 [==============================] - 1s 3ms/step - loss: 1.2330 - accuracy: 0.4642 - val_loss: 1.2586 - val_accuracy: 0.4619\nEpoch 8/50\n240/240 [==============================] - 1s 2ms/step - loss: 1.2193 - accuracy: 0.4665 - val_loss: 1.2448 - val_accuracy: 0.4613\nEpoch 9/50\n240/240 [==============================] - 1s 3ms/step - loss: 1.2105 - accuracy: 0.4708 - val_loss: 1.2377 - val_accuracy: 0.4622\nEpoch 10/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.2070 - accuracy: 0.4655 - val_loss: 1.2371 - val_accuracy: 0.4642\nEpoch 11/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.2008 - accuracy: 0.4737 - val_loss: 1.2230 - val_accuracy: 0.4660\nEpoch 12/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.1410 - accuracy: 0.5050 - val_loss: 1.1361 - val_accuracy: 0.5242\nEpoch 13/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.0190 - accuracy: 0.5516 - val_loss: 1.0478 - val_accuracy: 0.5467\nEpoch 14/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.9785 - accuracy: 0.5649 - val_loss: 0.9969 - val_accuracy: 0.5644\nEpoch 15/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.9415 - accuracy: 0.5791 - val_loss: 0.9576 - val_accuracy: 0.5727\nEpoch 16/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.9153 - accuracy: 0.5905 - val_loss: 0.9360 - val_accuracy: 0.5847\nEpoch 17/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.9017 - accuracy: 0.5966 - val_loss: 0.9458 - val_accuracy: 0.5847\nEpoch 18/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.8910 - accuracy: 0.6009 - val_loss: 0.9270 - val_accuracy: 0.5822\nEpoch 19/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.8784 - accuracy: 0.6180 - val_loss: 0.8976 - val_accuracy: 0.6333\nEpoch 20/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.8352 - accuracy: 0.6661 - val_loss: 0.8576 - val_accuracy: 0.6777\nEpoch 21/50\n240/240 [==============================] - 1s 2ms/step - loss: 0.7971 - accuracy: 0.6873 - val_loss: 0.8313 - val_accuracy: 0.6733\nEpoch 22/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.7644 - accuracy: 0.7017 - val_loss: 0.7697 - val_accuracy: 0.7097\nEpoch 23/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.7318 - accuracy: 0.7209 - val_loss: 0.7529 - val_accuracy: 0.7385\nEpoch 24/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.7057 - accuracy: 0.7413 - val_loss: 0.6952 - val_accuracy: 0.7667\nEpoch 25/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.6893 - accuracy: 0.7513 - val_loss: 0.7467 - val_accuracy: 0.7442\nEpoch 26/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.6604 - accuracy: 0.7602 - val_loss: 0.6979 - val_accuracy: 0.7577\nEpoch 27/50\n240/240 [==============================] - 1s 2ms/step - loss: 0.6607 - accuracy: 0.7580 - val_loss: 0.7469 - val_accuracy: 0.7286\nEpoch 28/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.6563 - accuracy: 0.7586 - val_loss: 0.7293 - val_accuracy: 0.7120\nEpoch 29/50\n240/240 [==============================] - 1s 2ms/step - loss: 0.6480 - accuracy: 0.7643 - val_loss: 0.6978 - val_accuracy: 0.7479\nEpoch 30/50\n240/240 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.7636 - val_loss: 0.6801 - val_accuracy: 0.7651\nEpoch 31/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.6383 - accuracy: 0.7667 - val_loss: 0.6997 - val_accuracy: 0.7478\nEpoch 32/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.6562 - accuracy: 0.7581 - val_loss: 0.6934 - val_accuracy: 0.7676\nEpoch 33/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.6394 - accuracy: 0.7653 - val_loss: 0.6672 - val_accuracy: 0.7673\nEpoch 34/50\n240/240 [==============================] - 1s 2ms/step - loss: 0.6428 - accuracy: 0.7643 - val_loss: 0.6584 - val_accuracy: 0.7627\nEpoch 35/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.6232 - accuracy: 0.7701 - val_loss: 0.7548 - val_accuracy: 0.7439\nEpoch 36/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.6402 - accuracy: 0.7639 - val_loss: 0.6739 - val_accuracy: 0.7664\nEpoch 37/50\n240/240 [==============================] - 1s 3ms/step - loss: 0.6296 - accuracy: 0.7692 - val_loss: 0.6644 - val_accuracy: 0.7657\nEpoch 38/50\n240/240 [==============================] - 1s 2ms/step - loss: 0.6076 - accuracy: 0.7751 - val_loss: 0.6762 - val_accuracy: 0.7726\nEpoch 39/50\n240/240 [==============================] - 1s 2ms/step - loss: 0.6217 - accuracy: 0.7686 - val_loss: 0.6467 - val_accuracy: 0.7686\nEpoch 40/50\n240/240 [==============================] - 1s 2ms/step - loss: 0.6227 - accuracy: 0.7717 - val_loss: 0.7088 - val_accuracy: 0.7470\nEpoch 41/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.6263 - accuracy: 0.7676 - val_loss: 0.6680 - val_accuracy: 0.7705\nEpoch 42/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.6181 - accuracy: 0.7732 - val_loss: 0.6628 - val_accuracy: 0.7694\nEpoch 43/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.6025 - accuracy: 0.7766 - val_loss: 0.6621 - val_accuracy: 0.7640\nEpoch 44/50\n240/240 [==============================] - 1s 2ms/step - loss: 0.6106 - accuracy: 0.7748 - val_loss: 0.6728 - val_accuracy: 0.7655\nEpoch 45/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.6247 - accuracy: 0.7727 - val_loss: 0.7016 - val_accuracy: 0.7570\nEpoch 46/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.6118 - accuracy: 0.7766 - val_loss: 0.6759 - val_accuracy: 0.7631\nEpoch 47/50\n240/240 [==============================] - 1s 3ms/step - loss: 0.6121 - accuracy: 0.7748 - val_loss: 0.6359 - val_accuracy: 0.7703\nEpoch 48/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.5991 - accuracy: 0.7783 - val_loss: 0.6671 - val_accuracy: 0.7711\nEpoch 49/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.6036 - accuracy: 0.7785 - val_loss: 0.6549 - val_accuracy: 0.7624\nEpoch 50/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.6135 - accuracy: 0.7747 - val_loss: 0.7121 - val_accuracy: 0.7519\n\n\n<keras.callbacks.History at 0x7f60745bd420>\n\n\n\n%tensorboard --logdir logs --host 0.0.0.0\n\n\n      \n      \n      \n    \n\n\n(4) (3)에서 적합된 네트워크를 이용하여 test data의 accuracy를 구하라. (2)의 결과와 비교하라.\n\nnet.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 1ms/step - loss: 0.7562 - accuracy: 0.7376\n\n\n[0.7561669945716858, 0.7376000285148621]\n\n\n(5) 조기종료기능을 이용하여 (3)의 네트워크를 다시 학습하라. 학습결과를 텐서보드를 이용하여 시각화 하라. - patience=3 으로 설정할 것\n\ntf.random.set_seed(4305)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(20,activation='relu'))\nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy'])\n\n\ncb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=3)\nnet.fit(X,y,epochs=50,batch_size=200,validation_split=0.2,callbacks=[cb1,cb2])\n\nEpoch 1/50\n240/240 [==============================] - 1s 2ms/step - loss: 3.7604 - accuracy: 0.2533 - val_loss: 1.8268 - val_accuracy: 0.3212\nEpoch 2/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.7592 - accuracy: 0.3275 - val_loss: 1.6927 - val_accuracy: 0.3509\nEpoch 3/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.6008 - accuracy: 0.3767 - val_loss: 1.5118 - val_accuracy: 0.4139\nEpoch 4/50\n240/240 [==============================] - 1s 2ms/step - loss: 1.4380 - accuracy: 0.4215 - val_loss: 1.3867 - val_accuracy: 0.4374\nEpoch 5/50\n240/240 [==============================] - 1s 2ms/step - loss: 1.3066 - accuracy: 0.4505 - val_loss: 1.2980 - val_accuracy: 0.4444\nEpoch 6/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.2581 - accuracy: 0.4582 - val_loss: 1.2748 - val_accuracy: 0.4487\nEpoch 7/50\n240/240 [==============================] - 1s 3ms/step - loss: 1.2330 - accuracy: 0.4642 - val_loss: 1.2586 - val_accuracy: 0.4619\nEpoch 8/50\n240/240 [==============================] - 1s 2ms/step - loss: 1.2193 - accuracy: 0.4665 - val_loss: 1.2448 - val_accuracy: 0.4613\nEpoch 9/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.2105 - accuracy: 0.4708 - val_loss: 1.2377 - val_accuracy: 0.4622\nEpoch 10/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.2061 - accuracy: 0.4659 - val_loss: 1.2383 - val_accuracy: 0.4653\nEpoch 11/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.1626 - accuracy: 0.4972 - val_loss: 1.0994 - val_accuracy: 0.5357\nEpoch 12/50\n240/240 [==============================] - 0s 2ms/step - loss: 1.0339 - accuracy: 0.5463 - val_loss: 1.0432 - val_accuracy: 0.5406\nEpoch 13/50\n240/240 [==============================] - 0s 1ms/step - loss: 0.9805 - accuracy: 0.5605 - val_loss: 1.0071 - val_accuracy: 0.5533\nEpoch 14/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.9512 - accuracy: 0.5733 - val_loss: 0.9570 - val_accuracy: 0.5700\nEpoch 15/50\n240/240 [==============================] - 1s 2ms/step - loss: 0.9320 - accuracy: 0.5836 - val_loss: 0.9522 - val_accuracy: 0.5748\nEpoch 16/50\n240/240 [==============================] - 1s 3ms/step - loss: 0.9043 - accuracy: 0.5893 - val_loss: 0.9527 - val_accuracy: 0.5817\nEpoch 17/50\n240/240 [==============================] - 1s 3ms/step - loss: 0.9029 - accuracy: 0.5948 - val_loss: 0.9524 - val_accuracy: 0.5967\nEpoch 18/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.8863 - accuracy: 0.6029 - val_loss: 0.8971 - val_accuracy: 0.6086\nEpoch 19/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.8806 - accuracy: 0.6055 - val_loss: 0.9137 - val_accuracy: 0.5938\nEpoch 20/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.8638 - accuracy: 0.6175 - val_loss: 0.9310 - val_accuracy: 0.5863\nEpoch 21/50\n240/240 [==============================] - 0s 2ms/step - loss: 0.8681 - accuracy: 0.6205 - val_loss: 0.9162 - val_accuracy: 0.5987\n\n\n<keras.callbacks.History at 0x7f60743504f0>\n\n\n\n%tensorboard --logdir logs --host 0.0.0.0\n\nReusing TensorBoard on port 6006 (pid 1543198), started 0:01:05 ago. (Use '!kill 1543198' to kill it.)\n\n\n\n      \n      \n      \n    \n\n\n\n\n\n(1) tf.keras.datasets.fashion_mnist.load_data()을 이용하여 fashion_mnist 자료를 불러온 뒤 아래의 네트워크를 이용하여 적합하라.\n\n이때 n1=6, n2=16, n3=120 으로 설정한다, 드랍아웃비율은 20%로 설정한다.\nnet.summary()를 출력하여 설계결과를 확인하라.\n\n\n\ntf.random.set_seed(4305)\nnet1 = tf.keras.Sequential()\nnet1.add(tf.keras.layers.Conv2D(6,(4,4),activation='relu'))\nnet1.add(tf.keras.layers.MaxPool2D()) \nnet1.add(tf.keras.layers.Conv2D(16,(4,4),activation='relu'))\nnet1.add(tf.keras.layers.MaxPool2D()) \nnet1.add(tf.keras.layers.Flatten())\nnet1.add(tf.keras.layers.Dense(120,activation='relu'))\nnet1.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet1.add(tf.keras.layers.Dropout(0.2))\nnet1.compile(optimizer='adam', loss=tf.losses.categorical_crossentropy,metrics='accuracy')\nnet1.fit(X,y,epochs=5,batch_size=200)\n\nEpoch 1/5\n300/300 [==============================] - 2s 3ms/step - loss: 4.7348 - accuracy: 0.5660\nEpoch 2/5\n300/300 [==============================] - 1s 2ms/step - loss: 3.5846 - accuracy: 0.6644\nEpoch 3/5\n300/300 [==============================] - 1s 3ms/step - loss: 3.5181 - accuracy: 0.6817\nEpoch 4/5\n300/300 [==============================] - 2s 6ms/step - loss: 3.5000 - accuracy: 0.6915\nEpoch 5/5\n300/300 [==============================] - 1s 2ms/step - loss: 3.4955 - accuracy: 0.6991\n\n\n<keras.callbacks.History at 0x7f6074230f70>\n\n\n\nnet1.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 1ms/step - loss: 0.4737 - accuracy: 0.8336\n\n\n[0.47367578744888306, 0.8335999846458435]\n\n\n\nnet1.summary()\n\nModel: \"sequential_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 25, 25, 6)         102       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 12, 12, 6)        0         \n )                                                               \n                                                                 \n conv2d_1 (Conv2D)           (None, 9, 9, 16)          1552      \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 4, 4, 16)         0         \n 2D)                                                             \n                                                                 \n flatten_3 (Flatten)         (None, 256)               0         \n                                                                 \n dense_10 (Dense)            (None, 120)               30840     \n                                                                 \n dense_11 (Dense)            (None, 10)                1210      \n                                                                 \n dropout (Dropout)           (None, 10)                0         \n                                                                 \n=================================================================\nTotal params: 33,704\nTrainable params: 33,704\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nc1, m1, c2, m2, flttn, dns1, dns2, dropout = net1.layers\n\n\nprint(X.shape)\nprint(c1(X).shape) \nprint(m1(c1(X)).shape)\nprint(c2(m1(c1(X))).shape)\nprint(m2(c2(m1(c1(X)))).shape)\nprint(flttn(m2(c2(m1(c1(X))))).shape)\nprint(dns1(flttn(m2(c2(m1(c1(X)))))).shape)\nprint(dns2(dns1(flttn(m2(c2(m1(c1(X))))))).shape)\nprint(dropout(dns2(dns1(flttn(m2(c2(m1(c1(X)))))))).shape)\n\n(60000, 28, 28, 1)\n(60000, 25, 25, 6)\n(60000, 12, 12, 6)\n(60000, 9, 9, 16)\n(60000, 4, 4, 16)\n(60000, 256)\n(60000, 120)\n(60000, 10)\n(60000, 10)\n\n\n(2) n1=(6,64,128), n2=(16,256)에 대하여 test set의 loss가 최소화되는 조합을 찾아라. 결과를 텐서보드로 시각화하는 코드를 작성하라. - epoc은 3회로 한정한다. - validation_split은 0.2로 설정한다.\n\nfrom tensorboard.plugins.hparams import api as hp\n\n\n!rm -rf logs\nfor u in [6,64,128]: \n    for d in [16,256]: \n        logdir = 'logs/hpguebin_{}_{}'.format(u,d)\n        with tf.summary.create_file_writer(logdir).as_default():\n            tf.random.set_seed(4305)\n            net1 = tf.keras.Sequential()\n            net1.add(tf.keras.layers.Conv2D(6,(4,4),activation='relu'))\n            net1.add(tf.keras.layers.MaxPool2D()) \n            net1.add(tf.keras.layers.Conv2D(16,(4,4),activation='relu'))\n            net1.add(tf.keras.layers.MaxPool2D()) \n            net1.add(tf.keras.layers.Flatten())\n            net1.add(tf.keras.layers.Dense(120,activation='relu'))\n            net1.add(tf.keras.layers.Dense(10,activation='softmax'))\n            net1.add(tf.keras.layers.Dropout(0.2))\n            net1.compile(optimizer='adam', loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n            cb3 = hp.KerasCallback(logdir, {'n1':u, 'n2':d})\n            net1.fit(X,y,epochs=3,batch_size=200,callbacks=cb3,validation_split=0.2)\n            _rslt=net.evaluate(XX,yy) \n            tf.summary.scalar('test set loss', _rslt[0], step=1)\n\nEpoch 1/3\n240/240 [==============================] - 1s 2ms/step - loss: 4.5264 - accuracy: 0.5632 - val_loss: 0.7014 - val_accuracy: 0.7626\nEpoch 2/3\n240/240 [==============================] - 1s 2ms/step - loss: 3.6046 - accuracy: 0.6613 - val_loss: 0.5816 - val_accuracy: 0.7987\nEpoch 3/3\n240/240 [==============================] - 1s 4ms/step - loss: 3.5402 - accuracy: 0.6794 - val_loss: 0.5017 - val_accuracy: 0.8221\n313/313 [==============================] - 1s 1ms/step - loss: 0.9186 - accuracy: 0.6016\nEpoch 1/3\n240/240 [==============================] - 1s 2ms/step - loss: 4.5248 - accuracy: 0.5633 - val_loss: 0.6992 - val_accuracy: 0.7633\nEpoch 2/3\n240/240 [==============================] - 1s 4ms/step - loss: 3.6040 - accuracy: 0.6615 - val_loss: 0.5797 - val_accuracy: 0.8017\nEpoch 3/3\n240/240 [==============================] - 1s 4ms/step - loss: 3.5399 - accuracy: 0.6792 - val_loss: 0.5047 - val_accuracy: 0.8190\n313/313 [==============================] - 0s 1ms/step - loss: 0.9186 - accuracy: 0.6016\nEpoch 1/3\n240/240 [==============================] - 1s 2ms/step - loss: 4.5264 - accuracy: 0.5634 - val_loss: 0.7020 - val_accuracy: 0.7631\nEpoch 2/3\n240/240 [==============================] - 0s 2ms/step - loss: 3.6056 - accuracy: 0.6609 - val_loss: 0.5857 - val_accuracy: 0.8000\nEpoch 3/3\n240/240 [==============================] - 0s 2ms/step - loss: 3.5418 - accuracy: 0.6779 - val_loss: 0.5214 - val_accuracy: 0.8131\n313/313 [==============================] - 0s 1ms/step - loss: 0.9186 - accuracy: 0.6016\nEpoch 1/3\n240/240 [==============================] - 1s 2ms/step - loss: 4.5273 - accuracy: 0.5639 - val_loss: 0.6979 - val_accuracy: 0.7663\nEpoch 2/3\n240/240 [==============================] - 1s 2ms/step - loss: 3.6087 - accuracy: 0.6604 - val_loss: 0.5780 - val_accuracy: 0.7997\nEpoch 3/3\n240/240 [==============================] - 1s 2ms/step - loss: 3.5387 - accuracy: 0.6801 - val_loss: 0.5023 - val_accuracy: 0.8219\n313/313 [==============================] - 0s 1ms/step - loss: 0.9186 - accuracy: 0.6016\nEpoch 1/3\n240/240 [==============================] - 1s 2ms/step - loss: 4.5266 - accuracy: 0.5634 - val_loss: 0.7039 - val_accuracy: 0.7641\nEpoch 2/3\n240/240 [==============================] - 0s 2ms/step - loss: 3.6053 - accuracy: 0.6612 - val_loss: 0.5807 - val_accuracy: 0.8037\nEpoch 3/3\n240/240 [==============================] - 0s 2ms/step - loss: 3.5400 - accuracy: 0.6788 - val_loss: 0.5033 - val_accuracy: 0.8204\n313/313 [==============================] - 0s 1ms/step - loss: 0.9186 - accuracy: 0.6016\nEpoch 1/3\n240/240 [==============================] - 1s 2ms/step - loss: 4.5264 - accuracy: 0.5642 - val_loss: 0.7000 - val_accuracy: 0.7630\nEpoch 2/3\n240/240 [==============================] - 0s 2ms/step - loss: 3.6065 - accuracy: 0.6603 - val_loss: 0.5829 - val_accuracy: 0.7987\nEpoch 3/3\n240/240 [==============================] - 0s 2ms/step - loss: 3.5391 - accuracy: 0.6795 - val_loss: 0.5180 - val_accuracy: 0.8127\n313/313 [==============================] - 0s 1ms/step - loss: 0.9186 - accuracy: 0.6016\n\n\n\n%tensorboard --logdir logs --host 0.0.0.0\n\nReusing TensorBoard on port 6006 (pid 1543198), started 0:03:17 ago. (Use '!kill 1543198' to kill it.)\n\n\n\n      \n      \n      \n    \n\n\n\n\n\ntf.keras.datasets.cifar10.load_data()을 이용하여 CIFAR10을 불러온 뒤 적당한 네트워크를 사용하여 적합하라.\n\n결과를 텐서보드로 시각화할 필요는 없다.\n자유롭게 모형을 설계하여 적합하라.\ntest set의 accuracy가 70%이상인 경우만 정답으로 인정한다.\n\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n\n\nx_train.shape\n\n(50000, 32, 32, 3)\n\n\n\nX = tf.constant(x_train.reshape(-1,32,32,3),dtype=tf.float64)\ny = tf.keras.utils.to_categorical(y_train)\nXX = tf.constant(x_test.reshape(-1,32,32,3),dtype=tf.float64)\nyy = tf.keras.utils.to_categorical(y_test)\n\n\nprint(X.shape)\nprint(y.shape)\nprint(XX.shape)\nprint(yy.shape)\n\n(50000, 32, 32, 3)\n(50000, 10)\n(10000, 32, 32, 3)\n(10000, 10)\n\n\n\nnet2 = tf.keras.Sequential()\nnet2.add(tf.keras.layers.Conv2D(512,(2,2),activation='relu'))\nnet2.add(tf.keras.layers.Conv2D(512,(2,2),activation='relu'))\nnet2.add(tf.keras.layers.Dropout(0.5))\nnet2.add(tf.keras.layers.MaxPool2D()) \nnet2.add(tf.keras.layers.Conv2D(512,(2,2),activation='relu'))\nnet2.add(tf.keras.layers.Conv2D(512,(2,2),activation='relu'))\nnet2.add(tf.keras.layers.Dropout(0.5))\nnet2.add(tf.keras.layers.MaxPool2D()) \nnet2.add(tf.keras.layers.Flatten())\nnet2.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet2.compile(optimizer='adam', loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n\n\nnet2.fit(X,y,epochs=5,validation_split=0.2)\n\nEpoch 1/5\n1250/1250 [==============================] - 43s 35ms/step - loss: 0.7524 - accuracy: 0.7392 - val_loss: 0.8735 - val_accuracy: 0.6990\nEpoch 2/5\n1250/1250 [==============================] - 43s 35ms/step - loss: 0.7346 - accuracy: 0.7436 - val_loss: 0.8461 - val_accuracy: 0.7115\nEpoch 3/5\n1250/1250 [==============================] - 44s 35ms/step - loss: 0.7320 - accuracy: 0.7479 - val_loss: 0.9425 - val_accuracy: 0.6764\nEpoch 4/5\n1250/1250 [==============================] - 44s 35ms/step - loss: 0.7215 - accuracy: 0.7490 - val_loss: 0.8847 - val_accuracy: 0.6996\nEpoch 5/5\n1250/1250 [==============================] - 44s 35ms/step - loss: 0.7275 - accuracy: 0.7484 - val_loss: 0.8131 - val_accuracy: 0.7151\n\n\n<keras.callbacks.History at 0x7f61c0b057b0>\n\n\n\nnet2.fit(X,y,epochs=10,validation_split=0.2)\n\nEpoch 1/10\n1250/1250 [==============================] - 44s 35ms/step - loss: 0.9037 - accuracy: 0.6853 - val_loss: 1.0318 - val_accuracy: 0.6497\nEpoch 2/10\n1250/1250 [==============================] - 44s 35ms/step - loss: 0.8785 - accuracy: 0.6936 - val_loss: 0.9734 - val_accuracy: 0.6715\nEpoch 3/10\n1250/1250 [==============================] - 44s 35ms/step - loss: 0.8478 - accuracy: 0.7062 - val_loss: 0.9265 - val_accuracy: 0.6795\nEpoch 4/10\n1250/1250 [==============================] - 44s 35ms/step - loss: 0.8271 - accuracy: 0.7121 - val_loss: 0.9939 - val_accuracy: 0.6640\nEpoch 5/10\n1250/1250 [==============================] - 44s 35ms/step - loss: 0.8103 - accuracy: 0.7203 - val_loss: 0.8956 - val_accuracy: 0.6903\nEpoch 6/10\n1250/1250 [==============================] - 44s 35ms/step - loss: 0.8028 - accuracy: 0.7214 - val_loss: 0.9100 - val_accuracy: 0.6915\nEpoch 7/10\n1250/1250 [==============================] - 44s 35ms/step - loss: 0.7797 - accuracy: 0.7308 - val_loss: 0.9072 - val_accuracy: 0.6873\nEpoch 8/10\n1250/1250 [==============================] - 44s 35ms/step - loss: 0.7773 - accuracy: 0.7318 - val_loss: 0.8384 - val_accuracy: 0.7102\nEpoch 9/10\n1250/1250 [==============================] - 44s 35ms/step - loss: 0.7509 - accuracy: 0.7397 - val_loss: 0.9481 - val_accuracy: 0.6789\nEpoch 10/10\n1250/1250 [==============================] - 44s 35ms/step - loss: 0.7638 - accuracy: 0.7347 - val_loss: 0.8913 - val_accuracy: 0.6971\n\n\n<keras.callbacks.History at 0x7f61c0b51ab0>\n\n\n\nnet2.evaluate(XX,yy)\n\n313/313 [==============================] - 4s 11ms/step - loss: 0.8302 - accuracy: 0.7135\n\n\n[0.830169141292572, 0.7135000228881836]\n\n\n\n\n\n(1) (1,128,128,3)의 shape을 가진 텐서가 tf.keras.layers.Conv2D(5,(2,2))으로 만들어진 커널을 통과할시 나오는 shape은?\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(5,(2,2))\nXXX = tnp.array([1]*1*128*128*3,dtype=tf.float64).reshape(1,128,128,3)\n\n\ncnv(XXX)\n\n<tf.Tensor: shape=(1, 127, 127, 5), dtype=float32, numpy=\narray([[[[-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         ...,\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703]],\n\n        [[-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         ...,\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703]],\n\n        [[-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         ...,\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703]],\n\n        ...,\n\n        [[-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         ...,\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703]],\n\n        [[-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         ...,\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703]],\n\n        [[-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         ...,\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703],\n         [-0.7661123 , -0.81788373,  0.151829  , -0.94353175,\n          -0.43155703]]]], dtype=float32)>\n\n\n답 : (1, 127, 127, 5)\n(2) (1,24,24,16)의 shape을 가진 텐서가 tf.keras.layers.Flatten()을 통과할때 나오는 텐서의 shape은?\n\n24*24*16\n\n9216\n\n\n답 : (1, 9216)\n(3)-(4)\n아래와 같은 모형을 고려하자.\n\\[y_i= \\beta_0 + \\sum_{k=1}^{5} \\beta_k \\cos(k t_i)+\\epsilon_i\\]\n여기에서 \\(t=(t_1,\\dots,t_{1000})=\\) np.linspace(0,5,1000) 이다. 그리고 \\(\\epsilon_i \\sim i.i.d~ N(0,\\sigma^2)\\), 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자.\n\nnp.random.seed(43052)\nt= np.linspace(0,5,1000)\ny = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2\nplt.plot(t,y,'.',alpha=0.1)\n\n\n\n\ntf.keras를 이용하여 \\(\\beta_0,\\dots,\\beta_5\\)를 추정하라. (\\(\\beta_0,\\dots,\\beta_5\\)의 참값은 각각 -2, 3, 1, 0, 0, 0.5 이다)\n(3) 모형에 대한 설명 중 옳은 것을 모두 골라라.\n(하영) 이 모형의 경우 MSEloss를 최소화하는 \\(\\hat{\\beta}_0,\\dots,\\hat{\\beta}_5\\)를 구하는것은 최대우도함수를 최대화하는 \\(\\hat{\\beta}_0,\\dots,\\hat{\\beta}_5\\)를 구하는 것과 같다.\n답 : 참\n(재인) 하영의 말이 옳은 이유는 오차항이 정규분포를 따른다는 가정이 있기 때문이다.\n답 : 참\n(서연) 이 모형에서 적절한 학습률이 선택되더라도 경사하강법을 이용하면 MSEloss를 최소화하는 \\(\\hat{\\beta}_0,\\dots,\\hat{\\beta}_5\\)를 종종 구할 수 없는 문제가 생긴다. 왜냐하면 손실함수가 convex하지 않아서 local minimum에 빠질 위험이 있기 때문이다.\n답 : 참\n(규빈) 만약에 경사하강법 대신 확률적 경사하강법을 쓴다면 local minimum을 언제나 탈출 할 수 있다. 따라서 서연이 언급한 문제점은 생기지 않는다.\n답 : 거짓\n(4) 다음은 아래 모형을 학습한 결과이다. 옳게 해석한 것을 모두 고르시오.\n\ny = y.reshape(1000,1)\nx1 = np.cos(t) \nx2 = np.cos(2*t)\nx3 = np.cos(3*t)\nx4 = np.cos(4*t)\nx5 = np.cos(5*t)\nX = tf.stack([x1,x2,x3,x4,x5],axis=1)\n\n2022-06-13 19:35:01.615989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1)) \nnet.compile(loss='mse',optimizer='adam')\nnet.fit(X,y,epochs=500,batch_size=100, validation_split=0.45,verbose=0)\n\n<keras.callbacks.History at 0x7f60e0150970>\n\n\n\nplt.plot(y,'.',alpha=0.1)\nplt.plot(net(X),'--')\n\n\n\n\n(재인) 처음 550개의 데이터만 학습하고 이후의 450개의 데이터는 학습하지 않고 validation으로 이용하였다.\n답 : 거짓\n(서연) validation에서의 적합결과가 좋지 않다.\n답 : 참\n(규빈) validation의 적합결과가 좋지 않기 때문에 오버피팅을 의심할 수 있다. 따라서 만약에 네트워크에 드랍아웃층을 추가한다면 오버피팅을 방지하는 효과가 있어 validation의 loss가 줄어들 것이다.\n답 : 거짓\n(하영) 이 모형의 경우 더 많은 epoch으로 학습한다면 train loss와 validation loss를 둘 다 줄일 수 있다.\n답 : 참\n(5) 다음을 잘 읽고 참 거짓을 판별하라. - Convolution은 선형변환이다.\n답 : 참 - CNN을 이용하면 언제나 손실함수를 MSEloss로 선택해야 한다.\n답 : 거짓 - CNN은 adam optimizer를 통해서만 최적화할 수 있다.\n답 : 참 - 이미지자료는 CNN을 이용하여서만 분석할 수 있으며 DNN으로는 분석불가능하다.\n답 : 거짓 - CNN은 칼라이미지일 경우에만 적용가능하다.\n답 : 거짓"
  },
  {
    "objectID": "posts/2021-12-31-python을 이용한 고유값과 고유벡터 구하기.html",
    "href": "posts/2021-12-31-python을 이용한 고유값과 고유벡터 구하기.html",
    "title": "jisim12",
    "section": "",
    "text": "python을 이용한 고유값과 고유벡터 문제풀이\n\n선형대수학\n\n\ntoc:false\nbranch: master\nbadges: true\ncomments: true\nauthor: 심재인\n\n\n선형대수학\n\n\nimport numpy as np\nimport numpy.linalg as lin\n\n\n1.\n\\(A\\) = \\(\\left[\\begin{array}{rrr} 0&1\\\\ 1&0\\\\ \\end{array}\\right]\\) 의 고유값과 고유벡터를 구해라.\n\nexEin = np.array([[0,1],[1,0]])\n\n\nlin.eig(exEin)\n\n(array([ 1., -1.]),\n array([[ 0.70710678, -0.70710678],\n        [ 0.70710678,  0.70710678]]))\n\n\n\n\n2.\n\\(A\\) = \\(\\left[\\begin{array}{rrr} 4&2\\\\ 3&5\\\\ \\end{array}\\right]\\) 의 고유값과 고유벡터를 구해라.\n\nexEin = np.array([[4,2],[3,5]])\n\n\nlin.eig(exEin)\n\n(array([2., 7.]),\n array([[-0.70710678, -0.5547002 ],\n        [ 0.70710678, -0.83205029]]))\n\n\n\n\n3.\n\\(A\\) = \\(\\left[\\begin{array}{rrr} 1&0&0\\\\ 0&1&0\\\\ 0&0&1\\\\ \\end{array}\\right]\\) 의 고유값과 고유벡터를 구해라.\n\nexEin = np.array([[1,0,0],[0,1,0],[0,0,1]])\n\n\nlin.eig(exEin)\n\n(array([1., 1., 1.]),\n array([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]]))\n\n\n\n\n4.\n\\(A\\) = \\(\\left[\\begin{array}{rrr} -4&-6\\\\ 3&5\\\\ \\end{array}\\right]\\) 의 고유값과 고유벡터를 구해라.\n\nexEin = np.array([[-4,-6],[3,5]])\n\n\nlin.eig(exEin)\n\n(array([-1.,  2.]),\n array([[-0.89442719,  0.70710678],\n        [ 0.4472136 , -0.70710678]]))\n\n\n\n\n5.\n\\(A\\) = \\(\\left[\\begin{array}{rrr} 5&-1\\\\ -2&1\\\\ \\end{array}\\right]\\) 의 고유값과 고유벡터를 구해라.\n\nexEin = np.array([[5, -1], [-2, 1]])\n\n\nlin.eig(exEin)\n\n(array([5.44948974, 0.55051026]),\n array([[ 0.91209559,  0.21927526],\n        [-0.40997761,  0.97566304]]))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "jisim12",
    "section": "",
    "text": "4주차-3월 28일\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n5주차-4월 04일\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n2주차-3월 14일\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n선형대수 고유값 고유벡터\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression Diagnostic\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n9주차-5월 02일 (2)\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Linear Regression\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n2022.09.02\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n3주차-3월 21일\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Selection\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n중간고사해설\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n기말고사 1~2번풀이 (심재인)\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n기말고사 예상문제\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple Linear Regression\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n6주차 4월11일\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n10주차-5월 09일\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n7주차-4월 18일\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n중간고사예상문제-4월 28일\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n빅데이터분석 특강 중간고사\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n12주차-5월 23일\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n9주차-5월 02일 (1)\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n1주차-3월 07일\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n11주차-5월 16일\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n기말고사 예상문제\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\npython을 이용한 고유값과 고유벡터 문제풀이\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n2023/03/17 테스트\n\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2023\n\n\n심재인\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCough Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nSep 21, 2022\n\n\n심재인\n\n\n\n\n\n\nNo matching items"
  }
]