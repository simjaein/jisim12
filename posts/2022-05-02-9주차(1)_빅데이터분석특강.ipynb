{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad7cffd-b0c7-43e5-8555-4613cc22d17d",
   "metadata": {},
   "source": [
    "# 9주차-5월 02일 (1)\n",
    "> 빅데이터분석특강\n",
    "\n",
    "- toc:false\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: 심재인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577f5c3f-ea20-4dbd-b779-c02aac35079d",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6527fd09-c582-4cf7-9d00-54135c7e1461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.experimental.numpy as tnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9232293e-29bd-47e4-9359-2f51003e8e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnp.experimental_enable_numpy_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e0627b0-4411-490a-8af0-b79326593bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107f769e-b3a4-4549-a2eb-a452d03179d4",
   "metadata": {},
   "source": [
    "### 우도함수와 최대우도추정량\n",
    "**예제**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a2d5f7-9811-4b21-b685-512fba5843b2",
   "metadata": {},
   "source": [
    "$X_i \\overset{iid}{\\sim} Ber(p)$에서 얻은 샘플이 아래와 같다고 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b6fdd0c-b01a-4a75-a4bd-2e164eaae19f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=[0,1,0,1]\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f64cbd-012d-4140-8e60-8d1e17add605",
   "metadata": {},
   "source": [
    "$p$는 얼마라고 볼 수 있는가? --> 0.5\n",
    "\n",
    "왜?? $p$가 0.5라고 주장할 수 있는 이론적 근거, 혹은 논리체계가 무엇인가?\n",
    "\n",
    "`-` suppose: $p=0.1$ 이라고 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f532f768-10fb-4369-961b-388c9af80e99",
   "metadata": {},
   "source": [
    "그렇다면 $(x_1,x_2,x_3,x_4)=(0,1,0,1)$와 같은 샘플이 얻어질 확률이 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "932be8c4-026c-4d79-9be9-be495ce90a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008100000000000001"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.9 * 0.1 * 0.9 * 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e89a4bc-4b80-4d5c-bebc-c0c0c973809c",
   "metadata": {},
   "source": [
    "`-` suppose: $p=0.2$ 이라고 하자.\n",
    "\n",
    "그렇다면 $(x_1,x_2,x_3,x_4)=(0,1,0,1)$와 같은 샘플이 얻어질 확률이 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53b9f1d0-3e32-41d2-bb2d-fd2f7a35792b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025600000000000008"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.8 * 0.2 * 0.8 * 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3160be36-edeb-4d12-83d1-59b22a0a6324",
   "metadata": {},
   "source": [
    "`-` 질문1: $p=0.1$인것 같냐? 아니면 $p=0.2$인것 같냐? -> $p=0.2$\n",
    "- 왜?? $p=0.2$일 ~~확률~~이 더 크다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd47d5ee-dbb7-43a3-bf66-a6635e5f3c2d",
   "metadata": {},
   "source": [
    "***(여기서 잠깐 중요한것) 확률이라는 말을 함부로 쓸 수 없다.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fc2a75-68ce-4844-9367-9a93d7bfbea6",
   "metadata": {},
   "source": [
    "`-` 0.0256은 \"$p=0.2$일 경우 샘플 (0,1,0,1)이 얻어질 확률\"이지 \"$p=0.2$일 확률\"은 아니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea3b78-c53b-4f4c-8ab2-07dbedbec168",
   "metadata": {},
   "source": [
    "\"$p=0.2$인 확률\" 이라는 개념이 성립하려면 아래코드에서 `sum([(1-p)*p*(1-p)*p for p in _plist])`이 1보다는 작아야 한다. (그런데 1보다 크다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7227c9bd-9849-45c6-af07-c04959b3fcf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.49983299986714"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_plist = np.linspace(0.499,0.501,1000)\n",
    "sum([(1-p)*p*(1-p)*p for p in _plist])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0eda77-3e70-4cc8-b01b-e04b13435e94",
   "metadata": {},
   "source": [
    "`-` 확률이라는 말을 쓸 수 없지만 확률의 느낌은 있음 -> 가능도라는 말을 쓰자.\n",
    "- 0.0256 $=$ $p$가 0.2일 경우 샘플 (0,1,0,1)이 얻어질 확률 $=$ $p$가 0.2일 가능도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc12595-b2a9-49a7-9482-56514e8ddaf9",
   "metadata": {},
   "source": [
    "`-` 다시 질문1로 돌아가자!\n",
    "- 질문1: $p=0.1$인 것 같냐? 아니면 $p=0.2$인 것 같냐? -> 답 $p=0.2$ -> 왜? $p=0.2$인 가능도가 더 크니까!\n",
    "- 질문2: $p=0.2$인 것 같냐? 아니면 $p=0.3$인 것 같냐? -> 답 $p=0.3$ -> 왜? $p=0.3$인 가능도가 더 크니까!\n",
    "- 질문3: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356a21ca-dc00-4a89-9169-1f5b91d9901e",
   "metadata": {},
   "source": [
    "`-` 궁극의 질문: $p$가 뭐일 것 같아?\n",
    "- $p$가 입력으로 들어가면 가능도가 계산되는 함수를 만들자.\n",
    "- 그 함수를 최대화하는 $p$를 찾자.\n",
    "- 그 $p$가 궁극의 질문에 대한 대답이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c9c63f-aca9-4fb1-83b1-cc263bed2c57",
   "metadata": {},
   "source": [
    "`-` 잠깐 용어정리\n",
    "- 가능도함수 $=$ 우도함수 $=$ likelihood function $:=$ $L(p)$\n",
    "- $p$의 maximum likelihood estimator $=$ p의 MLE $:=$ $\\hat{p}^{mle}$ $=$ $\\text{argmax}_p L(p)$ $=$ $\\hat{p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4def6b84-b351-4822-b451-9453aacb8f2c",
   "metadata": {},
   "source": [
    "(예제의 풀이)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506f6235-7a52-49ee-85d9-2bffdcdca29a",
   "metadata": {},
   "source": [
    "`-` 이 예제의 경우 가능도함수를 정의하자.\n",
    "- $L(p)$: $p$의 가능도함수 = $p$가 모수일때 샘플 (0,1,0,1)이 얻어질 확률 = $p$가 모수일때 $x_1$이 0일 확률 $\\times \\dots \\times$ $p$가 모수일때 $x_4$가 1일 확률\n",
    "- $L(p)=\\prod_{i=1}^{4} f(x_i;p)= \\prod_{i=1}^{4}p^{x_i}(1-p)^{1-x_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2921ce2-ca85-4a6c-92c3-f09d1abe4aac",
   "metadata": {},
   "source": [
    "> note: 참고로 이 과정을 일반화 하면 $X_1,\\dots,X_n \\overset{iid}{\\sim} Ber(p)$ 일때 $p$의 likelihood function은 $\\prod_{i=1}^{n}p^{x_i}(1-p)^{1-x_i}$ 라고 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4f93e5-f2d4-4e43-a2a4-7543004387f9",
   "metadata": {},
   "source": [
    "> note: 더 일반화: $x_1,\\dots,x_n$이 pdf가 $f(x)$인 분포에서 뽑힌 서로 독립인 샘플일때 likelihood function은 $\\prod_{i=1}^{n}f(x_i)$라고 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74ff276-733b-4e54-b491-bba644e11199",
   "metadata": {},
   "source": [
    "`-` 이 예제의 경우 $p$의 최대우도추정량을 구하면 \n",
    "\n",
    "$$\\hat{p}^{mle} = \\text{argmax}_p L(p) = \\text{argmax}_p  \\big\\{ p^2(1-p)^2 \\big\\}= \\frac{1}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d02da2-88b5-4d55-8f04-2f36fa5db4f6",
   "metadata": {},
   "source": [
    "### 중간고사 1번"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31327b55-19c2-4621-92f3-55d60cc79cb7",
   "metadata": {},
   "source": [
    "`(1)` $N(\\mu,\\sigma)$에서 얻은 샘플이 아래와 같다고 할때 $\\mu,\\sigma$의 MLE를 구하여라.\n",
    "\n",
    "```\n",
    "<tf.Tensor: shape=(10000,), dtype=float64, numpy=\n",
    "array([ 4.12539849,  5.46696729,  5.27243374, ...,  2.89712332,\n",
    "        5.01072291, -1.13050477])>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cd34a7-d069-4b8c-8753-57d28ad18633",
   "metadata": {},
   "source": [
    "`(2)` $Ber(p)$에서 얻은 샘플이 아래와 같다고 할 때 $p$의 MLE를 구하여라. \n",
    "\n",
    "```\n",
    "<tf.Tensor: shape=(10000,), dtype=int64, numpy=array([1, 1, 1, ..., 0, 0, 1])>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da4390f-6ca3-41ca-be13-faf201baf2d4",
   "metadata": {},
   "source": [
    "`(3)` $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$,  $\\epsilon_i \\overset{iid}{\\sim} N(0,1)$ 일때 $(\\beta_0,\\beta_1)$의 MLE를 구하여라. (회귀모형)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc1f934-bb0e-4aa0-9ed6-bdf60e9b6100",
   "metadata": {},
   "source": [
    "(풀이) 가능도함수\n",
    "\n",
    "\n",
    "$$L(\\beta_0,\\beta_1)=\\prod_{i=1}^{n}f(y_i), \\quad f(y_i)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(y_i-\\mu_i)^2}, \\quad \\mu_i=\\beta_0+\\beta_1 x_i$$\n",
    "\n",
    "를 최대화하는 $\\beta_0,\\beta_1$을 구하면된다. 그런데 이것은 아래를 최소화하는 $\\beta_0,\\beta_1$을 구하는 것과 같다. \n",
    "\n",
    "$$-\\log L(\\beta_0,\\beta_1) = \\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2$$\n",
    "\n",
    "위의 식은 SSE와 같다. 결국 오차항이 정규분포를 따르는 회귀모형의 MLE는 MSE를 최소화하는 $\\beta_0,\\beta_1$을 구하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46af3221-8506-4467-9f02-31e5ed3faa98",
   "metadata": {},
   "source": [
    "**중간고사 1-(3)의 다른 풀이**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa61872b-a440-40ea-9e1e-ad2e1477e266",
   "metadata": {},
   "source": [
    "step1: 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03f2db70-76f2-45d6-9766-bf821acaa4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= tf.constant(np.arange(1,10001)/10000)\n",
    "y= tnp.random.randn(10000) + (0.5 + 2*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08369d9d-cbc6-4222-86f6-f939991387c1",
   "metadata": {},
   "source": [
    "step2: minimize MSEloss (원래는 maximize log-likelihood)\n",
    "\n",
    "- maximize likelihood였던 문제를 minimize MSEloss로 바꾸어도 되는근거? 주어진 함수(=가능도함수)를 최대화하는 $\\beta_0,\\beta_1$은 MSE를 최소화하는 $\\beta_0,\\beta_1$과 동일하므로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaeeb4a5-a835-4b33-b1f2-5afa6716331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta0= tf.Variable(1.0)\n",
    "beta1= tf.Variable(1.0)\n",
    "for i in range(2000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        #minus_log_likelihood = tf.reduce_sum((y-beta0-beta1*x)**2)\n",
    "        loss =  tf.reduce_sum((y-beta0-beta1*x)**2)\n",
    "    slope1, slope2 = tape.gradient(loss,[beta0,beta1])\n",
    "    beta0.assign_sub(slope1* 0.1/10000) # N=10000\n",
    "    beta1.assign_sub(slope2* 0.1/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afc98c77-26fc-4f7d-b8e0-f610812bbbab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.47993016>,\n",
       " <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.020918>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta0,beta1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ddc8c4-e22b-4e42-bfa3-404589b35254",
   "metadata": {},
   "source": [
    "`-` 문제를 풀면서 생각해보니 손실함수는 `-로그가능도함수`로 선택하면 될 것 같다? \n",
    "- 손실함수를 선택하는 기준이 -로그가능도함수만 존재하는 것은 아니나 대부분 그러하긴함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319bbf17-e5c4-47d7-8528-799005967801",
   "metadata": {},
   "source": [
    "`(4)` 출제하지 못한 중간고사 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ca99c8-0566-4fb3-8420-9865c5c0f213",
   "metadata": {},
   "source": [
    "아래의 모형을 생각하자. \n",
    "- $Y_i \\overset{iid}{\\sim} Ber(\\pi_i)$\n",
    "- $\\pi_i = \\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}=\\frac{\\exp(-1+5x_i)}{1+\\exp(-1+5x_i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fdb048-7223-48ba-84a6-e11e6bea58d0",
   "metadata": {},
   "source": [
    "아래는 위의 모형에서 얻은 샘플이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0faf518a-14e2-4c6f-b195-efa06988afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tnp.linspace(-1,1,2000)\n",
    "pi = tnp.exp(-1+5*x) / (1+tnp.exp(-1+5*x))\n",
    "y = np.random.binomial(1,pi)\n",
    "y = tf.constant(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e98ed62-d078-4813-b3ac-2d1c52b619cd",
   "metadata": {},
   "source": [
    "함수 $L(w_0,w_1)$을 최대화하는 $(w_0,w_1)$를 `tf.GradeintTape()`를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 $(w_0,w_1)$의 초기값은 모두 0.1로 설정할 것)\n",
    "\n",
    "$$L(w_0,w_1)=\\prod_{i=1}^{n}f(y_i), \\quad f(x_i)={\\pi_i}^{y_i}(1-\\pi_i)^{1-y_i},\\quad \\pi_i=\\text{sigmoid}(w_0+w_1x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9183b8b9-03c8-4d62-895b-79cb9d5b9560",
   "metadata": {},
   "source": [
    "(풀이1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb263f67-0c79-4441-a743-0bc07660758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0hat = tf.Variable(1.0)\n",
    "w1hat = tf.Variable(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "409af599-92bd-4a12-9cfe-d5e0d852e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pihat = tnp.exp(w0hat+w1hat *x) / (1+tnp.exp(w0hat+w1hat *x))\n",
    "        pdf = pihat**y * (1-pihat)**(1-y)\n",
    "        logL = tf.reduce_mean(tnp.log(pdf))\n",
    "    slope1,slope2 = tape.gradient(logL,[w0hat,w1hat])\n",
    "    w0hat.assign_add(slope1*0.1)\n",
    "    w1hat.assign_add(slope2*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99074b29-d348-4bd3-8bd5-ad42ded4c094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.88931483>,\n",
       " <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.231925>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0hat,w1hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567bbe79-860e-4e2b-929e-c1ecf67b111c",
   "metadata": {},
   "source": [
    "(해석) - 로지스틱에서 가능도함수와 BCEloss의 관계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0e8904-5ce5-464e-915c-38986e94495e",
   "metadata": {},
   "source": [
    "$L(w_0,w_1)$를 최대화하는 $w_0,w_1$은 아래를 최소화하는 $w_0,w_1$와 같다.\n",
    "\n",
    "$$-\\log L(w_0,w_1) = - \\sum_{i=1}^{n}\\big(y_i \\log(\\pi_i) + (1-y_i)\\log(1-\\pi_i)\\big)$$\n",
    "\n",
    "이것은 최적의 $w_0,w_1$을 $\\hat{w}_0,\\hat{w}_1$이라고 하면 $\\hat{\\pi}_i=\\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\hat{y}_i$이 되고 따라서 위의 식은 $n\\times$BCEloss의 형태임을 쉽게 알 수 있다.\n",
    "\n",
    "결국 로지스틱 모형에서 $(w_0,w_1)$의 MLE를 구하기 위해서는 BCEloss를 최소화하는 $(w_0,w_1)$을 구하면 된다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a539fd4d-5c50-47cb-b3da-bebcff83f89f",
   "metadata": {},
   "source": [
    "(풀이2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d40006ee-6349-469d-8402-111e6ae1de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0hat = tf.Variable(1.0)\n",
    "w1hat = tf.Variable(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca760569-adc5-4be4-bd09-f4a563ac07ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        yhat = tnp.exp(w0hat+w1hat *x) / (1+tnp.exp(w0hat+w1hat *x))\n",
    "        loss = tf.losses.binary_crossentropy(y,yhat)\n",
    "    slope1,slope2 = tape.gradient(loss,[w0hat,w1hat])\n",
    "    w0hat.assign_sub(slope1*0.1)\n",
    "    w1hat.assign_sub(slope2*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45f08b35-e5e6-4509-a1e2-2db091ad687c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.88931495>,\n",
       " <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.2319255>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0hat,w1hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3280723-d4f5-44f7-a3e8-d8eb83c59fd9",
   "metadata": {},
   "source": [
    "### 손실함수의 설계 (선택)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8f6da2-56c6-4814-a6f1-79fbad15dfa6",
   "metadata": {},
   "source": [
    "`-` 회귀분석이든 로지스틱이든 손실함수는 minus_log_likelihood 로 선택한다.\n",
    "- 그런데 (오차항이 정규분포인) 회귀분석 일때는 minus_log_likelihood 가 MSEloss가 되고\n",
    "- 로지스틱일때는 minus_log_likelihood 가 BCEloss가 된다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649c444e-696b-4593-8a14-de81ec1a923d",
   "metadata": {},
   "source": [
    "`-` minus_log_likelihood가 손실함수를 선택하는 유일한 기준은 아니다. <--- 참고만하세요, 이 수업에서는 안중요합니다.\n",
    "- 오차항이 대칭이고 서로독립이며 등분산 가정을 만족하는 어떠한 분포에서의 회귀모형이 있다고 하자. 이 회귀모형에서 $\\hat{\\beta}$은 여전히 MSEloss를 최소화하는 $\\beta$를 구함으로써 얻을 수 있다.\n",
    "- 이 경우 MSEloss를 쓰는 이론적근거? $\\hat{\\beta}$이 BLUE가 되기 때문임 (가우스-마코프정리)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
